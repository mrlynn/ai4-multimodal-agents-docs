{"cells":[{"source":"# Building Multimodal AI Applications with MongoDB and Voyage AI","metadata":{},"id":"8c8fde4c-9222-4265-b72b-8d7693520250","cell_type":"markdown"},{"source":"In this codealong, you will learn how to build a multimodal AI agent from scratch using Voyage AI's multimodal embedding models, Google's multimodal LLMs, and MongoDB as a vector database and memory provider for the agent.","metadata":{},"id":"3e302e1c-4c18-4c44-87fd-ba935c3a0853","cell_type":"markdown"},{"source":"# Task 0: Install required libraries\n\n- **pymongo**: Python driver for MongoDB\n- **voyageai**: Python client for Voyage AI\n- **google-genai**: Python library to access Google's embedding models and LLMs via Google AI Studio\n- **PyMuPDF**: Python library for analyzing and manipulating PDFs\n- **Pillow**: A Python imaging library\n- **tqdm**: Show progress bars for loops in Python","metadata":{},"id":"a9274661-8d8c-4cc5-901e-5fc497866b89","cell_type":"markdown"},{"source":"!pip install -qU pymongo voyageai google-genai PyMuPDF Pillow tqdm","metadata":{"executionCancelledAt":null,"executionTime":6301,"lastExecutedAt":1752779887191,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install -qU pymongo voyageai google-genai PyMuPDF Pillow tqdm","outputsMetadata":{"0":{"height":80,"type":"stream"}}},"id":"f81d6d0e-986b-49f4-94e1-7315a7f0bd67","cell_type":"code","execution_count":3,"outputs":[]},{"source":"# Task 1: Setup prerequisites\n\n- **MongoDB cluster setup**:\n    - Register for a [free MongoDB Atlas account](https://www.mongodb.com/cloud/atlas/register/?utm_campaign=devrel&utm_source=third-party-content&utm_medium=cta&utm_content=datacamp&utm_term=apoorva.joshi).\n    - [Create a new database cluster](https://www.mongodb.com/docs/guides/atlas/cluster/?utm_campaign=devrel&utm_source=third-party-content&utm_medium=cta&utm_content=datacamp&utm_term=apoorva.joshi).\n    - [Obtain the connection string for your database cluster](https://www.mongodb.com/docs/guides/atlas/connection-string/?utm_campaign=devrel&utm_source=third-party-content&utm_medium=cta&utm_content=datacamp&utm_term=apoorva.joshi).\n- **Obtain a Voyage AI API key**: Follow the steps [here](https://docs.voyageai.com/docs/api-key-and-installation#authentication-with-api-keys) to get a Voyage AI API key.\n- **Obtain a Gemini API key**: Follow the steps [here](https://ai.google.dev/gemini-api/docs/api-key) to get a Gemini API key via Google AI Studio.","metadata":{},"id":"7f6c51c7-bbbb-4f0f-b218-850221f3dcdf","cell_type":"markdown"},{"source":"import getpass\nimport os","metadata":{"executionCancelledAt":null,"executionTime":1584,"lastExecutedAt":1752779888777,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import getpass\nimport os"},"id":"474d98a8-33ce-4898-a641-4853f17e5738","cell_type":"code","execution_count":4,"outputs":[]},{"source":"# Set your MongoDB connection string\nMONGODB_URI = getpass.getpass(\"Enter your MongoDB connection string: \")","metadata":{"executionCancelledAt":null,"executionTime":2298,"lastExecutedAt":1752779891076,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Set your MongoDB connection string\nMONGODB_URI = getpass.getpass(\"Enter your MongoDB connection string: \")","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"id":"7f00b842-a949-4f01-ac90-d63ab84a2899","cell_type":"code","execution_count":5,"outputs":[]},{"source":"# Set Voyage AI API Key\nos.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter your Voyage AI API key: \")","metadata":{"executionCancelledAt":null,"executionTime":5035,"lastExecutedAt":1752779897689,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Set Voyage AI API Key\nos.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter your Voyage AI API key: \")","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"id":"ded228bd-cbdd-48a9-a3ff-f11c93d5d56c","cell_type":"code","execution_count":6,"outputs":[]},{"source":"# Set Gemini API Key\nGEMINI_API_KEY = getpass.getpass(\"Enter your Gemini API key: \")","metadata":{"executionCancelledAt":null,"executionTime":9240,"lastExecutedAt":1752779908370,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Set Gemini API Key\nGEMINI_API_KEY = getpass.getpass(\"Enter your Gemini API key: \")","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"id":"4992a5c3-c72d-47b3-8503-0331180cd27d","cell_type":"code","execution_count":7,"outputs":[]},{"source":"In this codealong, we will build an AI agent that can help users make sense of large documents containing text interleaved with figures and tables. Some examples of these in the real world are financial reports, technical manuals, business proposals, product catalogs, research papers, etc. To represent this type of data in the codealong, we will use the [Deepseek-R1 paper](https://arxiv.org/pdf/2501.12948).\n\nThe goal of our agent will be two-fold:\n- Answer questions about the paper\n- Explain charts and diagrams found in the paper\n\nLet's first preprocess the document in order to effectively retrieve information from it. Here's what the data processing workflow will look like:\n\n![data_processing.png](data_processing.png)\n\n- 1: Convert each document to a series of screenshots\n- 2: Save the screenshots to blob storage (AWS S3, GCS etc.) and extract a unique identifier for each screenshot\n- 3-4: Pass the screenshots through Voyage AI's voyage-multimodal-3 model to generate embeddings\n- 5: Insert documents consisting of the screenshot embeddings and metadata into MongoDB","metadata":{},"id":"8d52ba0d-c1d1-46f9-bbca-983122c5af05","cell_type":"markdown"},{"source":"# Task 2: Read PDF from URL","metadata":{},"id":"65454beb-970f-4af6-a04c-798b9f665b6f","cell_type":"markdown"},{"source":"import pymupdf\nimport requests","metadata":{"id":"bA5ajAmk7XH6","executionTime":146,"lastSuccessfullyExecutedCode":"import pymupdf\nimport requests","executionCancelledAt":null,"lastExecutedAt":1752779911279,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null},"id":"d0eb4f16-5a99-460d-a5ba-706b7ef0bbe7","cell_type":"code","execution_count":8,"outputs":[]},{"source":"# Download the DeepSeek paper\nresponse = requests.get(\"https://arxiv.org/pdf/2501.12948\")\nif response.status_code != 200:\n    raise ValueError(f\"Failed to download PDF. Status code: {response.status_code}\")\n# Get the content of the response\npdf_stream = response.content\n# Open the data in `pdf_stream` as a PDF document.\npdf = pymupdf.Document(stream=pdf_stream, filetype=\"pdf\")","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1752779912335,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Download the DeepSeek paper\nresponse = requests.get(\"https://arxiv.org/pdf/2501.12948\")\nif response.status_code != 200:\n    raise ValueError(f\"Failed to download PDF. Status code: {response.status_code}\")\n# Get the content of the response\npdf_stream = response.content\n# Open the data in `pdf_stream` as a PDF document.\npdf = pymupdf.Document(stream=pdf_stream, filetype=\"pdf\")"},"id":"7c6d97cc-8a87-4059-a2d2-ccd52594f0f1","cell_type":"code","execution_count":9,"outputs":[]},{"source":"# Task 3: Store PDF images locally and extract metadata for MongoDB","metadata":{},"id":"443463b5-0fa3-4bcf-b5e1-90440b14b0e9","cell_type":"markdown"},{"source":"from tqdm import tqdm","metadata":{"executionCancelledAt":null,"executionTime":21,"lastExecutedAt":1752779913986,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from tqdm import tqdm"},"id":"ee6c792b-156f-47f9-93ee-35d3ac8c9ab9","cell_type":"code","execution_count":10,"outputs":[]},{"source":"docs = []","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1752779914924,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"docs = []"},"id":"7065b4c8-77e1-4c54-b419-ecebd8a20219","cell_type":"code","execution_count":11,"outputs":[]},{"source":"zoom = 3.0\n# Set image matrix dimensions\nmat = pymupdf.Matrix(zoom, zoom)\n# Iterate through the pages of the PDF\nfor n in tqdm(range(pdf.page_count)):\n    temp = {}\n    # Use the `get_pixmap` method to render the PDF page as a matrix of pixels as specified by the variable `mat`\n    pix = pdf[n].get_pixmap(matrix=mat)\n    # Store image locally\n    key = f\"{n+1}.png\"\n    pix.save(key)\n    # Extract image metadata to be stored in MongoDB\n    temp[\"key\"] = key\n    temp[\"width\"] = pix.width\n    temp[\"height\"] = pix.height\n    docs.append(temp)","metadata":{"executionCancelledAt":null,"executionTime":5013,"lastExecutedAt":1752779921192,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"zoom = 3.0\n# Set image matrix dimensions\nmat = pymupdf.Matrix(zoom, zoom)\n# Iterate through the pages of the PDF\nfor n in tqdm(range(pdf.page_count)):\n    temp = {}\n    # Use the `get_pixmap` method to render the PDF page as a matrix of pixels as specified by the variable `mat`\n    pix = pdf[n].get_pixmap(matrix=mat)\n    # Store image locally\n    key = f\"{n+1}.png\"\n    pix.save(key)\n    # Extract image metadata to be stored in MongoDB\n    temp[\"key\"] = key\n    temp[\"width\"] = pix.width\n    temp[\"height\"] = pix.height\n    docs.append(temp)","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"id":"3e1dbd1b-4762-473d-9b40-2ad0573eacfb","cell_type":"code","execution_count":12,"outputs":[]},{"source":"# Ensure that all pages of the PDF were processed\nlen(docs)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1752779921244,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Ensure that all pages of the PDF were processed\nlen(docs)"},"id":"4deddeeb-cb65-42f7-b665-709ea5e305ad","cell_type":"code","execution_count":13,"outputs":[]},{"source":"# Preview a document \ndocs[0]","metadata":{"executionCancelledAt":null,"executionTime":46,"lastExecutedAt":1752779921290,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Preview a document \ndocs[0]"},"id":"e023646f-6fa4-490e-989f-c589a8aad441","cell_type":"code","execution_count":14,"outputs":[]},{"source":"# Task 4: Add embeddings to the MongoDB documents","metadata":{},"id":"902f78d6-9a8a-4c1f-8fae-1d1f99e96ef4","cell_type":"markdown"},{"source":"from voyageai import Client\nfrom PIL import Image","metadata":{"executionCancelledAt":null,"executionTime":305,"lastExecutedAt":1752779924700,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from voyageai import Client\nfrom PIL import Image"},"id":"3ade2e6c-db25-4ab0-960b-2f8523f19c4c","cell_type":"code","execution_count":15,"outputs":[]},{"source":"# Initialize the Voyage AI client\nvoyageai_client = Client()","metadata":{"executionCancelledAt":null,"executionTime":9,"lastExecutedAt":1752779926151,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Initialize the Voyage AI client\nvoyageai_client = Client()"},"id":"da99c12a-491e-4ae9-b890-196087dfc078","cell_type":"code","execution_count":16,"outputs":[]},{"source":"# Helper function to generate embeddings using the voyage-multimodal-3 model\n# input_type can be one of `document` or `query` depending on what you are embedding \ndef get_embedding(data, input_type): \n    embedding = voyageai_client.multimodal_embed(\n        inputs=[[data]], model=\"voyage-multimodal-3\", input_type=input_type\n    ).embeddings[0]\n    return embedding","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1752779927168,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Helper function to generate embeddings using the voyage-multimodal-3 model\n# input_type can be one of `document` or `query` depending on what you are embedding \ndef get_embedding(data, input_type): \n    embedding = voyageai_client.multimodal_embed(\n        inputs=[[data]], model=\"voyage-multimodal-3\", input_type=input_type\n    ).embeddings[0]\n    return embedding"},"id":"dae56b63-dde1-4cc1-878d-140615d05718","cell_type":"code","execution_count":17,"outputs":[]},{"source":"embedded_docs = []\nfor doc in tqdm(docs):\n    # Open the image from file\n    img = Image.open(f\"{doc['key']}\")\n    # Add the embeddings to the document\n    doc[\"embedding\"] = get_embedding(img, \"document\")\n    embedded_docs.append(doc)","metadata":{"executionCancelledAt":null,"executionTime":26168,"lastExecutedAt":1752779954903,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"embedded_docs = []\nfor doc in tqdm(docs):\n    # Open the image from file\n    img = Image.open(f\"{doc['key']}\")\n    # Add the embeddings to the document\n    doc[\"embedding\"] = get_embedding(img, \"document\")\n    embedded_docs.append(doc)","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"id":"fa77de8b-81b4-4e2b-aec2-49e66f6d177d","cell_type":"code","execution_count":18,"outputs":[]},{"source":"# Preview a document\nembedded_docs[0]","metadata":{"executionCancelledAt":null,"executionTime":18,"lastExecutedAt":1752779958192,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Preview a document\nembedded_docs[0]","collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"id":"33151ec9-b0d0-4bff-a518-8c8149766be2","cell_type":"code","execution_count":19,"outputs":[]},{"source":"# Task 5: Write documents into a MongoDB collection","metadata":{},"id":"00bf0874-89da-48c1-9e1a-3fbbc9e17276","cell_type":"markdown"},{"source":"from pymongo import MongoClient","metadata":{"executionCancelledAt":null,"executionTime":88,"lastExecutedAt":1752779962780,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from pymongo import MongoClient"},"id":"fe3bad8d-803c-4046-a074-42b8d29a695c","cell_type":"code","execution_count":20,"outputs":[]},{"source":"# Create a MongoDB client\nmongodb_client = MongoClient(\n    MONGODB_URI, appname=\"devrel.datacamp_codealong\"\n)\n# Check connection to the cluster\nmongodb_client.admin.command(\"ping\")","metadata":{"executionCancelledAt":null,"executionTime":810,"lastExecutedAt":1752779965303,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a MongoDB client\nmongodb_client = MongoClient(\n    MONGODB_URI, appname=\"devrel.datacamp_codealong\"\n)\n# Check connection to the cluster\nmongodb_client.admin.command(\"ping\")"},"id":"d94b03ce-055e-4f55-8faa-5749584f9269","cell_type":"code","execution_count":21,"outputs":[]},{"source":"# Database name\nDB_NAME = \"datacamp\"\n# Name of the collection to insert documents into\nCOLLECTION_NAME = \"multimodal_codealong\"","metadata":{"executionCancelledAt":null,"executionTime":7,"lastExecutedAt":1752779966362,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Database name\nDB_NAME = \"datacamp\"\n# Name of the collection to insert documents into\nCOLLECTION_NAME = \"multimodal_codealong\""},"id":"17b4346b-fc41-4e56-8f23-c4ad259346dd","cell_type":"code","execution_count":22,"outputs":[]},{"source":"# Connect to the MongoDB collection\ncollection = mongodb_client[DB_NAME][COLLECTION_NAME]","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1752779967320,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Connect to the MongoDB collection\ncollection = mongodb_client[DB_NAME][COLLECTION_NAME]"},"id":"df80160a-6d34-4d81-a0b3-f6b82ac9ee62","cell_type":"code","execution_count":23,"outputs":[]},{"source":"# Delete existing documents from the collection\ncollection.delete_many({})","metadata":{"executionCancelledAt":null,"executionTime":80,"lastExecutedAt":1752779968531,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Delete existing documents from the collection\ncollection.delete_many({})"},"id":"113cf7aa-6607-4798-be88-7887ce70cdee","cell_type":"code","execution_count":24,"outputs":[]},{"source":"# Insert the embedded documents into the collection\ncollection.insert_many(embedded_docs)","metadata":{"executionCancelledAt":null,"executionTime":347,"lastExecutedAt":1752779970783,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Insert the embedded documents into the collection\ncollection.insert_many(embedded_docs)"},"id":"a48948a9-a947-4e0c-b9e3-a3002837ba88","cell_type":"code","execution_count":25,"outputs":[]},{"source":"# Task 6: Create a vector search index","metadata":{},"id":"56d0a0f8-2f28-4afc-9ce1-51892ea7ccd0","cell_type":"markdown"},{"source":"VS_INDEX_NAME = \"vector_index\"","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1752779973923,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"VS_INDEX_NAME = \"vector_index\""},"id":"9a099c2e-dcd6-416a-8edc-9a779adaaa05","cell_type":"code","execution_count":26,"outputs":[]},{"source":"# Create vector index definition specifying:\n# path: Path to the embeddings field\n# numDimensions: Number of embedding dimensions- depends on the embedding model used\n# similarity: Similarity metric. One of cosine, euclidean, dotProduct.\nmodel = {\n    \"name\": VS_INDEX_NAME,\n    \"type\": \"vectorSearch\",\n    \"definition\": {\n        \"fields\": [\n            {\n                \"type\": \"vector\",\n                \"path\": \"embedding\",\n                \"numDimensions\": 1024,\n                \"similarity\": \"cosine\",\n            }\n        ]\n    },\n}","metadata":{"executionCancelledAt":null,"executionTime":9,"lastExecutedAt":1752779975071,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create vector index definition specifying:\n# path: Path to the embeddings field\n# numDimensions: Number of embedding dimensions- depends on the embedding model used\n# similarity: Similarity metric. One of cosine, euclidean, dotProduct.\nmodel = {\n    \"name\": VS_INDEX_NAME,\n    \"type\": \"vectorSearch\",\n    \"definition\": {\n        \"fields\": [\n            {\n                \"type\": \"vector\",\n                \"path\": \"embedding\",\n                \"numDimensions\": 1024,\n                \"similarity\": \"cosine\",\n            }\n        ]\n    },\n}"},"id":"2dd14561-773a-4d74-b806-a87e56f7cf2d","cell_type":"code","execution_count":27,"outputs":[]},{"source":"# Create a vector search index with the above `model` for the `collection` collection\ncollection.create_search_index(model=model)","metadata":{"executionCancelledAt":null,"executionTime":358,"lastExecutedAt":1752771603379,"lastExecutedByKernel":"c3bd6dcc-44f8-42f0-9728-0204811c9e0f","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a vector search index with the above `model` for the `collection` collection\ncollection.create_search_index(model=model)"},"id":"8787523d-f2e6-466a-94e6-357d8eab4a2b","cell_type":"code","execution_count":27,"outputs":[]},{"source":"**NOTE**: Before proceeding further, navigate to the MongoDB Atlas UI and ensure that the vector search index is in READY status.","metadata":{},"id":"476767b4-76f0-46d0-92d0-8d502b344e41","cell_type":"markdown"},{"source":"Now that we have prepared our mixed-modality document for search and retrieval, let's build an AI agent that can retrieve information from text, images and tables in this document to help answer questions about the document. The workflow for the agent looks as follows:\n\n![agent_worflow_1.png](agent_workflow_1.png)\n\n![agent_worflow_2.png](agent_workflow_2.png)\n\n- 1: User sends a query to the agent\n- 2: Agent forwards the user query to an LLM\n- 3: LLM decides whether to call a tool or not. If yes, the LLM also extracts the arguments for the tool call.\n- 4: The agent calls the tool selected by the LLM using the arguments generated by the LLM\n- 5: If the vector search tool is called, it returns the IDs of the screenshots\n- 6-7: The agent obtains the relevant screenshots from blob storage using the IDs\n- 8: The user query and the retrieved screenshots are passed to the LLM\n- 9: The LLM generates an answer using the provided context\n- 10: The answer is forwarded to the user ","metadata":{},"id":"f159873a-d3db-46b3-932b-0efbcfeb8ac0","cell_type":"markdown"},{"source":"# Task 8: Create agent tools\n\nTools for agents are simply functions written in a programming language of your choice!","metadata":{},"id":"e20f6abd-2faf-4742-8f5c-fee04430920f","cell_type":"markdown"},{"source":"def get_information_for_question_answering(user_query: str):\n    \"\"\"\n    Retrieve information using vector search to answer a user query.\n\n    Args:\n    user_query (str): The user's query string.\n    \"\"\"\n    # Get query embedding using the `get_embedding` helper function\n    query_embedding = get_embedding(user_query, \"query\")\n    # Define the VS aggregation pipeline\n    pipeline = [\n        {\n            \"$vectorSearch\": {\n                \"index\": VS_INDEX_NAME,\n                \"queryVector\": query_embedding,\n                \"path\": f\"embedding\",\n                \"numCandidates\": 150,\n                \"limit\": 2,\n            }\n        },\n        {\n            \"$project\": {\n                \"_id\": 0,\n                \"key\": 1,\n                \"width\": 1,\n                \"height\": 1,\n                \"score\": {\"$meta\": \"vectorSearchScore\"},\n            }\n        },\n    ]\n\n    # Execute the aggregation `pipeline` against the `collection` collection and store the results in `results`\n    results = collection.aggregate(pipeline)\n    # Get images from local storage\n    keys = [result[\"key\"] for result in results]\n    print(f\"Keys: {keys}\")\n    return keys","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1752779978580,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def get_information_for_question_answering(user_query: str):\n    \"\"\"\n    Retrieve information using vector search to answer a user query.\n\n    Args:\n    user_query (str): The user's query string.\n    \"\"\"\n    # Get query embedding using the `get_embedding` helper function\n    query_embedding = get_embedding(user_query, \"query\")\n    # Define the VS aggregation pipeline\n    pipeline = [\n        {\n            \"$vectorSearch\": {\n                \"index\": VS_INDEX_NAME,\n                \"queryVector\": query_embedding,\n                \"path\": f\"embedding\",\n                \"numCandidates\": 150,\n                \"limit\": 2,\n            }\n        },\n        {\n            \"$project\": {\n                \"_id\": 0,\n                \"key\": 1,\n                \"width\": 1,\n                \"height\": 1,\n                \"score\": {\"$meta\": \"vectorSearchScore\"},\n            }\n        },\n    ]\n\n    # Execute the aggregation `pipeline` against the `collection` collection and store the results in `results`\n    results = collection.aggregate(pipeline)\n    # Get images from local storage\n    keys = [result[\"key\"] for result in results]\n    print(f\"Keys: {keys}\")\n    return keys"},"id":"9a5d9bd4-3d8b-4e74-bae1-a858052f9d35","cell_type":"code","execution_count":28,"outputs":[]},{"source":"# Test the vector search tool with an example query\nget_information_for_question_answering(\"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\")","metadata":{"executionCancelledAt":null,"executionTime":230,"lastExecutedAt":1752779981390,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Test the vector search tool with an example query\nget_information_for_question_answering(\"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\")","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"id":"27f6b0b5-d6b3-41f6-8a98-fcfce35f6d31","cell_type":"code","execution_count":29,"outputs":[]},{"source":"In addition to defining the tool itself, you need to create function schemas to help the LLM identify what tools to use and the arguments for the tool calls.","metadata":{},"id":"fd4a8f66-a2bc-4db5-9052-a7c3e8197cae","cell_type":"markdown"},{"source":"# Define the function declaration for the `get_information_for_question_answering` function\nget_information_for_question_answering_declaration = {\n    \"name\": \"get_information_for_question_answering\",\n    \"description\": \"Retrieve information using vector search to answer a user query.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"user_query\": {\n                \"type\": \"string\",\n                \"description\": \"Query string to use for vector search\",\n            }\n        },\n        \"required\": [\"user_query\"],\n    },\n}","metadata":{"executionCancelledAt":null,"executionTime":9,"lastExecutedAt":1752779986029,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Define the function declaration for the `get_information_for_question_answering` function\nget_information_for_question_answering_declaration = {\n    \"name\": \"get_information_for_question_answering\",\n    \"description\": \"Retrieve information using vector search to answer a user query.\",\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"user_query\": {\n                \"type\": \"string\",\n                \"description\": \"Query string to use for vector search\",\n            }\n        },\n        \"required\": [\"user_query\"],\n    },\n}"},"id":"ede95bf9-92c1-48ac-a4d0-2c688a7cc4e6","cell_type":"code","execution_count":30,"outputs":[]},{"source":"# Task 9: Instantiate the Gemini LLM and client","metadata":{},"id":"7968f654-0fb3-43d5-be1f-67ad0209a6c9","cell_type":"markdown"},{"source":"from google import genai\nfrom google.genai import types","metadata":{"executionCancelledAt":null,"executionTime":1847,"lastExecutedAt":1752779989616,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from google import genai\nfrom google.genai import types"},"id":"8ceec8d0-6fca-4220-b527-1d33d29e1dfc","cell_type":"code","execution_count":31,"outputs":[]},{"source":"# Gemini LLM to use\nLLM = \"gemini-2.5-flash\"","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1752779989666,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Gemini LLM to use\nLLM = \"gemini-2.5-flash\""},"id":"3b25f513-380a-456a-b6d6-9f8afb1d0834","cell_type":"code","execution_count":32,"outputs":[]},{"source":"# Instantiate the Gemini client\ngemini_client = genai.Client(api_key=GEMINI_API_KEY)","metadata":{"executionCancelledAt":null,"executionTime":181,"lastExecutedAt":1752779990023,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Instantiate the Gemini client\ngemini_client = genai.Client(api_key=GEMINI_API_KEY)"},"id":"310c7556-10ad-439e-812b-08a33c21c861","cell_type":"code","execution_count":33,"outputs":[]},{"source":"# Task 10: Create generation config\n\nFor Gemini models, the generation config specifies parameters for the LLM generation.\n\nThis might look different for a different LLM providers. Check the API documentation for the provider you are using to understand how to specify these parameters.","metadata":{},"id":"a6c7de8a-1491-4213-ae1f-1107be28fe56","cell_type":"markdown"},{"source":"# Create a generation config with the `get_information_for_question_answering_declaration` function declaration and `temperature` set to 0.0\ntools = types.Tool(\n    function_declarations=[get_information_for_question_answering_declaration]\n)\ntools_config = types.GenerateContentConfig(tools=[tools], temperature=0.0)","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1752779992480,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a generation config with the `get_information_for_question_answering_declaration` function declaration and `temperature` set to 0.0\ntools = types.Tool(\n    function_declarations=[get_information_for_question_answering_declaration]\n)\ntools_config = types.GenerateContentConfig(tools=[tools], temperature=0.0)"},"id":"ebdcca6e-9ea3-4eb5-b6d8-5dc818926a79","cell_type":"code","execution_count":34,"outputs":[]},{"source":"# Task 11: Define a function for tool selection\n\nTool selection in the context of AI agents involves using LLMs to identify which tool to call and the arguments for the tool call. Note that the LLM doesn't execute the tool call. This needs to be implemented in the agent's code, as you will see in Task 12.","metadata":{},"id":"c7a36bc3-62b0-4e5e-92eb-8285b10b7071","cell_type":"markdown"},{"source":"from google.genai.types import FunctionCall","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1752779994330,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from google.genai.types import FunctionCall"},"id":"cddc20e9-1248-4e3d-bdaf-9097df6102e8","cell_type":"code","execution_count":35,"outputs":[]},{"source":"# Function that uses an LLM to decide if a tool needs to be called\ndef select_tool(messages):\n    system_prompt = [\n        (\n            \"You're an AI assistant. Based on the given information, decide which tool to use.\"\n            \"If the user is asking to explain an image, don't call any tools unless that would help you better explain the image.\"\n            \"Here is the provided information:\\n\"\n        )\n    ]\n    # Input to the LLM\n    contents = system_prompt + messages\n    # Use the `gemini_client`, `LLM`, `contents` and `tools_config` defined previously to generate a response using Gemini\n    response = gemini_client.models.generate_content(\n        model=LLM, contents=contents, config=tools_config\n    )\n    # Extract and return the function call from the response\n    return response.candidates[0].content.parts[0].function_call","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1752779995380,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Function that uses an LLM to decide if a tool needs to be called\ndef select_tool(messages):\n    system_prompt = [\n        (\n            \"You're an AI assistant. Based on the given information, decide which tool to use.\"\n            \"If the user is asking to explain an image, don't call any tools unless that would help you better explain the image.\"\n            \"Here is the provided information:\\n\"\n        )\n    ]\n    # Input to the LLM\n    contents = system_prompt + messages\n    # Use the `gemini_client`, `LLM`, `contents` and `tools_config` defined previously to generate a response using Gemini\n    response = gemini_client.models.generate_content(\n        model=LLM, contents=contents, config=tools_config\n    )\n    # Extract and return the function call from the response\n    return response.candidates[0].content.parts[0].function_call"},"id":"598a7f31-b28e-4278-99c1-d68741466e5f","cell_type":"code","execution_count":36,"outputs":[]},{"source":"# Task 12: Define a function to execute tools and generate responses","metadata":{},"id":"bcc8e62c-6b38-44b8-9236-f21710d724e8","cell_type":"markdown"},{"source":"def generate_answer(user_query, images):\n    # Use the `select_tool` function above to get the tool config\n    tool_call = select_tool([user_query])\n    # If a tool call is found and the name is `get_information_for_question_answering`\n    if (\n        tool_call is not None\n        and tool_call.name == \"get_information_for_question_answering\"\n    ):\n        print(f\"Agent: Calling tool: {tool_call.name}\")\n        # Call the tool with the arguments extracted by the LLM\n        tool_images = get_information_for_question_answering(**tool_call.args)\n        # Add images returned by the tool to the list of input images if any\n        images.extend(tool_images)\n\n    system_prompt = f\"Answer the questions based on the provided context only. If the context is not sufficient, say I DON'T KNOW. DO NOT use any other information to answer the question.\"\n    # Pass the system prompt, user query, and content retrieved using vector search (`images`) as input to the LLM\n    contents = [system_prompt] + [user_query] + [Image.open(image) for image in images]\n\n    # Get the response from the LLM\n    response = gemini_client.models.generate_content(\n        model=LLM,\n        contents=contents,\n        config=types.GenerateContentConfig(temperature=0.0),\n    )\n    answer = response.text\n    return answer","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1752779997209,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def generate_answer(user_query, images):\n    # Use the `select_tool` function above to get the tool config\n    tool_call = select_tool([user_query])\n    # If a tool call is found and the name is `get_information_for_question_answering`\n    if (\n        tool_call is not None\n        and tool_call.name == \"get_information_for_question_answering\"\n    ):\n        print(f\"Agent: Calling tool: {tool_call.name}\")\n        # Call the tool with the arguments extracted by the LLM\n        tool_images = get_information_for_question_answering(**tool_call.args)\n        # Add images returned by the tool to the list of input images if any\n        images.extend(tool_images)\n\n    system_prompt = f\"Answer the questions based on the provided context only. If the context is not sufficient, say I DON'T KNOW. DO NOT use any other information to answer the question.\"\n    # Pass the system prompt, user query, and content retrieved using vector search (`images`) as input to the LLM\n    contents = [system_prompt] + [user_query] + [Image.open(image) for image in images]\n\n    # Get the response from the LLM\n    response = gemini_client.models.generate_content(\n        model=LLM,\n        contents=contents,\n        config=types.GenerateContentConfig(temperature=0.0),\n    )\n    answer = response.text\n    return answer"},"id":"08d82d77-a372-448d-bca9-1ab6ddbe919d","cell_type":"code","execution_count":37,"outputs":[]},{"source":"# Task 13: Execute the agent","metadata":{},"id":"5de045bf-ffec-437d-a556-d4c63bc44428","cell_type":"markdown"},{"source":"def execute_agent(user_query, images=[]):\n    # Use the `generate_answer` function to generate responses \n    response = generate_answer(user_query, images)\n    print(\"Agent:\", response)","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1752779998817,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def execute_agent(user_query, images=[]):\n    # Use the `generate_answer` function to generate responses \n    response = generate_answer(user_query, images)\n    print(\"Agent:\", response)","outputsMetadata":{"0":{"height":248,"type":"stream"}}},"id":"c08736d7-7fa1-4eab-bc3d-34ba4e812358","cell_type":"code","execution_count":38,"outputs":[]},{"source":"# Test the agent with a text input\nexecute_agent(\"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\")","metadata":{"executionCancelledAt":null,"executionTime":11828,"lastExecutedAt":1752780011747,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Test the agent with a text input\nexecute_agent(\"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\")","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"id":"c3fcf1c4-d283-4372-93f3-34d8bd02d3e8","cell_type":"code","execution_count":39,"outputs":[]},{"source":"# Test the agent with an image input\nexecute_agent(\"Explain the graph in this image:\", [\"test.png\"])","metadata":{"executionCancelledAt":null,"executionTime":6133,"lastExecutedAt":1752780019824,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Test the agent with an image input\nexecute_agent(\"Explain the graph in this image:\", [\"test.png\"])","outputsMetadata":{"0":{"height":500,"type":"stream"}}},"id":"ed56a1cc-c7f0-4d17-ba00-529284bbe347","cell_type":"code","execution_count":40,"outputs":[]},{"source":"# Task 14: Add memory to the agent\n\nMemory is important for agents to learn from past interactions and maintain consistent, coherent dialogue across conversations. \n\nIn this codealong, we will use MongoDB to persist and manage short-term memory of the agent. The memory management workflow of our agent looks as follows:\n\n![memory_mgmt_1.png](memory_mgmt_1.png)\n\n![memory_mgmt_2.png](memory_mgmt_2.png)\n\n- 1: Get the session ID that the user question belongs to\n- 2: Retrieve session history from MongoDB using the session ID\n- 3: Pass the session history as additional context to the LLM\n- 4: Add the current question to the session history\n- 5: Add the LLM's answer to the session history\n","metadata":{},"id":"a77f89c7-917f-4661-b9de-40839876f177","cell_type":"markdown"},{"source":"from datetime import datetime","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1752780023742,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from datetime import datetime"},"id":"fecc66f6-3f8d-47d9-b18e-abf5de0c6d00","cell_type":"code","execution_count":41,"outputs":[]},{"source":"# Instantiate the history collection\nhistory_collection = mongodb_client[DB_NAME][\"history\"]","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1752780024702,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Instantiate the history collection\nhistory_collection = mongodb_client[DB_NAME][\"history\"]"},"id":"28652ccf-8b67-4288-a442-0e4c40bcb53f","cell_type":"code","execution_count":42,"outputs":[]},{"source":"# Create an index on `session_id` on the `history_collection` collection\nhistory_collection.create_index(\"session_id\")","metadata":{"executionCancelledAt":null,"executionTime":73,"lastExecutedAt":1752780025731,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create an index on `session_id` on the `history_collection` collection\nhistory_collection.create_index(\"session_id\")"},"id":"bfaa6baf-30ba-4aa5-a293-053550702ad3","cell_type":"code","execution_count":43,"outputs":[]},{"source":"def store_chat_message(session_id, role, type, content):\n    # Create a message object with `session_id`, `role`, `type`, `content` and `timestamp` fields\n    # `timestamp` should be set to the current timestamp\n    message = {\n        \"session_id\": session_id,\n        \"role\": role,\n        \"type\": type,\n        \"content\": content,\n        \"timestamp\": datetime.now(),\n    }\n    # Insert the `message` into the `history_collection` collection\n    history_collection.insert_one(message)","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1752780026636,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def store_chat_message(session_id, role, type, content):\n    # Create a message object with `session_id`, `role`, `type`, `content` and `timestamp` fields\n    # `timestamp` should be set to the current timestamp\n    message = {\n        \"session_id\": session_id,\n        \"role\": role,\n        \"type\": type,\n        \"content\": content,\n        \"timestamp\": datetime.now(),\n    }\n    # Insert the `message` into the `history_collection` collection\n    history_collection.insert_one(message)"},"id":"91eed39c-a90c-4b8a-a150-ddad3be86e58","cell_type":"code","execution_count":44,"outputs":[]},{"source":"def retrieve_session_history(session_id):\n    # Query the `history_collection` collection for documents where the \"session_id\" field has the value of the input `session_id`\n    # Sort the results in increasing order of the values in `timestamp` field\n    cursor = history_collection.find({\"session_id\": session_id}).sort(\"timestamp\", 1)\n    messages = []\n    if cursor:\n        for msg in cursor:\n            # If the message type is `text`, append the content as is\n            if msg[\"type\"] == \"text\":\n                messages.append(msg[\"content\"])\n            # If message type is `image`, open the image\n            elif msg[\"type\"] == \"image\":\n                messages.append(Image.open(msg[\"content\"]))\n    return messages","metadata":{"executionCancelledAt":null,"executionTime":9,"lastExecutedAt":1752780027687,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def retrieve_session_history(session_id):\n    # Query the `history_collection` collection for documents where the \"session_id\" field has the value of the input `session_id`\n    # Sort the results in increasing order of the values in `timestamp` field\n    cursor = history_collection.find({\"session_id\": session_id}).sort(\"timestamp\", 1)\n    messages = []\n    if cursor:\n        for msg in cursor:\n            # If the message type is `text`, append the content as is\n            if msg[\"type\"] == \"text\":\n                messages.append(msg[\"content\"])\n            # If message type is `image`, open the image\n            elif msg[\"type\"] == \"image\":\n                messages.append(Image.open(msg[\"content\"]))\n    return messages"},"id":"2d54259a-98bf-4adb-8d52-f689d773754a","cell_type":"code","execution_count":45,"outputs":[]},{"source":"def generate_answer(session_id, user_query, images):\n    # Retrieve past conversation history for the specified `session_id` using the `retrieve_session_history` method\n    print(\"Retrieving chat history...\")\n    history = retrieve_session_history(session_id)\n    # Determine if any additional tools need to be called\n    tool_call = select_tool(history + [user_query])\n    if (\n        tool_call is not None\n        and tool_call.name == \"get_information_for_question_answering\"\n    ):\n        print(f\"Agent: Calling tool: {tool_call.name}\")\n        # Call the tool with the arguments extracted by the LLM\n        tool_images = get_information_for_question_answering(**tool_call.args)\n        # Add images returned by the tool to the list of input images if any\n        images.extend(tool_images)\n\n    # Pass the system prompt, conversation history, user query and retrieved context (`images`) to the LLM to generate an answer\n    system_prompt = f\"Answer the questions based on the provided context only. If the context is not sufficient, say I DON'T KNOW. DO NOT use any other information to answer the question.\"\n    contents = (\n        [system_prompt]\n        + history\n        + [user_query]\n        + [Image.open(image) for image in images]\n    )\n    # Get a response from the LLM\n    response = gemini_client.models.generate_content(\n        model=LLM,\n        contents=contents,\n        config=types.GenerateContentConfig(temperature=0.0),\n    )\n    answer = response.text\n    # Write the current user query to memory using the `store_chat_message` function\n    # The `role` for user queries is \"user\" and `type` is \"text\"\n    print(\"Updating chat history...\")\n    store_chat_message(session_id, \"user\", \"text\", user_query)\n    # Write the filepaths of input/retrieved images to memory using the store_chat_message` function\n    # The `role` for these is \"user\" and `type` is \"image\"\n    for image in images:\n        store_chat_message(session_id, \"user\", \"image\", image)\n    # Write the LLM generated response to memory\n    # The `role` for these is \"agent\" and `type` is \"text\"\n    store_chat_message(session_id, \"agent\", \"text\", answer)\n    return answer","metadata":{"executionCancelledAt":null,"executionTime":15,"lastExecutedAt":1752780029489,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def generate_answer(session_id, user_query, images):\n    # Retrieve past conversation history for the specified `session_id` using the `retrieve_session_history` method\n    print(\"Retrieving chat history...\")\n    history = retrieve_session_history(session_id)\n    # Determine if any additional tools need to be called\n    tool_call = select_tool(history + [user_query])\n    if (\n        tool_call is not None\n        and tool_call.name == \"get_information_for_question_answering\"\n    ):\n        print(f\"Agent: Calling tool: {tool_call.name}\")\n        # Call the tool with the arguments extracted by the LLM\n        tool_images = get_information_for_question_answering(**tool_call.args)\n        # Add images returned by the tool to the list of input images if any\n        images.extend(tool_images)\n\n    # Pass the system prompt, conversation history, user query and retrieved context (`images`) to the LLM to generate an answer\n    system_prompt = f\"Answer the questions based on the provided context only. If the context is not sufficient, say I DON'T KNOW. DO NOT use any other information to answer the question.\"\n    contents = (\n        [system_prompt]\n        + history\n        + [user_query]\n        + [Image.open(image) for image in images]\n    )\n    # Get a response from the LLM\n    response = gemini_client.models.generate_content(\n        model=LLM,\n        contents=contents,\n        config=types.GenerateContentConfig(temperature=0.0),\n    )\n    answer = response.text\n    # Write the current user query to memory using the `store_chat_message` function\n    # The `role` for user queries is \"user\" and `type` is \"text\"\n    print(\"Updating chat history...\")\n    store_chat_message(session_id, \"user\", \"text\", user_query)\n    # Write the filepaths of input/retrieved images to memory using the store_chat_message` function\n    # The `role` for these is \"user\" and `type` is \"image\"\n    for image in images:\n        store_chat_message(session_id, \"user\", \"image\", image)\n    # Write the LLM generated response to memory\n    # The `role` for these is \"agent\" and `type` is \"text\"\n    store_chat_message(session_id, \"agent\", \"text\", answer)\n    return answer"},"id":"65b4722e-f7a2-465a-8e40-7be6f32bdb0b","cell_type":"code","execution_count":46,"outputs":[]},{"source":"def execute_agent_with_memory(session_id, user_query, images=[]):\n    response = generate_answer(session_id, user_query, images)\n    print(\"Agent:\", response)","metadata":{"executionCancelledAt":null,"executionTime":8,"lastExecutedAt":1752780032276,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def execute_agent_with_memory(session_id, user_query, images=[]):\n    response = generate_answer(session_id, user_query, images)\n    print(\"Agent:\", response)"},"id":"51c1bb7f-f7c2-40f1-90ed-17830c0117c5","cell_type":"code","execution_count":47,"outputs":[]},{"source":"execute_agent_with_memory(\n    \"1\",\n    \"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\",\n)","metadata":{"executionCancelledAt":null,"executionTime":9104,"lastExecutedAt":1752780042233,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"execute_agent_with_memory(\n    \"5\",\n    \"What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?\",\n)","outputsMetadata":{"0":{"height":80,"type":"stream"}}},"id":"c5f23321-a9a1-4cd4-a261-b905b6ff5ce7","cell_type":"code","execution_count":48,"outputs":[]},{"source":"# Follow-up question to make sure chat history is being used.\nexecute_agent_with_memory(\n    \"1\",\n    \"What question did I just ask you?\",\n)","metadata":{"executionCancelledAt":null,"executionTime":6118,"lastExecutedAt":1752780048351,"lastExecutedByKernel":"6b5a481f-3eb8-483d-81f2-571eb9426e27","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Follow-up question to make sure chat history is being used.\nexecute_agent_with_memory(\n    \"5\",\n    \"What question did I just ask you?\",\n)","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":80,"type":"stream"}}},"id":"bf322023-cab5-4be5-a7bc-ec913b19d7ff","cell_type":"code","execution_count":49,"outputs":[]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}