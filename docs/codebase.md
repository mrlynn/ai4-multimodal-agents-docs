# 10-intro.mdx

```mdx
---
sidebar_position: 10
---

# üìò Introduction to Multimodal PDF Agents with n8n

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

|Workshop goals|Build a multimodal AI agent that processes PDFs using n8n, MongoDB Atlas, and Voyage AI|
|-|-|
|What you'll learn|Visual workflow automation, vector search, multimodal embeddings, AI agent patterns|
|Prerequisites|n8n installed, MongoDB Atlas account, Voyage AI API key|
|Time to complete|2 hours|

<SlideRecap 
  title="From Presentation to Practice"
  items={[
    {
      icon: "üèóÔ∏è",
      title: "Architecture Overview",
      description: "We covered how n8n, MongoDB Atlas, and Voyage AI work together"
    },
    {
      icon: "üéØ", 
      title: "Learning Objectives",
      description: "Build a production-ready multimodal PDF processing system"
    },
    {
      icon: "‚ö°",
      title: "Visual Workflows",
      description: "Why n8n's no-code approach makes AI development accessible"
    }
  ]}
  nextSection="Now we'll build this system step by step!"
/>

<ProgressTracker steps={[
  {
    title: "Environment Setup",
    description: "Set up Docker, n8n, and MongoDB Atlas",
    timeEstimate: "15 minutes",
    difficulty: "beginner"
  },
  {
    title: "PDF Processing Workflow", 
    description: "Build workflow to extract and process PDF content",
    timeEstimate: "25 minutes",
    difficulty: "intermediate"
  },
  {
    title: "Vector Search Implementation",
    description: "Configure MongoDB Atlas Vector Search",
    timeEstimate: "20 minutes", 
    difficulty: "intermediate"
  },
  {
    title: "AI Agent Creation",
    description: "Build intelligent agent with tool calling",
    timeEstimate: "30 minutes",
    difficulty: "advanced"
  },
  {
    title: "Memory & Context",
    description: "Add conversation history and context management",
    timeEstimate: "20 minutes",
    difficulty: "advanced"
  },
  {
    title: "Production Deployment",
    description: "Deploy and scale your multimodal agent",
    timeEstimate: "15 minutes",
    difficulty: "intermediate"
  }
]} />

## üéØ What We're Building

A production-ready system that:
- **Ingests PDFs** and extracts both text and images
- **Generates multimodal embeddings** using Voyage AI
- **Stores in MongoDB Atlas** with vector search capabilities
- **Provides an AI agent** with tool calling via Gemini 2.0
- **Maintains conversation memory** for context
- **Implements ReAct pattern** for intelligent reasoning

## üèóÔ∏è Architecture Overview

\`\`\`mermaid
graph LR
    A[PDF Input] --> B[n8n Workflow]
    B --> C[Page Extraction]
    C --> D[Voyage AI Embeddings]
    D --> E[MongoDB Atlas]
    F[User Query] --> G[AI Agent]
    G --> H[Vector Search]
    H --> E
    G --> I[Gemini Response]
\`\`\`

## üöÄ Why This Approach?

### Visual Development
- No code complexity
- See data flow in real-time
- Easy debugging and modification

### Production Ready
- Built-in error handling
- Scalable architecture
- Enterprise features

### Best-in-Class Tools
- **MongoDB Atlas**: Leading vector database
- **Voyage AI**: State-of-the-art multimodal embeddings
- **n8n**: Powerful workflow automation
- **Gemini 2.0**: Advanced AI capabilities

## üöÄ Choose Your Setup Method

<WorkshopExercise 
  title="Setup Your Development Environment" 
  difficulty="beginner"
  timeEstimate="15 minutes"
  objectives={[
    "Choose between Codespaces or local development",
    "Set up all required services and tools", 
    "Verify your environment is working correctly"
  ]}
>

<Tabs>
  <TabItem value="cloud" label="‚òÅÔ∏è GitHub Codespaces (Fastest)" default>

<ExerciseStep stepNumber="1" title="Launch GitHub Codespaces">

**Zero Installation Required!** Everything runs in your browser:

<TerminalCommand 
  command="# Navigate to deployment repository"
  output="Opening: https://github.com/mrlynn/ai4-multimodal-agents-n8n"
/>

1. Go to [deployment repository](https://github.com/mrlynn/ai4-multimodal-agents-n8n)
2. Click **Code** ‚Üí **Codespaces** ‚Üí **Create codespace**  
3. Wait for automatic setup (3-5 minutes first time)
4. All services start automatically

**Requirements:**
- GitHub account (free)
- Web browser
- Internet connection

</ExerciseStep>

<ExerciseValidation 
  title="Verify Codespaces Setup"
  checks={[
    {
      id: "codespace_running",
      description: "Codespace is running and accessible",
      hint: "Check that VS Code opens in your browser"
    },
    {
      id: "ports_forwarded", 
      description: "Ports are automatically forwarded (check Ports tab)",
      hint: "Look for ports 5678 and 3000 in the Ports panel"
    },
    {
      id: "services_healthy",
      description: "All Docker services are running (green status)",
      hint: "Run 'docker-compose ps' in the terminal"
    }
  ]}
/>

üëâ **[Continue with Codespaces Setup](./github-codespaces)**

  </TabItem>
  <TabItem value="local" label="üê≥ Local Docker">

<ExerciseStep stepNumber="1" title="Clone and Start Services">

**Full control** - Work offline, customize everything:

<CodeBlock language="bash" title="Local Setup Commands">
{`# Clone workshop deployment repo
git clone https://github.com/mrlynn/ai4-multimodal-agents-n8n.git
cd multimodal-pdf-agent-n8n

# Start services
cp .env.example .env
docker-compose up -d`}
</CodeBlock>

<ServiceTester 
  serviceName="n8n"
  testUrl="http://localhost:5678"
  testData={{}}
/>

**Requirements:**
- Docker Desktop
- Git
- 8GB RAM

</ExerciseStep>

<ExerciseValidation 
  title="Verify Local Setup"
  checks={[
    {
      id: "docker_running",
      description: "Docker Desktop is running",
      hint: "Check Docker icon in system tray"
    },
    {
      id: "services_up",
      description: "All services are up (docker-compose ps shows 'Up')",
      hint: "Run 'docker-compose ps' to check status"
    },
    {
      id: "n8n_accessible",
      description: "n8n is accessible at http://localhost:5678",
      hint: "Open the URL in your browser"
    },
    {
      id: "atlas_configured",
      description: "MongoDB Atlas connection is configured",
      hint: "Open the URL in your browser"
    }
  ]}
/>

üëâ **[Continue with Local Setup](./prerequisites)**

  </TabItem>
</Tabs>

</WorkshopExercise>

## üìã Required for Both Methods

‚úÖ **MongoDB Atlas account** (free tier) - [Sign up](https://www.mongodb.com/try)  
‚úÖ **Google AI Studio API key** for Gemini (optional) - [Get key](https://makersuite.google.com/app/apikey)  
‚úÖ **Sample PDFs** for testing  

## üó∫Ô∏è Workshop Journey

1. **Environment Setup** ‚Üí Get all tools configured
2. **PDF Processing** ‚Üí Build ingestion workflow
3. **Vector Search** ‚Üí Implement similarity search
4. **AI Agent** ‚Üí Create intelligent assistant
5. **Memory & Context** ‚Üí Add conversation history
6. **ReAct Pattern** ‚Üí Implement reasoning loops
7. **Production Deploy** ‚Üí Scale your solution

## üí° Key Concepts

### Multimodal Embeddings
Voyage AI's `voyage-multimodal-3` model processes both text and images into a unified vector space, enabling semantic search across different content types.

### Vector Search
MongoDB Atlas Vector Search provides efficient similarity search at scale, perfect for RAG (Retrieval Augmented Generation) applications.

### Visual Workflows
n8n's node-based interface makes complex integrations intuitive, with built-in error handling and monitoring.

### Agent Patterns
We'll implement industry-standard patterns like tool calling and ReAct (Reasoning and Acting) for intelligent agent behavior.

## üéì Learning Outcomes

By the end of this workshop, you'll be able to:

1. **Design and build** visual AI workflows in n8n
2. **Process multimodal content** from PDFs
3. **Implement vector search** with MongoDB Atlas
4. **Create AI agents** with tool calling capabilities
5. **Add memory** to maintain context
6. **Deploy to production** with confidence

## üêç Beyond Visual Workflows

While this workshop focuses on n8n's visual approach, we also cover **Python integration options** for developers who want programmatic control:

- **MongoDB Multimodal Search Library** - High-level Python SDK
- **Direct PyMongo Integration** - Maximum performance and control  
- **Hybrid Approaches** - Combine n8n orchestration with Python logic
- **LangChain Integration** - Advanced RAG patterns

These approaches all use the same MongoDB Atlas and Voyage AI foundation you'll learn about in the visual workflows!

## üö¶ Ready to Start?

Let's begin by setting up your development environment and ensuring all prerequisites are in place.

[Get Started with Prerequisites ‚Üí](./prerequisites)
```

# 15-github-codespaces.mdx

```mdx
---
sidebar_position: 15
---

# üåê GitHub Codespaces: Zero-Install Workshop

The **fastest way** to start this workshop - no local setup required!

<InstructorNotes 
  timing="GitHub Codespaces Setup (5-8 minutes)"
  notes={[
    "Codespaces can take 3-5 minutes on first launch - warn attendees",
    "Free GitHub accounts get 60 hours/month - more than enough for workshop",
    "Port forwarding is automatic but ports must be made 'Public' for sharing",
    "Some corporate networks block Codespaces - have local setup as backup",
    "Attendees often close browser tab accidentally - show how to reconnect"
  ]}
  tips={[
    "Have attendees start Codespace creation at the very beginning",
    "Show how to check build progress in the terminal",
    "Demonstrate port visibility settings in the Ports tab",
    "Explain that Codespaces persists between sessions for 30 days",
    "Mention Codespaces works on tablets/Chromebooks unlike local setup"
  ]}
/>

## üöÄ What is GitHub Codespaces?

GitHub Codespaces provides a complete, cloud-based development environment that runs in your browser. For this workshop, it means:

- ‚úÖ **No local installation** - Works on any device with a browser
- ‚úÖ **Pre-configured environment** - All services start automatically
- ‚úÖ **Consistent experience** - Same setup for all participants
- ‚úÖ **Built-in VS Code** - Full IDE in your browser

## üéØ Quick Start (30 Seconds!)

### 1. Launch Codespace

<Screenshot src="/img/codespaces-button.png" alt="GitHub Codespaces Button" />

1. Go to the [workshop deployment repository](https://github.com/mrlynn/ai4-multimodal-agents-n8n)

<QRCodeAccess 
  url="https://github.com/mrlynn/ai4-multimodal-agents-n8n"
  title="Workshop Repository"
/>

2. Click the green **Code** button
3. Select **Codespaces** tab
4. Click **Create codespace on main**

### 2. Wait for Setup (3-5 minutes)

The first time takes a few minutes as it:
- Builds the Docker environment
- Installs all dependencies
- Starts all services
- Configures port forwarding

<Screenshot src="/img/codespaces-building.png" alt="Codespaces Building" />

### 3. Services Auto-Start

Once ready, you'll see:
- Terminal showing service status
- Ports automatically forwarded
- Pop-up notifications for available services

## üåü Accessing Workshop Services

### Automatic Port Forwarding

GitHub Codespaces automatically forwards all ports with secure URLs:

| Service | Local Port | Codespaces URL | Access |
|---------|------------|----------------|--------|
| **n8n Editor** | 5678 | `https://<name>-5678.preview.app.github.dev` | Auto-opens |
| **MongoDB Atlas** | Cloud | Connection via Atlas UI | Configure Atlas |
| **Documentation** | 3000 | `https://<name>-3000.preview.app.github.dev` | Auto-opens |

### Finding Your URLs

1. Click **Ports** tab in the terminal panel
2. Hover over any port
3. Click the globe icon to open

<Screenshot src="/img/codespaces-ports.png" alt="Codespaces Ports Panel" />

## üìã Codespaces vs Local Docker

| Feature | GitHub Codespaces | Local Docker |
|---------|------------------|--------------|
| **Setup Time** | 30 seconds | 10-15 minutes |
| **Prerequisites** | GitHub account | Docker Desktop |
| **Performance** | Cloud-based | Local machine |
| **Cost** | Free tier (60 hrs/month) | Free |
| **Persistence** | While active | Permanent |
| **Offline Work** | ‚ùå | ‚úÖ |

## üîß Codespaces Features

### Pre-installed Extensions

Your Codespace includes:
- MongoDB for VS Code
- Python & Pylance
- YAML support
- Docker tools
- Prettier formatter

### Terminal Commands

All workshop commands work identically:

\`\`\`bash
# Check service status
docker-compose ps

# View logs
docker-compose logs -f n8n

# Restart services
docker-compose restart

# Run documentation
npm start
\`\`\`

### File Persistence

- Files are saved automatically
- Changes persist between sessions
- Export important workflows before stopping

## üí° Pro Tips for Codespaces

### 1. Keep It Running

Codespaces auto-suspend after 30 minutes of inactivity:
- Keep the browser tab open
- Interact periodically
- Or adjust timeout in settings

### 2. Resource Management

Default Codespace has:
- 2 CPU cores
- 4 GB RAM
- 32 GB storage

For better performance:
1. Go to repository settings
2. Select "Change machine type"
3. Choose 4-core option

### 3. Multiple Windows

- **Split Terminal**: Drag terminal tabs
- **Preview Side-by-Side**: Right-click tabs ‚Üí "Split Right"
- **Multiple Browser Tabs**: Open services in separate tabs

## üõ†Ô∏è Troubleshooting Codespaces

### Services Not Starting

\`\`\`bash
# Restart all services
docker-compose down
docker-compose up -d

# Check logs
docker-compose logs
\`\`\`

### Port Not Accessible

1. Check **Ports** tab
2. Ensure port visibility is "Public" or "Private"
3. Click globe icon to open

### Performance Issues

- Close unused browser tabs
- Upgrade to larger machine type
- Stop unused services

## üîí Security & Privacy

### Port Visibility

By default, ports are private to you. The workshop sets:
- Documentation (3000): Public
- n8n (5678): Private
- MongoDB Atlas: Cloud-based

### Data Security

- All data stays in your Codespace
- Encrypted in transit
- Deleted when Codespace is deleted

## üìä Managing Your Codespace

### View Active Codespaces

1. Go to [github.com/codespaces](https://github.com/codespaces)
2. See all your active Codespaces
3. Monitor usage and billing

### Stop vs Delete

- **Stop**: Preserves state, can resume later
- **Delete**: Removes everything permanently

\`\`\`bash
# Export workflows before deleting
docker-compose exec n8n n8n export:workflow --all
\`\`\`

## üéì Workshop-Specific Setup

**Important**: Launch Codespaces from the [deployment repository](https://github.com/mrlynn/ai4-multimodal-agents-n8n), not this documentation site!

Our devcontainer automatically:

1. **Installs Dependencies**
   - Node.js 22 (for n8n)
   - Python 3.11 (for AI libraries)
   - MongoDB tools

2. **Starts Services**
   - MongoDB with auth
   - n8n with MongoDB connection
   - MongoDB Atlas setup

3. **Configures Environment**
   - Copies `.env.example` to `.env`
   - Sets up workshop directories
   - Initializes sample data

## üöÄ Ready to Start?

With Codespaces, you can:
- Start the workshop in seconds
- Work from any device
- Share your environment easily
- Focus on learning, not setup

### Next Steps

1. ‚úÖ Launch Codespace from [deployment repository](https://github.com/mrlynn/ai4-multimodal-agents-n8n)
2. ‚úÖ Services are accessible
3. ‚úÖ Ready to build workflows

Continue to [n8n First Run ‚Üí](./n8n-first-run)

---

**Tip**: Bookmark your Codespace URL for quick access!
```

# 20-prerequisites.mdx

```mdx
---
sidebar_position: 20
---

# üõ†Ô∏è Prerequisites & Setup

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Before we start building, let's get all the tools and accounts you'll need.

<InstructorNotes 
  timing="Prerequisites Setup (10-15 minutes)"
  notes={[
    "This is where most workshops get derailed - allocate extra time",
    "Docker Desktop issues are the #1 problem on Windows/Mac",
    "Have attendees start MongoDB Atlas signup early (email verification takes time)",
    "Codespaces users often forget to make ports public",
    "Local setup varies significantly - recommend Codespaces for mixed groups"
  ]}
  tips={[
    "Poll the room: who has Docker already running?",
    "Share your screen showing successful docker-compose ps output",
    "Have backup plan ready for attendees with setup issues",
    "Consider pairing experienced developers with beginners",
    "Emphasize that Codespaces 'just works' if local setup fails"
  ]}
/>

## üê≥ Docker-Based Workshop Setup

This workshop uses Docker to provide a consistent, pre-configured environment. No need to install n8n or worry about Node.js versions!

### ‚ö†Ô∏è Prerequisites Overview

**Required:**
- Docker Desktop (includes Docker and Docker Compose)
- Git (for cloning the workshop)
- Text editor (VS Code recommended)
- MongoDB Atlas account (free tier)

**Optional:**
- Voyage AI API key (workshop provides serverless endpoint)
- Google AI Studio API key (for Gemini features)

## üöÄ Setup Instructions

<Tabs>
  <TabItem value="codespaces" label="GitHub Codespaces" default>

### Instant Cloud Setup

1. **Open in Codespaces**
   \`\`\`
   Repository ‚Üí Code ‚Üí Codespaces ‚Üí Create codespace on main
   \`\`\`

2. **Automatic Configuration**
   - All services start automatically
   - Ports are forwarded with secure URLs
   - VS Code opens in your browser

3. **Access Services**
   - Click **Ports** tab in terminal
   - Click globe icon next to each service
   - Or wait for pop-up notifications

<Screenshot src="/img/n8n-3.png" alt="Ensure Ports are Public, and Open in Browser" />


üéâ **That's it! Skip to [MongoDB Atlas Setup](./mongodb-atlas-setup)**

  </TabItem>
  <TabItem value="mac" label="macOS (Local)">

### Step 1: Install Docker Desktop

\`\`\`bash
# Download Docker Desktop for Mac
# Visit: https://www.docker.com/products/docker-desktop/
# Choose Apple Silicon (M1/M2) or Intel version

# After installation, verify Docker is running:
docker --version
docker-compose --version
\`\`\`

### Step 2: Clone the Workshop

\`\`\`bash
# Clone the workshop deployment repository
git clone https://github.com/mrlynn/ai4-multimodal-agents-n8n.git
cd multimodal-pdf-agent-n8n

# Copy environment template
cp .env.example .env
\`\`\`

### Step 3: Start Workshop Services

\`\`\`bash
# Start all services with one command
docker-compose up -d

# Services will be available at:
# n8n: http://localhost:5678
# MongoDB Atlas: Ready for vector search
# Documentation: Run 'npm start' separately
\`\`\`

:::tip Why Docker?
We use Docker to avoid Node.js version conflicts and provide a consistent environment. Everything is pre-configured and ready to go!
:::

  </TabItem>
  <TabItem value="windows" label="Windows">

### Step 1: Install Docker Desktop

\`\`\`powershell
# Download Docker Desktop for Windows
# Visit: https://www.docker.com/products/docker-desktop/
# Download the installer and run it

# After installation, verify Docker is running:
docker --version
docker-compose --version
\`\`\`

### Step 2: Clone the Workshop

\`\`\`powershell
# Clone the workshop deployment repository
git clone https://github.com/mrlynn/ai4-multimodal-agents-n8n.git
cd multimodal-pdf-agent-n8n

# Copy environment template
copy .env.example .env
\`\`\`

### Step 3: Start Workshop Services

\`\`\`powershell
# Start all services with one command
docker-compose up -d

# Services will be available at:
# n8n: http://localhost:5678
# MongoDB Atlas: Ready for vector search
# Documentation: Run 'npm start' separately
\`\`\`

:::tip Windows Docker Setup
- Ensure WSL 2 is enabled for better performance
- Docker Desktop handles this automatically during installation
- You may need to restart after installing Docker Desktop
:::

  </TabItem>
  <TabItem value="linux" label="Linux">

### Step 1: Install Docker

**Ubuntu/Debian:**
\`\`\`bash
# Update package index
sudo apt update

# Install Docker
sudo apt install docker.io docker-compose

# Add your user to docker group (logout/login required)
sudo usermod -aG docker $USER

# Verify installation
docker --version
docker-compose --version
\`\`\`

**Fedora/RHEL/CentOS:**
\`\`\`bash
# Install Docker
sudo dnf install docker docker-compose

# Start Docker service
sudo systemctl start docker
sudo systemctl enable docker

# Add user to docker group
sudo usermod -aG docker $USER
\`\`\`

### Step 2: Clone the Workshop

\`\`\`bash
# Clone the workshop deployment repository
git clone https://github.com/mrlynn/ai4-multimodal-agents-n8n.git
cd multimodal-pdf-agent-n8n

# Copy environment template
cp .env.example .env
\`\`\`

### Step 3: Start Workshop Services

\`\`\`bash
# Start all services with one command
docker-compose up -d

# Services will be available at:
# n8n: http://localhost:5678
# MongoDB Atlas: Ready for vector search
# Documentation: Run 'npm start' separately
\`\`\`

:::tip Linux Docker Permissions
If you get "permission denied" errors, logout and login again after adding yourself to the docker group.
:::

  </TabItem>
</Tabs>

## üê≥ Understanding the Docker Setup

Our workshop uses Docker Compose to manage all services:

### Services Included

1. **n8n** (port 5678)
   - Pre-configured workflow automation platform
   - Connected to MongoDB for data persistence
   - All necessary environment variables set

2. **MongoDB Atlas** (cloud database)
   - Database for storing workflows and vector data
   - Pre-configured with authentication
   - Health checks enabled

3. **MongoDB Atlas** (cloud database)
   - Vector database for storing workflows and embeddings
   - Managed service with built-in security
   - Vector Search indexes for semantic similarity

### Starting Services

\`\`\`bash
# Start all services
docker-compose up -d

# Check service status
docker-compose ps

# View logs
docker-compose logs -f

# Stop services
docker-compose down
\`\`\`

:::tip Container Management
The `-d` flag runs containers in detached mode (background). Remove it to see logs in real-time.
:::

## üì¶ Workshop Files & Structure

\`\`\`
multimodal-pdf-agent-n8n/           # Deployment repository
‚îú‚îÄ‚îÄ docker-compose.yml              # Service definitions
‚îú‚îÄ‚îÄ .env.example                   # Environment template
‚îú‚îÄ‚îÄ .env                           # Your configuration (create from .env.example)
‚îú‚îÄ‚îÄ .devcontainer/                 # GitHub Codespaces configuration
‚îú‚îÄ‚îÄ init/
‚îÇ   ‚îú‚îÄ‚îÄ scripts/                  # Setup and utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ workflows/                # Sample n8n workflows
‚îÇ   ‚îî‚îÄ‚îÄ sample-data/              # Test PDFs
‚îú‚îÄ‚îÄ files/                        # File uploads directory
‚îú‚îÄ‚îÄ scripts/                      # Testing and management scripts
‚îî‚îÄ‚îÄ workshop-embedding-api/       # Serverless embedding endpoint
\`\`\`

## üîê n8n Account Setup

### Community Edition Features

When you first access n8n:

1. **Create Account**: Enter your email address
2. **Community Edition**: You start with the free community edition
3. **Unlock Features**: Get advanced features free forever!

### Getting Your Free License Key

1. Go to **Settings** ‚Üí **Usage and Plan**
2. Click **"Unlock selected paid features for free"**
3. Features you'll get:
    - ‚úÖ Workflow history
    - ‚úÖ Advanced debugging
    - ‚úÖ Execution search and tagging
    - ‚úÖ Folder organization
4. Click **"Send me a free license key"**
5. Check your email immediately
6. Click the activation link in the email

:::info Activation Tip
The activation link in your email automatically applies your license key - no need to copy/paste!
:::

## Understanding n8n's Interface

### Key Areas to Explore

1. **Workflows**: Your automation canvas
2. **Credentials**: Store API keys securely
3. **Executions**: View run history
4. **Settings**: Personal, instance, and community nodes

### Community Nodes

n8n has a rich ecosystem of community-contributed nodes:

1. Go to **Settings** ‚Üí **Community Nodes**
2. Click **"Browse"** to see available nodes
3. Popular nodes include:
    - DeepSeek AI integration
    - MCP (Model Context Protocol) nodes
    - Custom database connectors
    - Specialized AI tools

:::tip For Our Workshop
We'll use the built-in MongoDB and HTTP Request nodes, but feel free to explore community nodes for extended functionality!
:::

## üîß Docker Troubleshooting

<details>
<summary>Docker services won't start</summary>

\`\`\`bash
# Check if Docker is running
docker info

# If not, start Docker Desktop (GUI) or:
sudo systemctl start docker  # Linux

# Check for errors in logs
docker-compose logs

# Rebuild if needed
docker-compose build --no-cache
docker-compose up -d
\`\`\`

</details>

<details>
<summary>Port 5678 already in use</summary>

Another service is using the port:

<Tabs>
  <TabItem value="mac" label="macOS" default>

\`\`\`bash
# Find what's using port 5678
lsof -i :5678

# Kill the process (replace PID with actual process ID)
kill -9 <PID>

# Or use different port:
n8n start --port=5679
\`\`\`

</TabItem>
<TabItem value="windows" label="Windows">

\`\`\`powershell
# Find what's using port 5678
netstat -ano | findstr :5678

# Kill the process (replace PID with actual process ID)
taskkill /PID <PID> /F

# Or use different port:
n8n start --port=5679
\`\`\`

</TabItem>
<TabItem value="linux" label="Linux">

\`\`\`bash
# Find what's using port 5678
lsof -i :5678
# or
netstat -tulpn | grep :5678

# Kill the process (replace PID with actual process ID)
kill -9 <PID>

# Or use different port:
n8n start --port=5679
\`\`\`

</TabItem>
</Tabs>

</details>

<details>
<summary>Permission/Installation Errors</summary>

<Tabs>
  <TabItem value="mac" label="macOS" default>

\`\`\`bash
# Don't use: brew install n8n (package may be broken)
# Use npm instead: npm install -g n8n

# If permission errors:
sudo npm install -g n8n

# Better: Configure npm to avoid sudo
mkdir ~/.npm-global
npm config set prefix '~/.npm-global'
echo 'export PATH=~/.npm-global/bin:$PATH' >> ~/.zshrc
source ~/.zshrc
npm install -g n8n
\`\`\`

</TabItem>
<TabItem value="windows" label="Windows">

\`\`\`powershell
# Make sure you're running PowerShell as Administrator

# If execution policy errors:
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser

# Clear npm cache if needed:
npm cache clean --force

# Reinstall n8n:
npm install -g n8n
\`\`\`

</TabItem>
<TabItem value="linux" label="Linux">

\`\`\`bash
# If permission errors with system npm:
sudo npm install -g n8n

# Better: Use nvm (no sudo needed):
# First install nvm if not already installed
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash
source ~/.bashrc
nvm install 22
nvm use 22
npm install -g n8n
\`\`\`

</TabItem>
</Tabs>

</details>

## MongoDB Atlas Setup

### Create Your Atlas Account

1. Go to [MongoDB Atlas](https://www.mongodb.com/try)
2. Sign up with your email
3. Create a new project
4. Build your first cluster (free tier)

### Configure Database Access

1. **Database Access**: Create a user
   - Username: `n8n-user`
   - Password: Generate secure password
   - Database User Privileges: Read and write to any database

2. **Network Access**: Add IP addresses
   - Add your current IP
   - For development: Allow access from anywhere (0.0.0.0/0)

### Get Connection String

1. Click **Connect** on your cluster
2. Choose **Connect your application**
3. Copy the connection string
4. Replace `<password>` with your user password

## Workshop Embedding Service

For this workshop, we'll use a serverless endpoint that handles embeddings without requiring API key management.

### Test the Workshop Service

\`\`\`bash
curl -X POST "https://ai4-workshop-embeddings.vercel.app/api/embed" \
  -H "Content-Type: application/json" \
  -d '{
    "input": "Test document",
    "input_type": "document"
  }'
\`\`\`

This endpoint provides:
- ‚úÖ Voyage AI multimodal embeddings
- ‚úÖ No API key required
- ‚úÖ Workshop-optimized rate limits
- ‚úÖ Built-in error handling

## ‚úÖ Pre-Workshop Checklist

Complete these BEFORE the workshop:

- [ ] Install Docker Desktop
- [ ] Clone workshop deployment repository
- [ ] Copy `.env.example` to `.env`
- [ ] Run `docker-compose up -d`
- [ ] Verify n8n service at http://localhost:5678
- [ ] Create MongoDB Atlas account (free tier)
- [ ] Have a test PDF ready

## üö® Common Docker Issues & Solutions

1. **Docker not running**: Start Docker Desktop first
2. **Port conflicts**: Check `lsof -i :5678` (Mac/Linux) or `netstat -ano | findstr :5678` (Windows)
3. **Permission denied**: Add user to docker group (Linux)
4. **Services won't start**: Check `docker-compose logs` for errors
5. **Can't connect to services**: Ensure using `http://` not `https://`

## üéØ Quick Reference

| Service | URL | Purpose |
|---------|-----|---------|
| n8n Editor | http://localhost:5678 | Build workflows |
| Documentation | http://localhost:3000 | Workshop guide |
| MongoDB Atlas | Via connection string | Vector database |

Ready to build your multimodal PDF agent! ‚Üí
```

# 25-n8n-first-run.mdx

```mdx
---
sidebar_position: 25
---

# üöÄ n8n First Run Experience

<StepIndicator current={1} total={6} titles={[
  "Environment Setup", 
  "n8n First Run", 
  "MongoDB Atlas", 
  "PDF Processing", 
  "Vector Search", 
  "AI Agent"
]} />

Let's walk through your first n8n experience and set up the foundation for our multimodal PDF agent.

<InstructorNotes 
  timing="n8n First Run section (10-15 minutes)"
  notes={[
    "This is often where attendees encounter their first issues",
    "Common problems: Docker not running, ports in use, browser cache",
    "Have attendees confirm they can access n8n before proceeding",
    "License activation email sometimes goes to spam folder"
  ]}
  tips={[
    "Share your own n8n screen to show what success looks like",
    "Encourage attendees to help each other with setup issues",
    "Keep a few minutes buffer for troubleshooting",
    "Consider setting up a backup n8n instance for demos"
  ]}
/>

<WorkshopExercise 
  title="Launch and Configure n8n" 
  difficulty="beginner"
  timeEstimate="10 minutes"
  objectives={[
    "Verify n8n is running and accessible",
    "Complete initial setup wizard",
    "Unlock premium features for free",
    "Explore the n8n interface"
  ]}
>

<ExerciseStep stepNumber="1" title="Verify n8n Service">

With our Docker setup, n8n should already be running! Let's verify:

<TerminalCommand 
  command="docker-compose ps"
  output={`NAME           IMAGE              STATUS       PORTS
n8n-workshop   n8nio/n8n:latest   Up 2 mins    0.0.0.0:5678->5678/tcp
n8n-workshop   n8nio/n8n:latest   Up 2 mins    0.0.0.0:5678->5678/tcp`}
/>

If n8n isn't running, start the services:

<CodeBlock language="bash" title="Start All Services">
{`# Start all services
docker-compose up -d

# Wait 10-15 seconds for services to initialize
# Then check logs
docker-compose logs n8n`}
</CodeBlock>

<ServiceTester 
  serviceName="n8n" 
  testUrl="http://localhost:5678"
  testData={{}}
/>

</ExerciseStep>

<ExerciseStep stepNumber="2" title="Complete Initial Setup Wizard">

Navigate to `http://localhost:5678` in your browser:

**Account Creation:**
- Enter your email address
- Create a secure password  
- You're automatically on the Community Edition (perfect for learning!)

<Screenshot src="/img/n8n-1.png" alt="n8n Initial Setup" />

</ExerciseStep>

<ExerciseStep stepNumber="3" title="Unlock Premium Features (Free!)">

Don't skip this step! Get premium features at no cost:

<InteractiveDemo 
  title="Unlock n8n Premium Features"
  description="Follow these steps to get advanced features free forever"
  steps={[
    {
      title: "Navigate to Settings",
      content: (
        <div>
          <p>1. Click the **Settings** (gear icon) in the left sidebar</p>
          <p>2. Look for the **Usage and Plan** section</p>
        </div>
      ),
      timeEstimate: "1 minute"
    },
    {
      title: "Request Free License",
      content: (
        <div>
          <p>3. Click **"Get paid features for free forever"**</p>
          <p>4. Enter your email when prompted</p>
          <p>5. Check your email inbox immediately</p>
        </div>
      ),
      timeEstimate: "2 minutes"
    },
    {
      title: "Activate License",
      content: (
        <div>
          <p>6. Click the activation link in your email</p>
          <p>7. Your license will be automatically applied</p>
          <p>‚úÖ You now have access to premium features!</p>
        </div>
      ),
      timeEstimate: "1 minute"
    }
  ]}
/>

**Premium Features You'll Get:**
- ‚úÖ Workflow history and versioning
- ‚úÖ Advanced debugging tools
- ‚úÖ Execution search and tagging
- ‚úÖ Folder organization
- ‚úÖ Enhanced performance monitoring

</ExerciseStep>

<ExerciseStep stepNumber="4" title="Configure Your Workspace">

Let's customize n8n for our workshop:

**Theme Selection:**
- Navigate to **Settings** ‚Üí **Personal**
- Choose Light or Dark theme (instructor prefers light theme üòé)

**Community Nodes (Optional):**
- Go to **Settings** ‚Üí **Community Nodes**
- Browse available extensions
- We'll install specific nodes as needed during the workshop

</ExerciseStep>

<ExerciseValidation 
  title="Verify n8n Setup Complete"
  checks={[
    {
      id: "account_created",
      description: "Successfully created n8n account and logged in",
      hint: "You should see the n8n dashboard with your email in the top right"
    },
    {
      id: "license_activated",
      description: "Premium features unlocked (check Settings ‚Üí Usage and Plan)",
      hint: "Look for 'Premium features activated' or similar message"
    },
    {
      id: "interface_accessible",
      description: "Can navigate through different sections (Workflows, Credentials, Settings)",
      hint: "Click through the left sidebar menu items"
    }
  ]}
/>

</WorkshopExercise>

## Understanding the n8n Interface

### Main Navigation

<Screenshot src="/img/n8n-1.png" alt="n8n Overview Dashboard" />

### Workflow Canvas

When you create a new workflow:

1. **Canvas**: Main work area
2. **Node Panel**: Click + to add nodes
3. **Node Search**: Type to find nodes
4. **Execution Panel**: See run results

<Screenshot src="/img/n8n-2.png" alt="n8n Overview Dashboard" />

## Your First Test

Let's verify everything works:

### 1. Create Test Workflow

1. Click **Workflows** ‚Üí **New**
2. You'll see a blank canvas with "Start" node

### 2. Add a Simple Node

1. Click the **+** button next to the trigger node
2. In the search field at the top, type "Set"
3. Look for **"Edit Fields (Set)"** option (has a pencil icon)
4. Click on it to add the node
5. The node will be automatically connected to your trigger
6. Configure the Set node:
   - Click on the Set node to open its settings
   - You'll see fields to add data
   - Add a field with:
     - Name: `test`
     - Value: `Hello n8n!`
   - Add another field:
     - Name: `timestamp`
     - Value: `{{ new Date().toISOString() }}`
    

### 3. Execute

1. Click **Execute Workflow**
2. You should see output with your test data

## Pro Tips from the Field

### üí° Installation Tips

Based on real experience:

1. **Always use Node 22** - v24 doesn't work
2. **Use npm, not Homebrew** for n8n install
3. **Activate free license immediately** for better features
4. **Start simple** - test basic nodes first

### üéØ Workflow Development Tips

1. **Name your workflows** clearly
2. **Use folders** to organize (with free license)
3. **Test incrementally** - node by node
4. **Save often** - use Ctrl/Cmd+S

### üîß Useful Settings

Enable these for better experience:

1. **Settings** ‚Üí **Workflow Defaults**
    - Save execution progress: ON
    - Save manual executions: ON
2. **Settings** ‚Üí **Personal**
    - Execution timeout: 5 minutes (for our workshop)

## Common First-Time Issues

### "Site can't be reached"

- Make sure n8n is running in terminal
- Check [http://localhost:5678](http://localhost:5678/) (not https)
- Verify no firewall blocking

### "Invalid Node Version"

\`\`\`bash
# Quick fix:
nvm use 22
n8n
\`\`\`

### "License Key Not Working"

- Click the link in email (don't copy/paste)
- Check spam folder
- Request new key if needed

## Next Steps

Now that n8n is running:

‚úÖ n8n installed and running  
‚úÖ Account created  
‚úÖ Free license activated  
‚úÖ Interface explored  
‚úÖ First test workflow created

Ready to build our PDF system! ‚Üí
```

# 30-mongodb-atlas-setup.mdx

```mdx
---
sidebar_position: 30
---

# üóÑÔ∏è MongoDB Atlas Setup for Vector Search

This workshop uses two MongoDB environments:
**Note**: MongoDB Vector Search is only available in MongoDB Atlas, not in local MongoDB instances.
- **MongoDB Atlas** for production vector search

<SlideRecap 
  title="Database Foundation for Vector Search"
  items={[
    {
      icon: "üóÑÔ∏è",
      title: "MongoDB Atlas Vector Search",
      description: "Why we chose MongoDB Atlas for production-grade vector similarity search"
    },
    {
      icon: "üîç", 
      title: "Vector Indexes & Embeddings",
      description: "How 1024-dimensional vectors enable semantic search across multimodal content"
    },
    {
      icon: "‚ö°",
      title: "Scalability & Performance",
      description: "From prototype to production with MongoDB's managed vector database"
    }
  ]}
  nextSection="Let's set up your vector database now!"
/>

<InstructorNotes 
  timing="MongoDB Atlas Setup (15-20 minutes)"
  notes={[
    "Atlas signup can take a few minutes - start this section early",
    "Credit card may be required but we'll stay within free tier limits",
    "Network access setup often confuses first-time users",
    "Connection string formatting is a common source of errors"
  ]}
  tips={[
    "Demo the Atlas setup process on screen while attendees follow",
    "Emphasize the importance of saving connection strings securely",
    "Show how to test connections before moving to workflows",
    "Have backup connection strings ready for troubleshooting"
  ]}
/>

## ‚òÅÔ∏è MongoDB Atlas Setup

MongoDB Atlas provides managed vector search capabilities that are essential for our multimodal PDF agent.

### MongoDB Atlas Access

\`\`\`bash
# MongoDB Atlas connection is managed through n8n credentials
# No local MongoDB shell access needed

# Connection string for n8n
mongodb+srv://<username>:<password>@<cluster>.mongodb.net/<database>
\`\`\`

### MongoDB Atlas Setup

Access MongoDB Atlas through the cloud console to configure vector search
- No authentication required
- View collections and documents
- Run queries and manage data

## ‚òÅÔ∏è MongoDB Atlas (Production)

## Create Your Vector Database

### 1. Atlas Account Setup

If you don't have an account yet:

1. Visit [MongoDB Atlas](https://www.mongodb.com/try?utm_campaign=devrel&utm_source=workshop&utm_medium=cta&utm_content=multimodal_pdf_agent_n8n&utm_term=michael.lynn)

<QRCodeAccess 
  url="https://www.mongodb.com/try?utm_campaign=devrel&utm_source=workshop&utm_medium=cta&utm_content=multimodal_pdf_agent_n8n&utm_term=michael.lynn"
  title="MongoDB Atlas Signup"
/>
2. Sign up with your email
3. Verify your email address
4. Complete the onboarding questions

### 2. Create a Cluster

1. Click **"Build a Database"**
2. Choose **M0 Free Tier** (perfect for our workshop)
3. Select your preferred cloud provider and region
4. Name your cluster: `multimodal-workshop`
5. Click **"Create"** and wait for deployment (~2 minutes)

:::tip Cluster Location
Choose a region close to you for better performance during the workshop.
:::

### 3. Database Configuration

Once your cluster is ready:

1. Click **"Browse Collections"**
2. Click **"Add My Own Data"**
3. Create database and collection:
   \`\`\`
   Database: multimodal_workshop
   Collection: pdf_documents
   \`\`\`
4. Click **"Create"**

## 4. Create Vector Search Index

Navigate to **Atlas Search** ‚Üí **Create Index**:

1. Click **"Create Search Index"**
2. Select **"JSON Editor"**
3. Choose **"Atlas Vector Search"** as the index type
4. Choose your database: `multimodal_workshop`
5. Choose your collection: `pdf_documents`
6. Name your index: `vector_index`
7. Paste this configuration:

\`\`\`json
{
  "fields": [
    {
      "type": "vector",
      "path": "embedding",
      "numDimensions": 1024,
      "similarity": "cosine"
    },
    {
      "type": "filter",
      "path": "filename"
    },
    {
      "type": "filter",
      "path": "page_number"
    },
    {
      "type": "filter",
      "path": "content_type"
    },
    {
      "type": "filter",
      "path": "metadata.source_url"
    }
  ]
}
\`\`\`

:::info Vector Search Configuration
- **Vector Field**: `embedding` with 1024 dimensions for Voyage AI's `voyage-multimodal-3` model
- **Similarity Function**: `cosine` for normalized vectors (recommended for most embedding models)
- **Filter Fields**: Additional fields for pre-filtering search results by filename, page, content type, and source
:::

### Understanding the Index Configuration

**Vector Field Configuration:**
- `type: "vector"` - Specifies this field contains vector embeddings
- `path: "embedding"` - The field name in your documents containing the vector
- `numDimensions: 1024` - Must match your embedding model's output dimensions
- `similarity: "cosine"` - Similarity function for comparing vectors

**Filter Field Configuration:**
- `type: "filter"` - Allows pre-filtering documents before vector search
- `path: "field_name"` - The document field to enable filtering on

**Available Similarity Functions:**
- `cosine` - Measures angle between vectors (recommended for normalized embeddings)
- `euclidean` - Measures distance between vector endpoints
- `dotProduct` - Like cosine but considers magnitude (requires normalized vectors)

### Alternative Index Configurations

#### Minimal Vector-Only Index
If you only need basic vector search without filtering:

\`\`\`json
{
  "fields": [
    {
      "type": "vector",
      "path": "embedding",
      "numDimensions": 1024,
      "similarity": "cosine"
    }
  ]
}
\`\`\`

#### Performance-Optimized Index with Quantization
For large-scale deployments, enable scalar quantization to reduce memory usage:

\`\`\`json
{
  "fields": [
    {
      "type": "vector",
      "path": "embedding",
      "numDimensions": 1024,
      "similarity": "cosine",
      "quantization": "scalar"
    },
    {
      "type": "filter",
      "path": "filename"
    }
  ]
}
\`\`\`

:::tip Performance Optimization
- **Scalar Quantization**: Reduces memory usage by ~75% with minimal accuracy loss
- **Filter Fields**: Only add fields you'll actually filter on to avoid unnecessary indexing overhead
- **Similarity Function**: Choose based on your embedding model's training method
:::

### Index Creation via MongoDB Drivers

You can also create the index programmatically using MongoDB drivers:

\`\`\`javascript
// Example using MongoDB Node.js driver
const indexDefinition = {
  fields: [
    {
      type: "vector",
      path: "embedding", 
      numDimensions: 1024,
      similarity: "cosine"
    }
  ]
};

await collection.createSearchIndex({
  name: "vector_index",
  type: "vectorSearch", 
  definition: indexDefinition
});
\`\`\`

## Connection in n8n

### 1. Get Your Connection String

1. In Atlas, click **"Connect"** on your cluster
2. Choose **"Connect your application"**
3. Select **"Node.js"** as driver
4. Copy the connection string:
   \`\`\`
   mongodb+srv://<username>:<password>@multimodal-workshop.xxxxx.mongodb.net/
   \`\`\`

### 2. Add MongoDB Credentials in n8n

1. In n8n, go to **Credentials** ‚Üí **New**
2. Search for **"MongoDB"**
3. Configure:
   - **Credential Name**: `MongoDB Atlas Workshop`
   - **Connection String**: Paste your connection string
   - **Database Name**: `multimodal_workshop`
   - Replace `<username>` and `<password>` with your actual credentials

### 3. Test Connection

Create a test workflow to verify:

1. Add a **Manual Trigger** node
2. Add a **MongoDB** node
3. Configure MongoDB node:
   - **Credential**: Select `MongoDB Atlas Workshop`
   - **Operation**: Find
   - **Collection**: `pdf_documents`
4. Execute the workflow

You should see an empty result (no documents yet) - this confirms the connection works!

## Document Schema

Our PDF documents will follow this schema:

\`\`\`javascript
{
  "_id": ObjectId("..."),
  "filename": "sample.pdf",
  "page_number": 1,
  "content_type": "image", // or "text"
  "embedding": [0.123, -0.456, ...], // 1024 dimensions
  "metadata": {
    "source_url": "https://example.com/sample.pdf",
    "processed_at": "2024-01-20T10:30:00Z",
    "total_pages": 10,
    "file_size": 2048576
  },
  "text_content": "Extracted text from page...", // if applicable
  "image_data": {
    "width": 1920,
    "height": 1080,
    "format": "png"
  }
}
\`\`\`

## Best Practices

### 1. Security

- **Never** commit connection strings to version control
- Use n8n's credential management system
- Enable IP whitelisting in production

### 2. Indexing

- Create indexes on frequently queried fields
- Monitor index performance in Atlas UI
- Consider compound indexes for complex queries

### 3. Data Management

- Set up automatic backups (available in Atlas)
- Monitor storage usage (512MB free tier limit)
- Implement data retention policies

## Troubleshooting

### Connection Issues

If you can't connect from n8n:

1. **Check Network Access**:
   - Atlas ‚Üí Security ‚Üí Network Access
   - Add your IP or `0.0.0.0/0` for development
   
2. **Verify Credentials**:
   - Ensure username/password are correct
   - Check database user has read/write permissions

3. **Connection String Format**:
   - Must include `mongodb+srv://` prefix
   - Replace placeholder values completely

### Index Creation Errors

- Ensure collection exists before creating index
- Check JSON syntax in index definition
- Verify field names match your schema

## üß† Knowledge Check

<Quiz 
  title="MongoDB Atlas & Vector Search Fundamentals"
  passingScore={75}
  questions={[
    {
      question: "What is the primary purpose of MongoDB Atlas Vector Search in our multimodal PDF agent?",
      options: [
        "To store PDF files directly in the database",
        "To enable semantic similarity search across document embeddings", 
        "To compress PDF documents for faster loading",
        "To convert PDFs to HTML format"
      ],
      correctAnswer: 1,
      explanation: "Vector Search enables semantic similarity search by comparing embedding vectors, allowing us to find relevant content based on meaning rather than exact keyword matches."
    },
    {
      question: "In our vector search index, what does 'numDimensions: 1024' specify?",
      options: [
        "The maximum number of documents that can be stored",
        "The size of each embedding vector from Voyage AI",
        "The number of PDF pages that can be processed",
        "The maximum file size in kilobytes"
      ],
      correctAnswer: 1,
      explanation: "numDimensions must match the embedding model's output size. Voyage AI's multimodal model produces 1024-dimensional vectors."
    },
    {
      question: "Why do we use cosine similarity for our vector search index?",
      options: [
        "It's the fastest similarity metric available",
        "It works well for high-dimensional embeddings and focuses on vector direction",
        "It requires less storage space than other metrics", 
        "It's the only metric supported by MongoDB Atlas"
      ],
      correctAnswer: 1,
      explanation: "Cosine similarity measures the angle between vectors, making it ideal for high-dimensional embeddings where direction matters more than magnitude."
    },
    {
      question: "What happens if you create a vector search index before the collection exists?",
      options: [
        "The index will be created successfully and wait for the collection",
        "MongoDB will automatically create the collection",
        "You'll get an error - the collection must exist first",
        "The index will be created in a different database"
      ],
      correctAnswer: 2,
      explanation: "Collections must exist before you can create search indexes on them. Always ensure your collection is created first."
    },
    {
      question: "What is the benefit of adding filter fields to a vector search index?",
      options: [
        "It makes the index smaller and faster to build",
        "It allows you to combine vector similarity with metadata filtering",
        "It automatically optimizes the embedding dimensions",
        "It enables real-time updates to the search results"
      ],
      correctAnswer: 1,
      explanation: "Filter fields enable hybrid search - you can find semantically similar content AND filter by metadata like filename, date, or document type."
    }
  ]}
/>

<QuickCheck 
  question="True or False: Scalar quantization in vector indexes reduces memory usage by approximately 75%?"
  options={["True", "False"]}
  correctAnswer={0}
  explanation="Scalar quantization reduces memory usage by ~75% with minimal accuracy loss, making it ideal for large-scale deployments."
/>

## Next Steps

With MongoDB Atlas configured:

‚úÖ Cluster created and running  
‚úÖ Database and collection ready  
‚úÖ Vector search index configured  
‚úÖ n8n connection established  
‚úÖ **Knowledge validated** with quiz completion

You're ready to start ingesting PDFs! Let's set up Voyage AI next.

[Continue to Voyage AI Setup ‚Üí](./voyage-ai-setup)
```

# 35-voyage-ai-setup.mdx

```mdx
---
sidebar_position: 35
---

# üö¢ Voyage AI Embedding Service Setup

Configure access to multimodal embeddings that process both text and images using our workshop serverless endpoint.

<InstructorNotes 
  timing="Voyage AI Setup (8-12 minutes)"
  notes={[
    "This section moves quickly since we provide the API endpoint",
    "Attendees often ask why we don't use their own Voyage API keys",
    "Common confusion: difference between voyage-2 and voyage-3 models",
    "The EmbeddingTester component is crucial for verifying connectivity",
    "Some attendees will want to explore other embedding providers"
  ]}
  tips={[
    "Demo the EmbeddingTester component live - show expected output format",
    "Explain workshop endpoint handles rate limiting and error handling",
    "Mention this serverless approach works for other embedding APIs too",
    "Keep this section brief - the real learning happens in workflows",
    "Have sample text ready for testing (complex multi-sentence examples work best)"
  ]}
/>

## Workshop Serverless Endpoint

For this workshop, we'll use a pre-configured serverless endpoint that handles Voyage AI embeddings without requiring you to manage API keys.

### 1. Endpoint Configuration

**Workshop Embedding Service URL**: `https://workshop-embedding-api.vercel.app/api/embed`

This endpoint provides:
- ‚úÖ Voyage AI `voyage-3` model access
- ‚úÖ 1024-dimensional embeddings
- ‚úÖ Support for text inputs
- ‚úÖ Rate limiting and error handling
- ‚úÖ No API key management required

### 2. Using in n8n HTTP Request Nodes

The workshop embedding API is designed to work directly with n8n's **HTTP Request** node without requiring special credentials:

**HTTP Request Node Configuration:**
1. **Method**: `POST`
2. **URL**: `https://workshop-embedding-api.vercel.app/api/embed`
3. **Authentication**: None (leave empty)
4. **Send Headers**: Toggle ON
5. **Header Parameters**: 
   - Name: `Content-Type`
   - Value: `application/json`
6. **Send Body**: Toggle ON
7. **Body Content Type**: `JSON`
8. **Specify Body**: Select `Using JSON`
9. **JSON**: Enter this exactly:
   \`\`\`json
   {
     "text": "actual text to embed here",
     "model": "voyage-3"
   }
   \`\`\`

   For dynamic content from previous nodes:
   \`\`\`json
   {
     "text": {{ JSON.stringify($json.textContent) }},
     "model": "voyage-3"
   }
   \`\`\`

:::warning Required Field
The `text` field is required! Make sure to include the actual text you want to embed. In workflows, this usually comes from a previous node using `{{ $json.textContent }}` or similar.
:::

### 3. Interactive API Testing

Test the embedding API directly from this documentation before building your n8n workflow:

<EmbeddingTester />

### 4. Testing in n8n

To test if the API is working in n8n, create a simple workflow:

1. **Manual Trigger** node
2. **Set** node with test data:
   \`\`\`json
   {
     "textContent": "This is a test document about artificial intelligence and machine learning."
   }
   \`\`\`
3. **HTTP Request** node configured as above
4. **Execute** the workflow

**Expected Response:**
\`\`\`json
{
  "embeddings": [[0.123, -0.456, ...]], // 1024-dimensional array
  "model": "voyage-3",
  "usage": {
    "total_tokens": 12
  }
}
\`\`\`

### 5. Troubleshooting

**Error: "Invalid character in header content"**
- This indicates an issue with the API key configuration on the server
- The instructor needs to check the Vercel environment variables
- Ensure no extra spaces or special characters in the API key

**Error: "The service was not able to process your request"**
- Check that the `text` field contains actual text (not empty)
- Ensure the JSON is properly formatted without extra quotes
- Verify the API URL is correct

**Test with cURL:**
\`\`\`bash
curl -X POST https://workshop-embedding-api.vercel.app/api/embed \
  -H "Content-Type: application/json" \
  -d '{"text": "test embedding", "model": "voyage-3"}'
\`\`\`

### 6. Alternative: Mock Embeddings for Testing

If the API is temporarily unavailable, you can use a **Code** node in n8n to generate mock embeddings:

\`\`\`javascript
// Mock embedding generator
const text = $json.textContent || "test";
const embedding = new Array(1024).fill(0).map((_, i) => 
  Math.sin(i * 0.1) * Math.cos(text.length * 0.01)
);

return [{
  json: {
    embeddings: [embedding],
    model: "mock-voyage-3",
    usage: { total_tokens: text.split(' ').length }
  }
}];
\`\`\`

:::tip No Credentials Required
The workshop API is publicly accessible and doesn't require authentication. Simply use the HTTP Request node with the configuration above.
:::

:::tip Workshop Benefits
Using the serverless endpoint means:
- No need to sign up for Voyage AI accounts
- No API key management
- Consistent workshop experience
- Built-in rate limiting and error handling
:::

## Understanding Multimodal Embeddings

### What Makes Voyage AI Special?

The `voyage-multimodal-3` model:
- **Unified Vector Space**: Text and images in the same 1024-dimensional space
- **Semantic Understanding**: Finds conceptual similarities across modalities
- **Optimized for RAG**: Built specifically for retrieval tasks
- **High Performance**: Fast inference with excellent accuracy

### API Endpoints

**Workshop Endpoint**: `https://ai4-workshop-embeddings.vercel.app/api/embed`

**Method**: POST

**Headers**:
\`\`\`json
{
  "Content-Type": "application/json"
}
\`\`\`

### Request Format

The workshop endpoint accepts a simplified format that handles both text and images:

For text embeddings:
\`\`\`json
{
  "input": "Your text content here",
  "input_type": "document"
}
\`\`\`

For image embeddings:
\`\`\`json
{
  "input": "data:image/png;base64,iVBORw0KGgo...",
  "input_type": "document"
}
\`\`\`

For search queries:
\`\`\`json
{
  "input": "search query",
  "input_type": "query"
}
\`\`\`

**Response Format**:
\`\`\`json
{
  "success": true,
  "embedding": [0.123, -0.456, ...],
  "dimensions": 1024,
  "model": "voyage-multimodal-3"
}
\`\`\`

:::info Input Types
- Use `"document"` for content being indexed
- Use `"query"` for search queries
- This distinction optimizes embeddings for their purpose
:::

## Testing Your Setup

### Quick Test with cURL

Test the workshop endpoint:

\`\`\`bash
curl -X POST "https://ai4-workshop-embeddings.vercel.app/api/embed" \
  -H "Content-Type: application/json" \
  -d '{
    "input": "Hello, multimodal world!",
    "input_type": "document"
  }'
\`\`\`

Expected response:
\`\`\`json
{
  "success": true,
  "embedding": [0.123, -0.456, ...],
  "dimensions": 1024,
  "model": "voyage-multimodal-3"
}
\`\`\`

### Test in n8n

Create a test workflow:

1. Add **Manual Trigger** node
2. Add **HTTP Request** node
3. Configure HTTP Request:
   - **Method**: POST
   - **URL**: `https://ai4-workshop-embeddings.vercel.app/api/embed`
   - **Authentication**: Select your `Workshop Embedding Service` credential
   - **Body Content Type**: JSON
   - **Body**:
     \`\`\`json
     {
       "input": "Test embedding for workshop",
       "input_type": "document"
     }
     \`\`\`
4. Execute and verify you get a response with `embedding` array

## Usage Limits & Workshop Policies

### Workshop Endpoint Limits
- Rate limit: 100 requests/minute per IP
- Max concurrent requests: 10
- Request timeout: 30 seconds
- Perfect for workshop activities

### Usage Guidelines
- Use the endpoint responsibly during the workshop
- Text processing: ~100ms average response time
- Image processing: ~200ms average response time
- Designed to handle workshop load comfortably

## Best Practices

### 1. Image Preparation

Before sending images:
- Convert to base64 format
- Resize large images (max 2048x2048 recommended)
- Use PNG or JPEG format
- Include proper data URI prefix

### 2. Batch Processing

Maximize efficiency:
\`\`\`json
{
  "input": [
    "First document",
    "Second document",
    "data:image/png;base64,...",
    "Fourth document"
  ],
  "model": "voyage-multimodal-3",
  "input_type": "document"
}
\`\`\`

### 3. Error Handling

Common errors and solutions:

| Error | Solution |
|-------|----------|
| 401 Unauthorized | Check API key format and validity |
| 429 Rate Limited | Implement retry logic with backoff |
| 413 Payload Too Large | Reduce image size or batch size |
| 400 Invalid Input | Check base64 encoding and format |

## Integration Tips for n8n

### Code Node Template

Here's a reusable template for the workshop embedding service in n8n:

\`\`\`javascript
// Helper function for workshop embeddings
async function getEmbedding(input, inputType = 'document') {
  const response = await $http.request({
    method: 'POST',
    url: 'https://ai4-workshop-embeddings.vercel.app/api/embed',
    headers: {
      'Content-Type': 'application/json'
    },
    body: {
      input: input,
      input_type: inputType
    }
  });
  
  if (!response.success) {
    throw new Error(`Embedding failed: ${response.error}`);
  }
  
  return response.embedding;
}

// Usage examples:
// Text embedding
const textEmbedding = await getEmbedding("Sample text");

// Image embedding (base64 encoded)
const imageEmbedding = await getEmbedding(`data:image/png;base64,${base64Data}`);

// Query embedding
const queryEmbedding = await getEmbedding("search query", "query");
\`\`\`

### Error Handling

\`\`\`javascript
try {
  const embedding = await getEmbedding(inputText);
  return [{ json: { embedding: embedding } }];
} catch (error) {
  // Fallback or retry logic
  console.error('Embedding generation failed:', error);
  return [{ json: { error: error.message, embedding: null } }];
}
\`\`\`

## Troubleshooting

### Common Issues

**Endpoint Not Responding**
- Check your internet connection
- Verify the URL is correct
- Workshop endpoint may be temporarily unavailable

**Rate Limiting**
- Reduce request frequency
- Add delays between requests
- Implement retry logic with backoff

**Large Image Errors**
- Resize images to max 2048x2048
- Use JPEG compression for large files
- Ensure base64 encoding is correct

### Performance Tips
- Keep images reasonably sized (< 2MB)
- Cache embeddings locally when possible
- Use batch processing where appropriate

### Getting Help
- Check workshop chat for real-time support
- Verify your network allows HTTPS requests
- Try the cURL test command first

## Next Steps

With the workshop embedding service configured:

‚úÖ Serverless endpoint access configured  
‚úÖ Credentials set up in n8n  
‚úÖ Test embeddings working  
‚úÖ Ready for multimodal processing  
‚úÖ No API key management required  

Now let's build the PDF processing workflow!

[Continue to PDF Processing ‚Üí](./pdf-processing-workflow)
```

# 40-pdf-processing-workflow.mdx

```mdx
---
sidebar_position: 40
---

# üìÑ PDF Processing Workflow

Build your first n8n workflow to process PDFs into searchable multimodal embeddings.

<SlideRecap 
  title="From Documents to Intelligent Data"
  items={[
    {
      icon: "üìÑ",
      title: "PDF Ingestion Pipeline",
      description: "How to automatically process documents and extract both text and visual content"
    },
    {
      icon: "üé®", 
      title: "Multimodal Understanding",
      description: "Why processing images and text together creates richer, more accurate search"
    },
    {
      icon: "üîÑ",
      title: "n8n Visual Workflows",
      description: "Building complex data pipelines with visual, no-code automation"
    }
  ]}
  nextSection="Time to build your first multimodal processing pipeline!"
/>

<InstructorNotes 
  timing="PDF Processing Workflow (25-30 minutes)"
  notes={[
    "This is the heart of the workshop - expect debugging time",
    "PDF download node often fails due to CORS or authentication issues",
    "PDF toolkit node can be memory intensive with large files",
    "Embedding generation is the slowest step - use smaller PDFs for demos",
    "MongoDB connection issues surface here if Atlas wasn't configured properly"
  ]}
  tips={[
    "Start with a simple 1-2 page PDF for initial testing",
    "Use the sample PDFs provided in the workshop repo",
    "Show attendees how to use the debug panel to inspect data flow",
    "Expect 5-10 minutes for first successful workflow execution",
    "Have attendees save workflow frequently - crashes can happen",
    "Demonstrate error handling patterns they'll need later"
  ]}
/>

## Workflow Overview

\`\`\`mermaid
graph LR
    A[Webhook Trigger] --> B[Download PDF]
    B --> C[Extract Pages]
    C --> D[Convert to Images]
    D --> E[Generate Embeddings]
    E --> F[Store in MongoDB]
\`\`\`

## Step 1: Create the Webhook Trigger

### Add Webhook Node

1. Create a new workflow in n8n
2. Click the **+** button to add a node
3. Search for **"Webhook"**
4. Configure:
   - **HTTP Method**: POST
   - **Path**: `/process-pdf`
   - **Response Mode**: Immediately respond
   - **Response Code**: 200
   - **Response Data**: `{"status": "processing"}`

### Copy the Webhook URL

1. Click on the Webhook node
2. Copy the **Test URL** (for development)
3. It will look like: `http://localhost:5678/webhook-test/process-pdf`

:::tip Production URLs
In production, use the Production URL instead of Test URL.
:::

## Step 2: Download PDF

### Add HTTP Request Node

1. Add **HTTP Request** node
2. Connect it to the Webhook node
3. Configure:
   - **Method**: GET
   - **URL**: `{{ $json.pdf_url }}`
   - **Response Format**: File
   - **Binary Property**: data

### Test Data Structure

Your webhook should receive:
\`\`\`json
{
  "pdf_url": "https://example.com/sample.pdf",
  "metadata": {
    "source": "user_upload",
    "tags": ["ai", "workshop"]
  }
}
\`\`\`

## Step 3: Extract PDF Pages

### Add Read PDF Node

1. Add **Read PDF** node (from community nodes if needed)
2. Connect to HTTP Request node
3. Configure:
   - **Operation**: Extract Pages as Images
   - **Binary Property**: data
   - **Output Format**: Separate Items
   - **Image Format**: PNG
   - **DPI**: 150 (balance quality/size)

:::info Alternative Approach
If Read PDF node isn't available, use a Code node with pdf-parse library.
:::

### Code Node Alternative

\`\`\`javascript
const PDFParser = require('pdf-parse');
const { createCanvas } = require('canvas');
const items = [];

// Get PDF buffer
const pdfBuffer = $binary.data.buffer;

// Parse PDF
const pdfData = await PDFParser(pdfBuffer);

// Extract text pages
for (let i = 0; i < pdfData.numpages; i++) {
  items.push({
    json: {
      page_number: i + 1,
      text: pdfData.text, // Simplified - real implementation would get page text
      total_pages: pdfData.numpages
    }
  });
}

return items;
\`\`\`

## Step 4: Generate Embeddings

Before building the embedding code, let's verify our API is working:

<QuickEmbeddingTest 
  text="This is sample PDF content that will be processed by our workflow"
  label="Test Embedding API"
/>

### Add Code Node for Voyage AI

1. Add **Code** node
2. Connect to PDF extraction node
3. Add this code:

\`\`\`javascript
const items = [];

for (const item of $input.all()) {
  try {
    // Get image data
    const imageBuffer = await item.binary.data.buffer;
    const base64Image = imageBuffer.toString('base64');
    
    // Prepare input based on content type
    const input = item.json.content_type === 'image' 
      ? `data:image/png;base64,${base64Image}`
      : item.json.text_content;
    
    // Call Workshop Embedding Service
    const response = await $http.request({
      method: 'POST',
      url: 'https://workshop-embedding-api.vercel.app/api/embed',
      headers: {
        'Content-Type': 'application/json'
      },
      body: {
        text: input,
        model: 'voyage-3'
      }
    });
    
    // Prepare document for MongoDB
    items.push({
      json: {
        filename: $('Webhook').item.json.metadata.filename || 'unknown.pdf',
        page_number: item.json.page_number,
        content_type: item.json.content_type || 'image',
        embedding: response.embeddings[0],
        metadata: {
          source_url: $('Webhook').item.json.pdf_url,
          processed_at: new Date().toISOString(),
          total_pages: item.json.total_pages,
          ...$('Webhook').item.json.metadata
        },
        text_content: item.json.text_content || null,
        image_data: {
          width: item.json.width || null,
          height: item.json.height || null,
          format: 'png'
        }
      }
    });
    
  } catch (error) {
    console.error(`Error processing page ${item.json.page_number}:`, error);
    // Continue with other pages
  }
}

return items;
\`\`\`

## Step 5: Store in MongoDB

### Add MongoDB Node

1. Add **MongoDB** node
2. Connect to Code node
3. Configure:
   - **Credential**: Select your MongoDB Atlas credential
   - **Operation**: Insert
   - **Collection**: `pdf_documents`
   - **Options**:
     - **Multiple Documents**: Toggle ON

## Complete Workflow

Your workflow should now look like:

\`\`\`
[Webhook] ‚Üí [HTTP Request] ‚Üí [Extract PDF] ‚Üí [Generate Embeddings] ‚Üí [MongoDB]
\`\`\`

## Testing the Workflow

### 1. Activate the Workflow

Click **"Execute Workflow"** to activate test mode.

### 2. Send Test Request

Use cURL or Postman:

\`\`\`bash
curl -X POST http://localhost:5678/webhook-test/process-pdf \
  -H "Content-Type: application/json" \
  -d '{
    "pdf_url": "https://example.com/sample.pdf",
    "metadata": {
      "filename": "ai-workshop.pdf",
      "source": "workshop_test"
    }
  }'
\`\`\`

### 3. Monitor Execution

1. Check each node for green checkmarks
2. Click nodes to see output data
3. Verify documents in MongoDB Atlas

## Error Handling

### Add Error Workflow

1. Create new workflow: "PDF Processing Error Handler"
2. Add **Error Trigger** node
3. Add **MongoDB** node to log errors:
   \`\`\`javascript
   {
     collection: "processing_errors",
     error: $json.error,
     workflow: $json.workflow,
     timestamp: new Date()
   }
   \`\`\`

### Link Error Handler

1. Go to main workflow settings
2. Set **Error Workflow** to your error handler
3. Errors will now be logged

## Performance Optimization

### Batch Processing

For multiple PDFs, modify the workflow:

1. Use **Split In Batches** node after webhook
2. Process 5 PDFs at a time
3. Add delay between batches

### Caching Strategy

Add a check before processing:

\`\`\`javascript
// Check if already processed
const existing = await $mongodb.find({
  filename: filename,
  page_number: pageNumber
});

if (existing.length > 0) {
  return [{json: {skipped: true, reason: 'Already processed'}}];
}
\`\`\`

## Monitoring & Logging

### Add Logging Node

Create a simple logger:

\`\`\`javascript
const logEntry = {
  workflow_id: $workflow.id,
  execution_id: $execution.id,
  timestamp: new Date(),
  status: 'completed',
  stats: {
    pages_processed: items.length,
    pdf_url: $('Webhook').item.json.pdf_url,
    processing_time: Date.now() - $execution.startedAt
  }
};

// Log to MongoDB
await $mongodb.insert('workflow_logs', logEntry);
\`\`\`

## Next Steps

Your PDF processing pipeline is ready! You can now:

‚úÖ Accept PDF URLs via webhook  
‚úÖ Extract pages as images  
‚úÖ Generate multimodal embeddings  
‚úÖ Store in MongoDB with vector search  

Let's build the search functionality next!

[Continue to Vector Search ‚Üí](./vector-search-workflow)
```

# 45-local-setup-tips.mdx

```mdx
---
sidebar_position: 45
---

# üíª Docker Development Best Practices

Based on real-world experience, here's how to optimize your Docker-based n8n development environment.

<InstructorNotes 
  timing="Docker Best Practices (Optional - 10 minutes)"
  notes={[
    "This is reference material - don't spend workshop time here unless issues arise",
    "Most useful for attendees doing local development vs Codespaces",
    "Docker memory issues are common on older machines with 8GB RAM",
    "Volume persistence problems cause workflow loss - emphasize backup",
    "Network connectivity issues surface with corporate firewalls"
  ]}
  tips={[
    "Point attendees here when they encounter Docker issues during workshop",
    "Use as troubleshooting reference rather than sequential content",
    "Emphasize that Codespaces avoids most of these complications",
    "Have this page bookmarked for quick access during debugging",
    "Consider this advanced material for post-workshop exploration"
  ]}
/>

## üê≥ Docker Environment Management

### Project Structure

\`\`\`
multimodal-pdf-agent-n8n/          # Deployment repository
‚îú‚îÄ‚îÄ docker-compose.yml              # Main services configuration
‚îú‚îÄ‚îÄ docker-compose.dev.yml          # Development overrides (optional)
‚îú‚îÄ‚îÄ .env                            # Environment variables
‚îú‚îÄ‚îÄ .devcontainer/                  # GitHub Codespaces configuration
‚îú‚îÄ‚îÄ volumes/
‚îÇ   ‚îú‚îÄ‚îÄ n8n/                       # n8n data persistence
‚îÇ   ‚îú‚îÄ‚îÄ n8n/                       # n8n workflow data
‚îÇ   ‚îî‚îÄ‚îÄ files/                     # Uploaded files
‚îú‚îÄ‚îÄ init/
‚îÇ   ‚îú‚îÄ‚îÄ workflows/                 # Sample n8n workflows
‚îÇ   ‚îú‚îÄ‚îÄ sample-data/               # Test PDFs
‚îÇ   ‚îî‚îÄ‚îÄ scripts/                   # Setup and utility scripts
‚îú‚îÄ‚îÄ scripts/                       # Testing and management scripts
‚îú‚îÄ‚îÄ workshop-embedding-api/        # Serverless embedding endpoint
‚îî‚îÄ‚îÄ logs/                          # Container logs (if configured)
\`\`\`

### Environment Configuration

Create a comprehensive `.env` file:

\`\`\`bash
# Docker Configuration
COMPOSE_PROJECT_NAME=n8n-workshop

# n8n Configuration
N8N_PORT=5678
N8N_PROTOCOL=http
N8N_HOST=localhost
N8N_LOG_LEVEL=info
N8N_METRICS=false
N8N_VERSION=1.103.2

# Resource Limits
N8N_MEMORY_LIMIT=2g

# Development Settings
DEBUG_MODE=true
ENABLE_HOT_RELOAD=true
\`\`\`

## üîÑ Service Management

### Starting Services

\`\`\`bash
# Start all services
docker-compose up -d

# Start specific service
docker-compose up -d n8n

# Start with logs visible
docker-compose up n8n

# Use development configuration
docker-compose -f docker-compose.yml -f docker-compose.dev.yml up
\`\`\`

### Monitoring Services

\`\`\`bash
# Check status
docker-compose ps

# View logs
docker-compose logs -f n8n

# Monitor resource usage
docker stats

# Access container shell
docker-compose exec n8n sh
\`\`\`

## üìÅ Data Persistence

### Volume Management

\`\`\`yaml
# docker-compose.yml
volumes:
  n8n_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/n8n
  
  n8n_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./volumes/mongodb
\`\`\`

### Backup Strategy

Create automated backup script:

\`\`\`bash
#!/bin/bash
# backup.sh

DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="./backups/$DATE"

echo "Creating backup: $BACKUP_DIR"
mkdir -p "$BACKUP_DIR"

# Backup n8n workflows
docker-compose exec n8n n8n export:workflow --all \
  --output=/home/node/backups/workflows_$DATE.json

# Backup n8n workflows
docker-compose exec n8n n8n export:workflow --all \
  --output=/home/node/backups/workflows_$DATE.json

# Copy files
cp -r ./volumes/files "$BACKUP_DIR/"

echo "Backup complete!"
\`\`\`

## üöÄ Performance Optimization

### Docker Resource Configuration

\`\`\`yaml
# docker-compose.yml
services:
  n8n:
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
\`\`\`

### Container Health Checks

\`\`\`yaml
services:
  n8n:
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:5678/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
\`\`\`

## üîß Development Workflow

### Hot Reload for Custom Nodes

\`\`\`yaml
# docker-compose.dev.yml
services:
  n8n:
    volumes:
      - ./custom-nodes:/home/node/.n8n/custom
      - ./packages:/home/node/packages
    environment:
      - N8N_CUSTOM_EXTENSIONS=/home/node/custom-nodes
\`\`\`

### Debugging Configuration

\`\`\`yaml
services:
  n8n:
    environment:
      - NODE_ENV=development
      - N8N_LOG_LEVEL=debug
      - DEBUG=n8n:*
    ports:
      - "9229:9229"  # Node.js debugger
\`\`\`

## üåê Network Configuration

### Service Communication

\`\`\`yaml
# docker-compose.yml
networks:
  n8n-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
\`\`\`

### External Service Access

\`\`\`bash
# MongoDB Atlas is accessed via connection string from Atlas UI
# No local MongoDB instances are used in this workshop
\`\`\`

## üìä Monitoring & Logging

### Centralized Logging

\`\`\`yaml
# docker-compose.yml
services:
  n8n:
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
\`\`\`

### Log Analysis

\`\`\`bash
# Search logs
docker-compose logs n8n | grep ERROR

# Export logs
docker-compose logs > workshop_logs_$(date +%Y%m%d).log

# Real-time monitoring
watch docker-compose ps
\`\`\`

## üõ°Ô∏è Security Best Practices

### Secrets Management

\`\`\`bash
# Use Docker secrets
echo "your-api-key" | docker secret create voyage_api_key -

# Reference in docker-compose.yml
secrets:
  voyage_api_key:
    external: true
\`\`\`

### Network Isolation

\`\`\`yaml
services:
  n8n:
    networks:
      - frontend
      - backend
  
  mongodb:
    networks:
      - backend
\`\`\`

## üéØ Common Tasks

### Rebuild Services

\`\`\`bash
# Rebuild single service
docker-compose build n8n

# Rebuild all services
docker-compose build

# Force rebuild without cache
docker-compose build --no-cache
\`\`\`

### Clean Up

\`\`\`bash
# Stop and remove containers
docker-compose down

# Remove volumes too
docker-compose down -v

# Clean Docker system
docker system prune -a
\`\`\`

## üí° Pro Tips

1. **Use Docker Compose profiles** for different environments:
   \`\`\`yaml
   services:
     voyage-mock:
       profiles: ["testing"]
   \`\`\`

2. **Implement health checks** for all services

3. **Use named volumes** for better data management

4. **Set resource limits** to prevent container runaway

5. **Regular backups** of workflows and data

6. **Monitor disk space** - Docker can consume significant space

## üö® Troubleshooting

### Container Won't Start

\`\`\`bash
# Check logs
docker-compose logs n8n

# Inspect container
docker-compose ps
docker inspect n8n-workshop

# Reset everything
docker-compose down -v
docker-compose up -d
\`\`\`

### Performance Issues

\`\`\`bash
# Check resource usage
docker stats

# Increase memory limits
docker-compose down
# Edit docker-compose.yml to increase limits
docker-compose up -d
\`\`\`

### Network Problems

\`\`\`bash
# Test n8n service
curl http://localhost:5678/healthz

# Check network configuration
docker network ls
docker network inspect workshop_n8n-network
\`\`\`

Ready to build production-ready workflows! ‚Üí
```

# 50-vector-search-workflow.mdx

```mdx
---
sidebar_position: 50
---

# üîç Vector Search Workflow

Create a search endpoint that uses MongoDB Atlas Vector Search to find relevant PDF content.

<InstructorNotes 
  timing="Vector Search Implementation (20-25 minutes)"
  notes={[
    "Vector search index creation can take 5-10 minutes - start this early",
    "Common error: querying before index is built ('index not found')",
    "MongoDB aggregation pipeline syntax trips up SQL developers",
    "Search quality depends heavily on having enough documents ingested",
    "Similarity scores need explanation - attendees expect exact matches"
  ]}
  tips={[
    "Create the vector index immediately after PDF ingestion finishes",
    "Use MongoDB Atlas UI to monitor index building progress",
    "Test with queries that should return obvious matches first",
    "Explain that cosine similarity scores range from 0-1 (higher = more similar)",
    "Have 3-5 test queries ready that demonstrate different search capabilities",
    "Show attendees how to interpret search results and similarity scores"
  ]}
/>

## Search Workflow Structure

\`\`\`mermaid
graph LR
    A[Webhook - Search Query] --> B[Generate Query Embedding]
    B --> C[MongoDB Vector Search]
    C --> D[Format Results]
    D --> E[Return Response]
\`\`\`

## Implementation Steps

### 1. Search Webhook

Add a new **Webhook** node:

1. **HTTP Method**: POST
2. **Path**: `/search`
3. **Response Mode**: Last Node
4. **Response Code**: 200

Expected request format:
\`\`\`json
{
  "query": "What is machine learning?",
  "limit": 5,
  "filters": {
    "source": "workshop_pdfs"
  }
}
\`\`\`

### 2. Query Embedding Generation

Add a **Code** node to generate query embeddings:

\`\`\`javascript
const query = $json.query;

if (!query || query.trim() === '') {
  throw new Error('Query cannot be empty');
}

// Call Workshop Embedding Service with query input type
const response = await $http.request({
  method: 'POST',
  url: 'https://ai4-workshop-embeddings.vercel.app/api/embed',
  headers: {
    'Content-Type': 'application/json'
  },
  body: {
    input: query,
    input_type: 'query' // Important: use 'query' for search
  }
});

return [{
  json: {
    query: query,
    embedding: response.embedding,
    limit: $json.limit || 5,
    filters: $json.filters || {}
  }
}];
\`\`\`

### 3. MongoDB Vector Search

Add a **MongoDB** node with aggregation pipeline:

1. **Operation**: Aggregate
2. **Collection**: `pdf_documents`
3. **Pipeline**:

\`\`\`javascript
[
  {
    "$vectorSearch": {
      "index": "vector_index",
      "path": "embedding",
      "queryVector": $json.embedding,
      "numCandidates": 100,
      "limit": $json.limit || 5
    }
  },
  {
    "$project": {
      "_id": 1,
      "filename": 1,
      "page_number": 1,
      "content_type": 1,
      "text_content": 1,
      "metadata": 1,
      "score": { "$meta": "vectorSearchScore" }
    }
  }
]
\`\`\`

:::tip Vector Search Parameters
- **queryVector**: The embedding vector to search for
- **numCandidates**: Number of candidates to consider (should be >= limit)
- **limit**: Final number of results to return
- **index**: Must match your Atlas Vector Search index name
:::

### 4. Format Results

Add a **Code** node to format the response:

\`\`\`javascript
const results = $json;
const query = $('Webhook').item.json.query;

// Format results for easy consumption
const formattedResults = results.map((doc, index) => ({
  rank: index + 1,
  score: doc.score,
  filename: doc.filename,
  page_number: doc.page_number,
  content_type: doc.content_type,
  preview: doc.text_content ? 
    doc.text_content.substring(0, 200) + '...' : 
    'Image content',
  metadata: doc.metadata
}));

return [{
  json: {
    query: query,
    total_results: formattedResults.length,
    results: formattedResults,
    search_timestamp: new Date().toISOString()
  }
}];
\`\`\`

## Advanced Search Features

### Hybrid Search (Text + Image)

Modify your search to handle both text queries and image queries:

\`\`\`javascript
// Detect if input is image or text
const input = $json.query;
let searchInput;
let inputType = 'query';

if (input.startsWith('data:image')) {
  // Image search
  searchInput = input;
} else {
  // Text search
  searchInput = input;
}

// Generate embedding
const embedding = await getVoyageEmbedding(searchInput, inputType);
\`\`\`

### Semantic + Metadata Filtering

Combine vector search with metadata filters:

\`\`\`javascript
[
  {
    "$vectorSearch": {
      "index": "vector_index",
      "path": "embedding",
      "queryVector": $json.embedding,
      "numCandidates": 100,
      "limit": 20,
      "filter": {
        "metadata.tags": { "$in": ["ai", "ml"] }
      }
    }
  },
  {
    "$limit": $json.limit || 5
  }
]
\`\`\`

### Vector Search with Pre-Filtering

Use the filter fields defined in your index:

\`\`\`javascript
[
  {
    "$vectorSearch": {
      "index": "vector_index",
      "path": "embedding", 
      "queryVector": $json.embedding,
      "numCandidates": 100,
      "limit": $json.limit || 5,
      "filter": {
        "$and": [
          { "content_type": { "$eq": "image" } },
          { "filename": { "$regex": "^ml_", "$options": "i" } }
        ]
      }
    }
  },
  {
    "$project": {
      "_id": 1,
      "filename": 1,
      "page_number": 1,
      "content_type": 1,
      "text_content": 1,
      "metadata": 1,
      "score": { "$meta": "vectorSearchScore" }
    }
  }
]
\`\`\`

:::tip Filter Performance
- Only use fields that are indexed as `"type": "filter"` in your vector search index
- Complex filters may impact search performance
- Pre-filtering reduces the candidate set before vector similarity calculation
:::

### Multi-Stage Search

Implement a two-stage search for better relevance:

\`\`\`javascript
// Stage 1: Initial broad search
const initialResults = await vectorSearch(embedding, 20);

// Stage 2: Rerank using additional criteria
const rerankedResults = initialResults
  .map(doc => ({
    ...doc,
    relevanceScore: calculateRelevance(doc, query)
  }))
  .sort((a, b) => b.relevanceScore - a.relevanceScore)
  .slice(0, limit);
\`\`\`

## Testing Your Search

### Test Queries

Create test requests:

\`\`\`bash
# Text search
curl -X POST http://localhost:5678/webhook/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "machine learning algorithms",
    "limit": 3
  }'

# Search with filters
curl -X POST http://localhost:5678/webhook/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "neural networks",
    "limit": 5,
    "filters": {
      "source": "workshop_pdfs"
    }
  }'
\`\`\`

### Expected Response

\`\`\`json
{
  "query": "machine learning algorithms",
  "total_results": 3,
  "results": [
    {
      "rank": 1,
      "score": 0.92,
      "filename": "ml-basics.pdf",
      "page_number": 5,
      "content_type": "image",
      "preview": "Image content",
      "metadata": {
        "source_url": "https://example.com/ml-basics.pdf",
        "processed_at": "2024-01-20T10:30:00Z"
      }
    }
  ],
  "search_timestamp": "2024-01-20T11:00:00Z"
}
\`\`\`

## Performance Optimization

### 1. Caching Layer

Add Redis caching for frequent queries:

\`\`\`javascript
// Check cache first
const cacheKey = `search:${query}:${limit}`;
const cached = await redis.get(cacheKey);
if (cached) {
  return JSON.parse(cached);
}

// Perform search
const results = await performVectorSearch(query);

// Cache results (5 minute TTL)
await redis.setex(cacheKey, 300, JSON.stringify(results));
\`\`\`

### 2. Query Expansion

Improve recall with query expansion:

\`\`\`javascript
// Generate variations of the query
const expandedQueries = [
  query,
  await generateSynonyms(query),
  await generateRelatedTerms(query)
];

// Get embeddings for all variations
const embeddings = await Promise.all(
  expandedQueries.map(q => getVoyageEmbedding(q))
);

// Average the embeddings
const avgEmbedding = averageVectors(embeddings);
\`\`\`

### 3. Result Deduplication

Remove duplicate content:

\`\`\`javascript
const uniqueResults = results.reduce((acc, current) => {
  const isDuplicate = acc.find(item => 
    item.filename === current.filename && 
    Math.abs(item.page_number - current.page_number) <= 1
  );
  
  if (!isDuplicate) {
    acc.push(current);
  }
  
  return acc;
}, []);
\`\`\`

## Error Handling

### Add Error Boundaries

\`\`\`javascript
try {
  // Main search logic
  const results = await performSearch();
  
  if (!results || results.length === 0) {
    return [{
      json: {
        query: query,
        total_results: 0,
        results: [],
        message: "No results found. Try different keywords."
      }
    }];
  }
  
  return formatResults(results);
  
} catch (error) {
  console.error('Search error:', error);
  
  // Return graceful error response
  return [{
    json: {
      error: true,
      message: "Search temporarily unavailable",
      query: query,
      timestamp: new Date().toISOString()
    }
  }];
}
\`\`\`

## Monitoring Search Quality

### Log Search Analytics

\`\`\`javascript
// Log search metrics
const searchMetrics = {
  query: query,
  results_count: results.length,
  top_score: results[0]?.score || 0,
  response_time: Date.now() - startTime,
  has_results: results.length > 0,
  timestamp: new Date()
};

await $mongodb.insert('search_analytics', searchMetrics);
\`\`\`

## Next Steps

Your vector search is now operational! You can:

‚úÖ Accept search queries via API  
‚úÖ Generate query embeddings  
‚úÖ Perform similarity search  
‚úÖ Return formatted results  

Let's add AI intelligence to create a conversational agent!

[Continue to AI Agent ‚Üí](./ai-agent-workflow)
```

# 60-ai-agent-workflow.mdx

```mdx
---
sidebar_position: 60
---

# ü§ñ AI Agent with Function Calling

Build an intelligent agent using Gemini 2.0 Flash and n8n's powerful workflow capabilities.

<SlideRecap 
  title="AI Agents & Intelligent Reasoning"
  items={[
    {
      icon: "ü§ñ",
      title: "Function Calling & Tool Use",
      description: "How modern AI agents decide when and how to use external tools for better answers"
    },
    {
      icon: "üß†", 
      title: "ReAct Pattern",
      description: "Reasoning and Acting - the industry-standard pattern for intelligent agent behavior"
    },
    {
      icon: "‚ö°",
      title: "Gemini 2.0 Integration",
      description: "Leveraging Google's latest multimodal AI model for sophisticated agent capabilities"
    }
  ]}
  nextSection="Let's build your intelligent agent step by step!"
/>

<InstructorNotes 
  timing="AI Agent Creation (30-35 minutes)"
  notes={[
    "This is the most complex section - expect questions about function calling",
    "Gemini API key signup requires Google account (some attendees may not have)",
    "Function calling syntax in n8n is different from OpenAI - emphasize this",
    "Tool selection logic often needs multiple iterations to get right",
    "Response formatting is crucial for good user experience"
  ]}
  tips={[
    "Demo the difference between simple chat and function calling first",
    "Show how to test function definitions before building full agent",
    "Use clear, simple test queries that obviously need search",
    "Explain the ReAct pattern (reason, act, observe) conceptually",
    "Have backup Gemini API key ready in case attendees hit rate limits",
    "Demonstrate debugging failed function calls step by step"
  ]}
/>

## Agent Architecture

\`\`\`mermaid
graph TD
    A[User Query] --> B{Tool Selection}
    B -->|Need Search| C[Vector Search Tool]
    B -->|Direct Answer| D[Generate Response]
    C --> E[Retrieve Documents]
    E --> D
    D --> F[Return Answer]
\`\`\`

## Gemini Integration Setup

### 1. Get Gemini API Key

1. Visit [Google AI Studio](https://makersuite.google.com/app/apikey)

<QRCodeAccess 
  url="https://makersuite.google.com/app/apikey"
  title="Get Gemini API Key"
/>

2. Click **"Create API Key"**
3. Copy the key for n8n

### 2. Add Gemini Credentials in n8n

1. **Credentials** ‚Üí **New**
2. Search for **"Header Auth"**
3. Configure:
   - **Name**: `Gemini AI`
   - **Header Auth**:
     - **Name**: `x-goog-api-key`
     - **Value**: Your API key

## Building the AI Agent Workflow

### 1. Agent Webhook

Create a new workflow with **Webhook** node:
- **Path**: `/agent`
- **Method**: POST
- **Response**: Last Node

Expected input:
\`\`\`json
{
  "message": "What does the PDF say about neural networks?",
  "session_id": "user-123",
  "context": []
}
\`\`\`

### 2. Function Calling Implementation

Add a **Code** node for tool selection:

\`\`\`javascript
const userMessage = $json.message;
const sessionId = $json.session_id || 'default';

// Define available tools
const tools = [{
  type: "function",
  function: {
    name: "search_documents",
    description: "Search through PDF documents for relevant information",
    parameters: {
      type: "object",
      properties: {
        query: {
          type: "string",
          description: "Search query to find relevant documents"
        },
        limit: {
          type: "number",
          description: "Number of results to return (default: 5)"
        }
      },
      required: ["query"]
    }
  }
}, {
  type: "function", 
  function: {
    name: "get_document_details",
    description: "Get detailed information about a specific document",
    parameters: {
      type: "object",
      properties: {
        filename: {
          type: "string",
          description: "Name of the PDF file"
        },
        page_number: {
          type: "number",
          description: "Specific page number"
        }
      },
      required: ["filename"]
    }
  }
}];

// Prepare messages for Gemini
const messages = [
  {
    role: "system",
    content: `You are an AI assistant with access to a PDF document search system. 
    You can search through documents to answer questions. 
    Always search for relevant information before answering.
    Be helpful and provide detailed answers based on the documents.`
  },
  {
    role: "user",
    content: userMessage
  }
];

// Call Gemini with function calling
const response = await $http.request({
  method: 'POST',
  url: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent',
  headers: {
    'Content-Type': 'application/json',
    'x-goog-api-key': await $credentials.get('gemini', 'apiKey')
  },
  body: {
    contents: messages.map(m => ({
      role: m.role === 'system' ? 'user' : m.role,
      parts: [{ text: m.content }]
    })),
    tools: tools,
    generationConfig: {
      temperature: 0.7,
      maxOutputTokens: 2048
    }
  }
});

// Extract function call if present
const candidate = response.candidates[0];
const content = candidate.content;

if (content.parts[0].functionCall) {
  const functionCall = content.parts[0].functionCall;
  return [{
    json: {
      action: 'function_call',
      function_name: functionCall.name,
      arguments: functionCall.args,
      original_message: userMessage,
      session_id: sessionId
    }
  }];
} else {
  return [{
    json: {
      action: 'direct_response',
      response: content.parts[0].text,
      original_message: userMessage,
      session_id: sessionId
    }
  }];
}
\`\`\`

### 3. Router Node

Add an **IF** node to route based on action:
- **Condition**: `{{ $json.action === 'function_call' }}`
- **True**: Route to function execution
- **False**: Route to response formatting

### 4. Function Execution Branch

#### Search Documents Function

Add **HTTP Request** node to call your search endpoint:

\`\`\`javascript
// In a Code node before HTTP Request
const functionName = $json.function_name;
const args = $json.arguments;

if (functionName === 'search_documents') {
  return [{
    json: {
      method: 'POST',
      url: 'http://localhost:5678/webhook/search',
      body: {
        query: args.query,
        limit: args.limit || 5
      },
      original_data: $json
    }
  }];
}
\`\`\`

#### Process Search Results

Add **Code** node after search:

\`\`\`javascript
const searchResults = $json.results || [];
const originalData = $('Router').item.json;

// Format results for Gemini
const formattedResults = searchResults.map(doc => ({
  filename: doc.filename,
  page: doc.page_number,
  relevance: doc.score,
  content: doc.preview,
  type: doc.content_type
}));

// Prepare function response
const functionResponse = {
  role: "function",
  name: originalData.function_name,
  content: JSON.stringify({
    results: formattedResults,
    total_found: searchResults.length
  })
};

return [{
  json: {
    function_response: functionResponse,
    original_message: originalData.original_message,
    session_id: originalData.session_id,
    search_performed: true
  }
}];
\`\`\`

### 5. Generate Final Response

Add **Code** node to generate final answer:

\`\`\`javascript
const functionResponse = $json.function_response;
const originalMessage = $json.original_message;
const searchPerformed = $json.search_performed || false;

// Build conversation with function results
const messages = [
  {
    role: "user",
    content: originalMessage
  }
];

if (searchPerformed && functionResponse) {
  messages.push({
    role: "model",
    content: "",
    parts: [{
      functionCall: {
        name: functionResponse.name,
        args: JSON.parse(functionResponse.content).query
      }
    }]
  });
  messages.push(functionResponse);
}

// Call Gemini again with function results
const response = await $http.request({
  method: 'POST',
  url: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp:generateContent',
  headers: {
    'Content-Type': 'application/json',
    'x-goog-api-key': await $credentials.get('gemini', 'apiKey')
  },
  body: {
    contents: messages.map(m => ({
      role: m.role,
      parts: m.parts || [{ text: m.content }]
    })),
    generationConfig: {
      temperature: 0.7,
      maxOutputTokens: 2048
    }
  }
});

const answer = response.candidates[0].content.parts[0].text;

return [{
  json: {
    response: answer,
    sources: searchPerformed ? JSON.parse(functionResponse.content).results : [],
    session_id: $json.session_id,
    timestamp: new Date().toISOString()
  }
}];
\`\`\`

## Testing the Agent

### Simple Query Test

\`\`\`bash
curl -X POST http://localhost:5678/webhook/agent \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What machine learning concepts are covered in the PDFs?"
  }'
\`\`\`

### Complex Query Test

\`\`\`bash
curl -X POST http://localhost:5678/webhook/agent \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Compare the different neural network architectures mentioned across all documents"
  }'
\`\`\`

## Advanced Agent Features

### Multi-Tool Support

Extend your agent with more tools:

\`\`\`javascript
const additionalTools = [{
  type: "function",
  function: {
    name: "summarize_document",
    description: "Generate a summary of a specific document",
    parameters: {
      type: "object",
      properties: {
        filename: { type: "string" }
      }
    }
  }
}, {
  type: "function",
  function: {
    name: "extract_tables",
    description: "Extract tabular data from PDFs",
    parameters: {
      type: "object",
      properties: {
        topic: { type: "string" }
      }
    }
  }
}];
\`\`\`

### Streaming Responses

For better UX, implement streaming:

\`\`\`javascript
// In webhook configuration
{
  "Response Mode": "When Last Node Finishes",
  "Response Headers": {
    "Content-Type": "text/event-stream",
    "Cache-Control": "no-cache"
  }
}

// Stream events
const streamEvent = (data) => {
  return `data: ${JSON.stringify(data)}\n\n`;
};
\`\`\`

### Error Recovery

Add intelligent error handling:

\`\`\`javascript
try {
  // Main agent logic
} catch (error) {
  // Fallback to direct response
  const fallbackResponse = await generateFallbackResponse(
    userMessage,
    error.message
  );
  
  return [{
    json: {
      response: fallbackResponse,
      error_occurred: true,
      fallback_used: true
    }
  }];
}
\`\`\`

## Agent Optimization

### Response Caching

Cache common queries:

\`\`\`javascript
const cacheKey = `agent:${hashMessage(userMessage)}`;
const cached = await getCache(cacheKey);

if (cached && !forceRefresh) {
  return [{
    json: {
      ...cached,
      from_cache: true
    }
  }];
}
\`\`\`

### Token Management

Monitor and optimize token usage:

\`\`\`javascript
const tokenEstimate = estimateTokens(userMessage + JSON.stringify(tools));

if (tokenEstimate > 3000) {
  // Reduce context or tool descriptions
  tools = tools.slice(0, 2); // Limit tools
}
\`\`\`

## Next Steps

Your AI agent is now functional! It can:

‚úÖ Understand natural language queries  
‚úÖ Decide when to search documents  
‚úÖ Retrieve relevant information  
‚úÖ Generate contextual responses  

You've built a powerful AI agent system! 

[View Workshop Summary ‚Üí](./summary)
```

# 70-complete-multimodal-agent.mdx

```mdx
---
sidebar_position: 70
---

# üß† Complete Multimodal PDF Agent

Build a complete multimodal AI agent that can process PDFs, understand both text and images, and answer questions using MongoDB Vector Search and Voyage AI.

<InstructorNotes 
  timing="Complete Agent Integration (25-30 minutes)"
  notes={[
    "This section integrates everything - expect integration issues to surface",
    "Memory management becomes important with large PDFs and long conversations",
    "Error handling is crucial - workflows are now complex enough to fail in multiple ways",
    "Performance optimization discussions often come up here",
    "Attendees want to test with their own PDFs - have guidelines ready"
  ]}
  tips={[
    "Start with end-to-end demo showing complete workflow",
    "Emphasize testing each component individually before integration",
    "Show monitoring and logging strategies for production workflows",
    "Discuss scaling considerations (rate limits, memory, storage)",
    "Have sample questions ready that showcase multimodal capabilities",
    "Address common production concerns (security, cost, reliability)"
  ]}
/>

## Agent Capabilities

Your finished agent will:
- ‚úÖ **Accept PDF uploads** via web interface
- ‚úÖ **Extract text and images** from PDF pages
- ‚úÖ **Generate multimodal embeddings** using Voyage AI
- ‚úÖ **Store in MongoDB** with vector search index
- ‚úÖ **Answer questions** about PDF content
- ‚úÖ **Understand images** and visual content
- ‚úÖ **Maintain conversation** context

## Architecture Overview

\`\`\`mermaid
graph TD
    A[PDF Upload] --> B[Extract Pages]
    B --> C[Text Extraction]
    B --> D[Image Extraction]
    C --> E[Generate Text Embeddings]
    D --> F[Generate Image Embeddings]
    E --> G[Store in MongoDB]
    F --> G
    H[User Question] --> I[Generate Query Embedding]
    I --> J[Vector Search]
    J --> K[Retrieve Context]
    K --> L[Generate Response]
    L --> M[Return Answer]
\`\`\`

## Complete Workflow Implementation

### 1. PDF Processing Pipeline

**Workflow Name**: `Multimodal PDF Processor`

#### Trigger: File Upload Webhook
\`\`\`json
{
  "path": "/upload-pdf",
  "method": "POST",
  "response_mode": "lastNode"
}
\`\`\`

#### Node 1: Validate Upload
\`\`\`javascript
// JavaScript node to validate PDF
const files = $input.all();
if (!files.length || !files[0].binary) {
  throw new Error('No PDF file uploaded');
}

const filename = files[0].binary.data.fileName;
if (!filename.toLowerCase().endsWith('.pdf')) {
  throw new Error('Only PDF files are supported');
}

return [{
  json: {
    filename: filename,
    fileSize: files[0].binary.data.fileSize,
    timestamp: new Date().toISOString()
  },
  binary: files[0].binary
}];
\`\`\`

#### Node 2: Convert PDF to Images
\`\`\`javascript
// Using PDF-lib or similar
// This extracts each page as an image for multimodal processing
const pdfBuffer = Buffer.from($binary.data, 'base64');
const pages = await extractPagesAsImages(pdfBuffer);

return pages.map((page, index) => ({
  json: {
    filename: $json.filename,
    pageNumber: index + 1,
    totalPages: pages.length
  },
  binary: {
    data: page.buffer,
    mimeType: 'image/png',
    fileName: `${$json.filename}_page_${index + 1}.png`
  }
}));
\`\`\`

#### Node 3: Extract Text Content
\`\`\`javascript
// Extract text from each page
const textContent = await extractTextFromPDF($binary.data);
return [{
  json: {
    filename: $json.filename,
    textContent: textContent,
    wordCount: textContent.split(' ').length
  }
}];
\`\`\`

#### Node 4: Generate Multimodal Embeddings
**HTTP Request Node** to your embedding API:
\`\`\`json
{
  "url": "https://workshop-embedding-api.vercel.app/api/embed",
  "method": "POST",
  "headers": {
    "Content-Type": "application/json"
  },
  "body": {
    "text": "={{ $json.textContent }}",
    "model": "voyage-3"
  }
}
\`\`\`

#### Node 5: Store in MongoDB
\`\`\`javascript
// MongoDB node configuration
{
  "collection": "pdf_documents",
  "operation": "insertOne",
  "document": {
    "filename": "={{ $json.filename }}",
    "pageNumber": "={{ $json.pageNumber }}",
    "textContent": "={{ $json.textContent }}",
    "embedding": "={{ $json.embeddings[0] }}",
    "metadata": {
      "uploadedAt": "={{ $json.timestamp }}",
      "wordCount": "={{ $json.wordCount }}",
      "fileSize": "={{ $json.fileSize }}"
    }
  }
}
\`\`\`

### 2. Conversational Agent Interface

**Workflow Name**: `Multimodal PDF Agent`

#### Trigger: Chat Webhook
\`\`\`json
{
  "path": "/chat",
  "method": "POST",
  "response_mode": "lastNode"
}
\`\`\`

Expected input:
\`\`\`json
{
  "message": "What is discussed about AI in the uploaded document?",
  "filename": "research_paper.pdf",
  "conversation_id": "unique-id"
}
\`\`\`

#### Node 1: Generate Query Embedding
**HTTP Request** to embedding API:
\`\`\`json
{
  "url": "https://workshop-embedding-api.vercel.app/api/embed",
  "method": "POST",
  "body": {
    "text": "={{ $json.message }}",
    "model": "voyage-3"
  }
}
\`\`\`

#### Node 2: Vector Search in MongoDB
\`\`\`javascript
// MongoDB Aggregate operation
[
  {
    $vectorSearch: {
      index: "vector_index",
      path: "embedding",
      queryVector: $json.embeddings[0],
      numCandidates: 50,
      limit: 5,
      filter: {
        filename: $json.filename
      }
    }
  },
  {
    $project: {
      textContent: 1,
      pageNumber: 1,
      score: { $meta: "vectorSearchScore" },
      filename: 1
    }
  }
]
\`\`\`

#### Node 3: Build Context
\`\`\`javascript
// Combine search results into context
const searchResults = $input.all();
const context = searchResults
  .map(result => `Page ${result.json.pageNumber}: ${result.json.textContent}`)
  .join('\n\n');

return [{
  json: {
    query: $('Node 1').first().json.message,
    context: context,
    relevantPages: searchResults.map(r => r.json.pageNumber),
    filename: $json.filename
  }
}];
\`\`\`

#### Node 4: Generate AI Response
**HTTP Request** to AI service (Gemini/OpenAI):
\`\`\`json
{
  "url": "https://api.openai.com/v1/chat/completions",
  "method": "POST",
  "headers": {
    "Authorization": "Bearer YOUR_API_KEY",
    "Content-Type": "application/json"
  },
  "body": {
    "model": "gpt-4",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful AI assistant that answers questions about PDF documents. Use the provided context to answer questions accurately. If the context doesn't contain relevant information, say so."
      },
      {
        "role": "user",
        "content": "Question: {{ $json.query }}\n\nContext from {{ $json.filename }}:\n{{ $json.context }}\n\nPlease answer the question based on the context provided."
      }
    ]
  }
}
\`\`\`

#### Node 5: Format Response
\`\`\`javascript
return [{
  json: {
    response: $json.choices[0].message.content,
    sources: {
      filename: $json.filename,
      relevantPages: $json.relevantPages
    },
    timestamp: new Date().toISOString(),
    conversationId: $json.conversation_id
  }
}];
\`\`\`

## Testing Your Agent

### 1. Upload a PDF
\`\`\`bash
curl -X POST http://localhost:5678/webhook/upload-pdf \
  -F "file=@sample.pdf"
\`\`\`

### 2. Ask Questions
\`\`\`bash
curl -X POST http://localhost:5678/webhook/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What are the main findings in this research?",
    "filename": "sample.pdf",
    "conversation_id": "test-123"
  }'
\`\`\`

## Advanced Features

### Image Understanding
For multimodal capabilities, enhance the embedding generation:

\`\`\`javascript
// Process images with Voyage AI multimodal
const imageEmbedding = await generateEmbedding({
  image: $binary.data, // Base64 image
  text: `Image from page ${$json.pageNumber} of ${$json.filename}`,
  model: "voyage-multimodal-3"
});
\`\`\`

### Conversation Memory
Store conversation history in MongoDB:

\`\`\`javascript
{
  "collection": "conversations",
  "operation": "updateOne",
  "filter": { "conversationId": "={{ $json.conversation_id }}" },
  "update": {
    "$push": {
      "messages": {
        "timestamp": "={{ $json.timestamp }}",
        "question": "={{ $json.query }}",
        "response": "={{ $json.response }}",
        "sources": "={{ $json.sources }}"
      }
    }
  },
  "upsert": true
}
\`\`\`

## Web Interface (Optional)

Create a simple HTML interface:

\`\`\`html
<!DOCTYPE html>
<html>
<head>
    <title>Multimodal PDF Agent</title>
</head>
<body>
    <div id="upload-section">
        <h2>Upload PDF</h2>
        <input type="file" id="pdfFile" accept=".pdf">
        <button onclick="uploadPDF()">Upload</button>
    </div>
    
    <div id="chat-section">
        <h2>Ask Questions</h2>
        <input type="text" id="question" placeholder="Ask about your PDF...">
        <button onclick="askQuestion()">Ask</button>
        <div id="response"></div>
    </div>

    <script>
        async function uploadPDF() {
            const file = document.getElementById('pdfFile').files[0];
            const formData = new FormData();
            formData.append('file', file);
            
            const response = await fetch('/webhook/upload-pdf', {
                method: 'POST',
                body: formData
            });
            
            const result = await response.json();
            console.log('Upload result:', result);
        }
        
        async function askQuestion() {
            const question = document.getElementById('question').value;
            const response = await fetch('/webhook/chat', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    message: question,
                    filename: 'uploaded.pdf',
                    conversation_id: 'web-session'
                })
            });
            
            const result = await response.json();
            document.getElementById('response').innerHTML = result.response;
        }
    </script>
</body>
</html>
\`\`\`

## Success Metrics

Your agent is working when you can:
- ‚úÖ Upload a PDF and see it processed
- ‚úÖ Ask questions about the content
- ‚úÖ Get relevant answers with page references
- ‚úÖ Handle both text and visual content
- ‚úÖ Maintain conversation context

## Next Steps

1. **Enhance with function calling** for complex queries
2. **Add support for multiple file formats**
3. **Implement user authentication**
4. **Add real-time chat interface**
5. **Deploy to production**

Your multimodal PDF agent is now complete and ready for real-world use!
```

# 75-mongodb-vector-setup.mdx

```mdx
---
sidebar_position: 75
---

# üóÑÔ∏è MongoDB Vector Search Setup

Configure MongoDB for real vector search capabilities in your multimodal PDF agent.

## MongoDB Atlas Vector Search Setup

### 1. Connect to MongoDB

\`\`\`bash
# Use MongoDB Atlas connection string from Atlas UI
# Use MongoDB Atlas connection string from Atlas UI
\`\`\`

### 2. Create the Vector Search Index

\`\`\`javascript
// In mongosh, run:
use workshop

db.pdf_documents.createIndex({
  "embedding": "cosmosSearch"
}, {
  "cosmosSearchOptions": {
    "kind": "vector-ivf",
    "numLists": 100,
    "dimensions": 1536,
    "similarity": "COS"
  }
})
\`\`\`

### 3. Alternative: Atlas Search Index

If using MongoDB Atlas, create this search index:

\`\`\`json
{
  "mappings": {
    "dynamic": true,
    "fields": {
      "embedding": {
        "type": "knnVector",
        "dimensions": 1536,
        "similarity": "cosine"
      }
    }
  }
}
\`\`\`

## Setting Up n8n Credentials

### 1. MongoDB Connection

1. In n8n, go to **Credentials** ‚Üí **New**
2. Search for **MongoDB**
3. Configure:
   - **Connection String**: Your MongoDB Atlas connection string
   - **Database**: `multimodal_workshop`

### 2. OpenAI API (for embeddings)

1. **Credentials** ‚Üí **New** ‚Üí **OpenAI**
2. Add your OpenAI API key
3. Name it: `OpenAI Workshop`

## Testing Your Setup

### 1. Test MongoDB Connection

\`\`\`javascript
// In mongosh:
db.pdf_documents.insertOne({
  filename: "test.pdf",
  textContent: "Test document",
  embedding: new Array(1536).fill(0.1)
})

// Verify:
db.pdf_documents.findOne()
\`\`\`

### 2. Test Vector Search

\`\`\`javascript
// Test vector search query:
db.pdf_documents.aggregate([
  {
    "$search": {
      "index": "vector_index",
      "knnBeta": {
        "vector": new Array(1536).fill(0.1),
        "path": "embedding",
        "k": 5
      }
    }
  }
])
\`\`\`

## Import the Real Workflow

1. In n8n, import: `/workspaces/multimodal-pdf-agent-n8n/init/workflows/05-real-multimodal-agent.json`
2. Update credentials:
   - MongoDB connection
   - OpenAI API key
3. Test the webhooks!

## Using the Real Agent

### Upload a PDF
\`\`\`bash
curl -X POST http://localhost:5678/webhook/pdf-upload \
  -F "file=@your-document.pdf"
\`\`\`

### Ask Questions
\`\`\`bash
curl -X POST http://localhost:5678/webhook/ask \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What does the document say about AI?"
  }'
\`\`\`

## Troubleshooting

### "Index not found"
- Ensure you created the vector index with the exact name: `vector_index`
- Check index exists: `db.pdf_documents.getIndexes()`

### "Dimension mismatch"
- OpenAI text-embedding-3-small produces 1536 dimensions
- Ensure your index matches this dimension count

### "Connection refused"
- Check MongoDB is running: `sudo service mongod status`
- Verify connection string includes full Atlas connection with credentials

Your real multimodal PDF agent is now ready!
```

# 80-upload-interface.mdx

```mdx
---
sidebar_position: 80
---

# üåê Web Upload Interface

A complete web interface for uploading PDFs and chatting with your multimodal agent.

## Accessing the Interface

### Option 1: Via Docusaurus (Recommended)

The upload interface is available at:
\`\`\`
http://localhost:3000/upload-interface.html
\`\`\`

### Option 2: Direct File Access

Open the file directly in your browser:
\`\`\`
/workspaces/multimodal-pdf-agent-n8n-docs/static/upload-interface.html
\`\`\`
(Note: The upload interface is in the documentation repository)

## Features

### üì§ PDF Upload
- **Drag & Drop** support
- **Click to browse** file selection
- **Automatic validation** (PDF files only)
- **Progress feedback** during processing

### üí¨ Chat Interface
- **Real-time Q&A** about uploaded PDFs
- **Conversation history** display
- **Loading indicators** for responses
- **Error handling** with clear messages

## Setup Requirements

### 1. Import the Workflow

First, import the real multimodal agent workflow:
\`\`\`
/workspaces/multimodal-pdf-agent-n8n/init/workflows/06-real-voyage-multimodal.json
\`\`\`

### 2. Configure Webhook URLs

The interface expects these webhook endpoints:
- **Upload**: `http://localhost:5678/webhook/pdf-upload`
- **Chat**: `http://localhost:5678/webhook/ask`

### 3. Start the Workflow

1. Open the imported workflow in n8n
2. Click **"Active"** toggle to enable
3. The webhooks are now ready to receive requests

## Using the Interface

### Step 1: Upload a PDF

1. **Drag a PDF** onto the upload area, or
2. **Click "Choose PDF File"** to browse
3. Wait for "Successfully processed" message
4. The chat interface will appear automatically

### Step 2: Ask Questions

1. Type your question in the input field
2. Press **Enter** or click **Ask**
3. Watch the AI analyze your PDF and respond
4. Continue the conversation with follow-up questions

## Example Questions

Try these types of questions:
- "What is the main topic of this document?"
- "Summarize the key findings"
- "What does it say about [specific topic]?"
- "List the main recommendations"

## Customization

### Modify the Interface

Edit `/static/upload-interface.html` (in the docs repository) to:
- Change colors and styling
- Add additional features
- Modify the layout

### Update Webhook URLs

If using different ports or paths:
\`\`\`javascript
const UPLOAD_URL = 'http://localhost:5678/webhook/pdf-upload';
const ASK_URL = 'http://localhost:5678/webhook/ask';
\`\`\`

## Troubleshooting

### "Failed to fetch" Error
- Ensure n8n workflow is active
- Check webhook URLs match your setup
- Verify MongoDB is running

### "No file uploaded" Error
- Ensure file is a valid PDF
- Check file size (keep under 10MB for testing)
- Verify binary data handling in webhook

### Chat Not Appearing
- Check browser console for errors
- Ensure PDF was processed successfully
- Verify MongoDB stored the document

## Production Deployment

For production use:
1. Update URLs to use HTTPS
2. Add authentication/security
3. Implement rate limiting
4. Add file size restrictions
5. Enable CORS properly

Your complete multimodal PDF agent is now accessible via a user-friendly web interface!
```

# 85-python-mongodb-approaches.mdx

```mdx
---
sidebar_position: 85
---

# üêç Python Approaches to MongoDB Vector Search

While n8n provides an excellent visual workflow approach, developers often need programmatic control. Let's explore different ways to interact with MongoDB vector search and Voyage AI using Python.

<InstructorNotes 
  timing="Python Integration Overview (15-20 minutes)"
  notes={[
    "This section complements the n8n approach - not a replacement",
    "Great for developers who want to understand what's happening 'under the hood'",
    "Python examples help attendees bridge visual workflows to code",
    "MongoDB's new multimodal search library simplifies a lot of complexity",
    "Some attendees may prefer Python for production systems"
  ]}
  tips={[
    "Emphasize that both approaches (n8n and Python) can coexist",
    "Show how n8n workflows can call Python scripts via HTTP",
    "Demonstrate the MongoDB multimodal search library live if time permits",
    "Point out that the concepts learned in n8n transfer directly to Python",
    "Consider this section optional if workshop is running behind schedule"
  ]}
/>

<SlideRecap 
  title="From Visual to Programmatic"
  items={[
    {
      icon: "üîÑ",
      title: "Multiple Interfaces",
      description: "n8n workflows, Python SDKs, and direct MongoDB drivers all access the same powerful vector search"
    },
    {
      icon: "üêç", 
      title: "Python Ecosystem",
      description: "Rich libraries for document processing, ML model integration, and data analysis"
    },
    {
      icon: "‚ö°",
      title: "Best of Both Worlds",
      description: "Use n8n for rapid prototyping, Python for custom logic and production systems"
    }
  ]}
  nextSection="Let's explore the different Python approaches available!"
/>

## üéØ Comparison: n8n vs Python Approaches

| Aspect | n8n Visual Workflows | Python Programming |
|--------|---------------------|-------------------|
| **Learning Curve** | Low - visual interface | Medium - requires Python knowledge |
| **Development Speed** | Fast prototyping | Slower initial setup |
| **Customization** | Limited to available nodes | Unlimited flexibility |
| **Debugging** | Visual execution flow | Traditional debugging tools |
| **Integration** | 400+ pre-built integrations | Vast Python ecosystem |
| **Production** | Enterprise-ready | Full control over deployment |
| **Team Collaboration** | Non-technical team friendly | Requires developer skills |

## üöÄ MongoDB Multimodal Search Library

MongoDB's new Python library simplifies multimodal vector search with a clean, intuitive API.

### Installation

\`\`\`python
pip install mongodb-multimodal-search
\`\`\`

### Basic Setup

\`\`\`python
from mongodb_multimodal_search import MultimodalSearch
from pymongo import MongoClient

# Initialize MongoDB connection
client = MongoClient("your-mongodb-connection-string")
db = client["multimodal_workshop"]
collection = db["pdf_documents"]

# Initialize multimodal search
search = MultimodalSearch(
    collection=collection,
    embedding_provider="voyage",  # or "openai", "cohere"
    embedding_model="voyage-multimodal-3",
    api_key="your-voyage-api-key"
)
\`\`\`

### Document Ingestion

\`\`\`python
# Process a PDF document
document_results = await search.ingest_document(
    file_path="sample.pdf",
    document_metadata={
        "source": "workshop",
        "category": "tutorial",
        "processed_date": datetime.now()
    },
    chunk_size=1000,  # Text chunk size
    overlap=200       # Chunk overlap
)

print(f"Processed {document_results.total_chunks} chunks")
print(f"Generated {document_results.total_embeddings} embeddings")
\`\`\`

### Vector Search

\`\`\`python
# Perform semantic search
results = await search.semantic_search(
    query="How do I create a vector index?",
    limit=5,
    filters={
        "metadata.category": "tutorial"
    },
    include_scores=True
)

for result in results:
    print(f"Score: {result.score:.3f}")
    print(f"Content: {result.content[:200]}...")
    print(f"Metadata: {result.metadata}")
    print("---")
\`\`\`

### Multimodal Queries

\`\`\`python
# Search with both text and image
from PIL import Image

image = Image.open("query_image.png")

results = await search.multimodal_search(
    text_query="architectural diagrams",
    image_query=image,
    limit=3,
    combine_strategy="weighted",  # "text_priority", "image_priority", "balanced"
    weights={"text": 0.6, "image": 0.4}
)
\`\`\`

## üîß Direct PyMongo Approach

For maximum control, you can interact directly with MongoDB using PyMongo and handle embeddings manually.

### Vector Search Pipeline

\`\`\`python
from pymongo import MongoClient
import requests

class VectorSearchClient:
    def __init__(self, connection_string, database, collection):
        self.client = MongoClient(connection_string)
        self.db = self.client[database]
        self.collection = self.db[collection]
    
    async def generate_embedding(self, content, content_type="text"):
        """Generate embeddings using Voyage AI API"""
        response = requests.post(
            "https://api.voyageai.com/v1/embeddings",
            headers={
                "Authorization": f"Bearer {voyage_api_key}",
                "Content-Type": "application/json"
            },
            json={
                "input": content,
                "model": "voyage-multimodal-3",
                "input_type": content_type
            }
        )
        return response.json()["data"][0]["embedding"]
    
    async def vector_search(self, query_text, limit=5):
        """Perform vector search using MongoDB aggregation"""
        # Generate query embedding
        query_embedding = await self.generate_embedding(query_text)
        
        # MongoDB vector search pipeline
        pipeline = [
            {
                "$vectorSearch": {
                    "index": "vector_index",
                    "path": "embedding",
                    "queryVector": query_embedding,
                    "numCandidates": limit * 10,
                    "limit": limit
                }
            },
            {
                "$project": {
                    "content": 1,
                    "metadata": 1,
                    "score": {"$meta": "vectorSearchScore"}
                }
            }
        ]
        
        return list(self.collection.aggregate(pipeline))

# Usage
search_client = VectorSearchClient(
    connection_string="mongodb+srv://...",
    database="multimodal_workshop",
    collection="pdf_documents"
)

results = await search_client.vector_search("machine learning concepts")
\`\`\`

### Advanced Filtering

\`\`\`python
# Hybrid search with metadata filtering
pipeline = [
    {
        "$vectorSearch": {
            "index": "vector_index",
            "path": "embedding", 
            "queryVector": query_embedding,
            "numCandidates": 100,
            "limit": 20,
            "filter": {
                "metadata.document_type": {"$eq": "technical"},
                "metadata.date": {"$gte": datetime(2024, 1, 1)}
            }
        }
    },
    {
        "$match": {
            "score": {"$gte": 0.7}  # Minimum similarity threshold
        }
    },
    {
        "$lookup": {
            "from": "document_metadata",
            "localField": "document_id",
            "foreignField": "_id",
            "as": "full_metadata"
        }
    }
]
\`\`\`

## üåê LangChain Integration

Integrate MongoDB vector search with LangChain for advanced RAG applications.

### Setup

\`\`\`python
from langchain.vectorstores import MongoDBAtlasVectorSearch
from langchain.embeddings import VoyageAIEmbeddings
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Initialize embeddings
embeddings = VoyageAIEmbeddings(
    voyage_api_key="your-api-key",
    model="voyage-multimodal-3"
)

# Setup vector store
vector_store = MongoDBAtlasVectorSearch(
    collection=collection,
    embedding=embeddings,
    index_name="vector_index"
)
\`\`\`

### RAG Chain

\`\`\`python
from langchain.chains import RetrievalQA
from langchain.llms import GoogleGenerativeAI

# Setup retrieval chain
qa_chain = RetrievalQA.from_chain_type(
    llm=GoogleGenerativeAI(model="gemini-2.0-flash-exp"),
    chain_type="stuff",
    retriever=vector_store.as_retriever(
        search_kwargs={"k": 5, "score_threshold": 0.7}
    ),
    return_source_documents=True
)

# Ask questions
result = qa_chain.invoke({
    "query": "How do I optimize vector search performance?"
})

print(f"Answer: {result['result']}")
print(f"Sources: {len(result['source_documents'])} documents")
\`\`\`

## üîÑ Hybrid n8n + Python Approach

Combine the best of both worlds by using n8n for orchestration and Python for custom logic.

### Python Microservice

\`\`\`python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class SearchRequest(BaseModel):
    query: str
    filters: dict = {}
    limit: int = 5

@app.post("/advanced-search")
async def advanced_search(request: SearchRequest):
    """Custom search logic called from n8n"""
    
    # Custom preprocessing
    processed_query = preprocess_query(request.query)
    
    # Advanced search logic
    results = await custom_vector_search(
        query=processed_query,
        filters=request.filters,
        limit=request.limit
    )
    
    # Post-processing
    enhanced_results = enhance_results(results)
    
    return {
        "results": enhanced_results,
        "total": len(enhanced_results),
        "query_processed": processed_query
    }

# Custom functions
def preprocess_query(query: str) -> str:
    # Query expansion, spell check, etc.
    return enhanced_query

async def custom_vector_search(query, filters, limit):
    # Your custom search logic
    pass

def enhance_results(results):
    # Add custom scoring, ranking, metadata
    return enhanced_results
\`\`\`

### n8n HTTP Request Node

\`\`\`json
{
  "method": "POST",
  "url": "http://your-python-service/advanced-search",
  "body": {
    "query": "{{ $json.user_query }}",
    "filters": {
      "document_type": "{{ $json.doc_type }}",
      "date_range": "{{ $json.date_filter }}"
    },
    "limit": 10
  }
}
\`\`\`

## üìä Performance Comparison

| Approach | Setup Time | Development Speed | Performance | Scalability |
|----------|------------|------------------|-------------|-------------|
| **MongoDB Multimodal Library** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Direct PyMongo** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **LangChain Integration** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **n8n Visual Workflows** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |

## üéØ When to Choose Which Approach

### Use **n8n Visual Workflows** when:
- ‚úÖ Rapid prototyping and testing
- ‚úÖ Non-technical team members involved
- ‚úÖ Standard workflow patterns
- ‚úÖ Need many integrations quickly

### Use **MongoDB Multimodal Library** when:
- ‚úÖ Python-first development approach
- ‚úÖ Need multimodal capabilities out-of-the-box
- ‚úÖ Want MongoDB best practices built-in
- ‚úÖ Balancing simplicity with control

### Use **Direct PyMongo** when:
- ‚úÖ Maximum performance requirements
- ‚úÖ Complex custom logic needed
- ‚úÖ Fine-grained control over queries
- ‚úÖ Existing MongoDB infrastructure

### Use **Hybrid Approach** when:
- ‚úÖ Team has mixed skill levels
- ‚úÖ Need both visual workflows and custom logic
- ‚úÖ Want to leverage existing n8n infrastructure
- ‚úÖ Iterative development process

## üîó Code Examples Repository

All Python examples from this section are available in the workshop repository:

<QRCodeAccess 
  url="https://github.com/mrlynn/ai4-multimodal-agents-n8n/tree/main/python-examples"
  title="Python Examples Repository"
/>

\`\`\`bash
# Clone and explore Python examples
git clone https://github.com/mrlynn/ai4-multimodal-agents-n8n.git
cd multimodal-pdf-agent-n8n/python-examples

# Install dependencies
pip install -r requirements.txt

# Run examples
python multimodal_search_example.py
python direct_pymongo_example.py
python langchain_integration_example.py
\`\`\`

## üöÄ Next Steps

1. **Try the MongoDB Multimodal Library** - Start with the high-level API
2. **Experiment with Hybrid Approaches** - Combine n8n with Python microservices  
3. **Optimize for Your Use Case** - Choose the right tool for each task
4. **Join the Community** - Share your Python integration patterns

The power of MongoDB vector search is that it works seamlessly across all these approaches - choose the one that fits your team and project best!

<WorkshopResources 
  title="Python Integration Resources"
  categories={{
    "MongoDB Python Libraries": [
      {
        title: "MongoDB Multimodal Search Library",
        url: "https://github.com/mongodb-labs/multimodal-search-python",
        description: "High-level Python library for multimodal vector search",
        icon: "üêç"
      },
      {
        title: "PyMongo Documentation", 
        url: "https://pymongo.readthedocs.io/",
        description: "Official MongoDB Python driver documentation",
        icon: "üìö"
      },
      {
        title: "Motor (Async MongoDB)",
        url: "https://motor.readthedocs.io/",
        description: "Asynchronous Python driver for MongoDB",
        icon: "‚ö°"
      }
    ],
    "ML/AI Integration": [
      {
        title: "LangChain MongoDB Integration",
        url: "https://python.langchain.com/docs/integrations/vectorstores/mongodb_atlas",
        description: "LangChain vector store integration",
        icon: "ü¶ú"
      },
      {
        title: "Voyage AI Python SDK",
        url: "https://docs.voyageai.com/docs/python-sdk",
        description: "Official Python SDK for Voyage AI embeddings",
        icon: "üö¢"
      },
      {
        title: "Hugging Face Transformers",
        url: "https://huggingface.co/docs/transformers/",
        description: "Alternative embedding and model options",
        icon: "ü§ó"
      }
    ],
    "Development Tools": [
      {
        title: "Jupyter Notebooks Examples",
        url: "https://github.com/mrlynn/vector-search-examples",
        description: "Interactive Python examples and tutorials",
        icon: "üìì"
      },
      {
        title: "FastAPI Documentation",
        url: "https://fastapi.tiangolo.com/",
        description: "For building Python microservices",
        icon: "üöÄ"
      },
      {
        title: "Streamlit",
        url: "https://streamlit.io/",
        description: "Rapid Python web app development",
        icon: "üé®"
      }
    ]
  }}
/>
```

# 90-approach-comparison.mdx

```mdx
---
sidebar_position: 90
---

# ‚öñÔ∏è n8n vs Python: Choosing Your Development Approach

Now that you've seen both visual workflows and programmatic approaches, let's dive deep into when and why to choose each method for your multimodal PDF agent projects.

<InstructorNotes 
  timing="Approach Comparison (10-15 minutes)"
  notes={[
    "This section helps attendees make informed decisions for their own projects",
    "Many developers want to know 'what should I use in production?'",
    "Emphasize that both approaches can coexist in the same system",
    "Real-world examples resonate more than abstract comparisons",
    "Some attendees may want to rewrite everything in Python after seeing examples"
  ]}
  tips={[
    "Share your own experience with both approaches",
    "Ask attendees about their team's technical background",
    "Demonstrate how to migrate from n8n to Python gradually",
    "Show specific examples from production systems if possible",
    "Address concerns about vendor lock-in with visual workflow tools"
  ]}
/>

## üéØ Decision Framework

Use this framework to choose the right approach for your specific situation:

### Team & Skills Assessment

<WorkshopExercise 
  title="Assess Your Development Context" 
  difficulty="beginner"
  timeEstimate="5 minutes"
  objectives={[
    "Evaluate your team's technical capabilities",
    "Identify project requirements and constraints",
    "Choose the most appropriate development approach"
  ]}
>

<ExerciseStep stepNumber="1" title="Team Skills Matrix">

Rate your team's capabilities (1-5 scale):

| Skill Area | Rating | n8n Advantage | Python Advantage |
|------------|--------|---------------|------------------|
| **Visual Thinking** | ___/5 | ‚úÖ Drag-and-drop workflows | ‚ùå Abstract code concepts |
| **Python Programming** | ___/5 | ‚ùå Limited to available nodes | ‚úÖ Unlimited flexibility |
| **API Integration** | ___/5 | ‚úÖ 400+ pre-built connectors | ‚ö†Ô∏è Manual integration work |
| **Debugging Skills** | ___/5 | ‚úÖ Visual execution flow | ‚úÖ Traditional debugging tools |
| **DevOps/Deployment** | ___/5 | ‚ö†Ô∏è n8n infrastructure required | ‚úÖ Standard containerization |

</ExerciseStep>

<ExerciseStep stepNumber="2" title="Project Requirements Analysis">

Check all that apply to your project:

**Business Requirements:**
- [ ] Rapid prototyping needed
- [ ] Non-technical stakeholders involved
- [ ] Frequent workflow changes expected
- [ ] Complex business logic required
- [ ] High-performance requirements
- [ ] Regulatory compliance needed

**Technical Requirements:**
- [ ] Custom ML model integration
- [ ] Complex data transformations
- [ ] Real-time processing (less than 100ms)
- [ ] Batch processing (over 1M documents)
- [ ] Multi-language support
- [ ] Advanced error handling

</ExerciseStep>

<ExerciseValidation 
  title="Approach Recommendation"
  checks={[
    {
      id: "skills_assessed",
      description: "Completed team skills assessment",
      hint: "Be honest about current capabilities vs aspirational goals"
    },
    {
      id: "requirements_analyzed",
      description: "Identified key project requirements",
      hint: "Focus on must-have features, not nice-to-have ones"
    },
    {
      id: "approach_selected",
      description: "Selected primary development approach based on analysis",
      hint: "Remember you can always start with one and migrate later"
    }
  ]}
/>

</WorkshopExercise>

## üèóÔ∏è Architecture Patterns

### Pattern 1: Pure n8n Visual Workflows

**Best for:** Rapid prototyping, business user involvement, standard use cases

\`\`\`mermaid
graph TD
    A[PDF Upload] --> B[n8n Webhook]
    B --> C[n8n PDF Processing]
    C --> D[n8n Voyage AI]
    D --> E[n8n MongoDB]
    E --> F[n8n Gemini Agent]
    F --> G[Response]
    
    style B fill:#667eea
    style C fill:#667eea
    style D fill:#667eea
    style E fill:#667eea
    style F fill:#667eea
\`\`\`

**Implementation Time:** 2-4 hours
**Maintenance:** Low
**Customization:** Limited to available nodes

### Pattern 2: Pure Python Development

**Best for:** Maximum control, complex logic, performance optimization

\`\`\`mermaid
graph TD
    A[PDF Upload] --> B[FastAPI Endpoint]
    B --> C[Python PDF Parser]
    C --> D[Voyage AI SDK]
    D --> E[MongoDB PyMongo]
    E --> F[Gemini API]
    F --> G[Response]
    
    style B fill:#10b981
    style C fill:#10b981
    style D fill:#10b981
    style E fill:#10b981
    style F fill:#10b981
\`\`\`

**Implementation Time:** 1-2 weeks
**Maintenance:** Medium-High
**Customization:** Unlimited

### Pattern 3: Hybrid Architecture

**Best for:** Team with mixed skills, gradual migration, leveraging strengths

\`\`\`mermaid
graph TD
    A[PDF Upload] --> B[n8n Orchestration]
    B --> C[Python Microservice<br/>PDF Processing]
    B --> D[n8n Voyage AI]
    D --> E[Python Microservice<br/>Vector Search]
    E --> F[n8n Gemini Agent]
    F --> G[Response]
    
    style B fill:#667eea
    style D fill:#667eea
    style F fill:#667eea
    style C fill:#10b981
    style E fill:#10b981
\`\`\`

**Implementation Time:** 1 week
**Maintenance:** Medium
**Customization:** High where needed

## üìä Real-World Performance Comparison

Based on production deployments:

### Processing Speed (1000 PDF pages)

| Approach | Setup | Processing | Query Response | Total |
|----------|-------|------------|----------------|--------|
| **n8n Visual** | 5 min | 45 min | 200ms | ~50 min |
| **Python SDK** | 30 min | 35 min | 150ms | ~65 min |
| **PyMongo Direct** | 60 min | 25 min | 100ms | ~85 min |
| **Hybrid** | 20 min | 40 min | 120ms | ~60 min |

### Cost Analysis (Monthly, 10K queries)

| Component | n8n | Python | Hybrid |
|-----------|-----|--------|--------|
| **Compute** | $50 (n8n Cloud) | $30 (AWS/GCP) | $40 |
| **Development** | $500 (2 days) | $2000 (8 days) | $1000 (4 days) |
| **Maintenance** | $200/month | $800/month | $400/month |
| **First Year Total** | $3,400 | $13,600 | $7,800 |

## üéØ Migration Strategies

### Strategy 1: Start Visual, Migrate Gradually

**Phase 1: n8n Prototype (Week 1)**
\`\`\`javascript
// n8n workflow handles everything
Webhook ‚Üí PDF ‚Üí Embeddings ‚Üí MongoDB ‚Üí Agent
\`\`\`

**Phase 2: Extract Custom Logic (Week 2-3)**
\`\`\`javascript
// Move complex processing to Python
Webhook ‚Üí [Python Service] ‚Üí n8n Embeddings ‚Üí MongoDB ‚Üí Agent
\`\`\`

**Phase 3: Full Migration (Month 2-3)**
\`\`\`python
# Full Python implementation
FastAPI ‚Üí Custom Processing ‚Üí MongoDB ‚Üí Custom Agent
\`\`\`

### Strategy 2: Hybrid from Start

**Core n8n Orchestration:**
\`\`\`json
{
  "nodes": [
    {
      "type": "webhook",
      "name": "PDF Upload"
    },
    {
      "type": "http-request", 
      "name": "Python PDF Processor",
      "url": "http://pdf-service/process"
    },
    {
      "type": "voyage-ai",
      "name": "Generate Embeddings" 
    },
    {
      "type": "http-request",
      "name": "Custom Vector Search",
      "url": "http://search-service/query"
    }
  ]
}
\`\`\`

**Python Microservices:**
\`\`\`python
# pdf-service/main.py
@app.post("/process")
async def process_pdf(file: UploadFile):
    # Custom PDF processing logic
    return {"pages": processed_pages}

# search-service/main.py  
@app.post("/query")
async def vector_search(query: SearchQuery):
    # Custom search logic
    return {"results": search_results}
\`\`\`

## üöÄ Production Deployment Patterns

### n8n Production Deployment

\`\`\`yaml
# docker-compose.production.yml
version: '3.8'
services:
  n8n:
    image: n8nio/n8n
    environment:
      - N8N_HOST=your-domain.com
      - N8N_PROTOCOL=https
      - N8N_ENCRYPTION_KEY=${ENCRYPTION_KEY}
    volumes:
      - n8n_data:/home/node/.n8n
    networks:
      - n8n-network
      
  traefik:
    image: traefik:v2.9
    command:
      - "--providers.docker=true"
      - "--entrypoints.web.address=:80"
      - "--certificatesresolvers.letsencrypt.acme.httpchallenge=true"
\`\`\`

### Python Production Deployment

\`\`\`python
# kubernetes/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multimodal-agent
spec:
  replicas: 3
  selector:
    matchLabels:
      app: multimodal-agent
  template:
    spec:
      containers:
      - name: agent
        image: your-registry/multimodal-agent:latest
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi" 
            cpu: "500m"
        env:
        - name: MONGODB_URI
          valueFrom:
            secretKeyRef:
              name: mongo-secret
              key: connection-string
\`\`\`

### Monitoring & Observability

**n8n Monitoring:**
\`\`\`javascript
// n8n webhook for monitoring
{
  "webhook_url": "https://hooks.slack.com/...",
  "on_error": "notify_team",
  "on_success": "log_metrics",
  "execution_timeout": "5m"
}
\`\`\`

**Python Monitoring:**
\`\`\`python
from prometheus_client import Counter, Histogram
import logging

# Metrics
REQUESTS_TOTAL = Counter('requests_total', 'Total requests')
REQUEST_DURATION = Histogram('request_duration_seconds', 'Request duration')

@app.middleware("http")
async def monitor_requests(request: Request, call_next):
    start_time = time.time()
    REQUESTS_TOTAL.inc()
    
    response = await call_next(request)
    
    REQUEST_DURATION.observe(time.time() - start_time)
    return response
\`\`\`

## üìà Success Stories & Case Studies

### Case Study 1: Legal Document Analysis (n8n)

**Company:** Mid-size law firm  
**Challenge:** Process 1000+ legal documents daily  
**Solution:** Pure n8n workflow  
**Results:**
- ‚úÖ 3-day implementation
- ‚úÖ 90% time savings
- ‚úÖ Non-technical staff can modify workflows
- ‚úÖ $50K annual savings

**Architecture:**
\`\`\`
Legal Docs ‚Üí n8n ‚Üí Text Extraction ‚Üí Voyage AI ‚Üí MongoDB ‚Üí Legal AI Agent
\`\`\`

### Case Study 2: Medical Research Platform (Python)

**Company:** Healthcare research institute  
**Challenge:** Complex multimodal analysis of medical literature  
**Solution:** Custom Python application  
**Results:**
- ‚úÖ 40% better accuracy vs standard tools
- ‚úÖ Custom medical entity recognition
- ‚úÖ Integration with existing Python ML pipeline
- ‚úÖ HIPAA compliance controls

**Architecture:**
\`\`\`python
Medical PDFs ‚Üí Custom Parser ‚Üí BioBERT + Voyage ‚Üí MongoDB ‚Üí Research Agent
\`\`\`

### Case Study 3: Corporate Knowledge Base (Hybrid)

**Company:** Fortune 500 consulting firm  
**Challenge:** Mixed team skills, rapid iteration needed  
**Solution:** n8n + Python microservices  
**Results:**
- ‚úÖ Fast prototyping with n8n
- ‚úÖ Custom logic where needed
- ‚úÖ Easy stakeholder demos
- ‚úÖ Scalable architecture

**Architecture:**
\`\`\`
Documents ‚Üí n8n Orchestration ‚Üí Python Processing ‚Üí n8n Workflows ‚Üí Custom Agent
\`\`\`

## üéØ Decision Matrix

Use this scoring system (1-5) to evaluate approaches for your project:

| Criteria | Weight | n8n | Python | Hybrid |
|----------|--------|-----|--------|--------|
| **Team Python Skills** | 20% | 2 | 5 | 4 |
| **Development Speed** | 25% | 5 | 2 | 4 |
| **Customization Needs** | 20% | 2 | 5 | 4 |
| **Maintenance Burden** | 15% | 5 | 2 | 3 |
| **Performance Requirements** | 10% | 3 | 5 | 4 |
| **Budget Constraints** | 10% | 4 | 3 | 3 |

**Calculate your score:**
\`\`\`python
def calculate_approach_score(weights, ratings):
    return sum(w * r for w, r in zip(weights, ratings)) / sum(weights)

# Example calculation
n8n_score = calculate_approach_score(
    weights=[0.2, 0.25, 0.2, 0.15, 0.1, 0.1],
    ratings=[2, 5, 2, 5, 3, 4]
)  # Result: 3.45
\`\`\`

## üîÑ Evolution Path

**Month 1-2: Proof of Concept**
- Start with n8n for rapid validation
- Test core functionality
- Get stakeholder buy-in

**Month 3-6: Production MVP**
- Decide on approach based on learnings
- Implement core features
- Basic monitoring and error handling

**Month 6-12: Scale & Optimize**
- Performance optimization
- Advanced features
- Comprehensive monitoring

**Year 2+: Advanced Features**
- ML model improvements
- Multi-language support
- Advanced analytics

## üéâ Making Your Choice

The "best" approach is the one that:
1. **Matches your team's skills** and comfort level
2. **Meets your performance requirements** adequately
3. **Fits your timeline** and budget constraints
4. **Allows for future growth** and changes

Remember: You can always start with one approach and evolve. Many successful projects begin with n8n prototypes and gradually introduce Python components as needs become more sophisticated.

<Quiz 
  title="Approach Selection Quiz"
  passingScore={80}
  questions={[
    {
      question: "Your team has strong Python skills but tight deadlines. What's the best initial approach?",
      options: [
        "Pure Python - leverage existing skills",
        "Pure n8n - fastest time to market", 
        "Hybrid - Python for core logic, n8n for orchestration",
        "Wait and hire n8n specialists"
      ],
      correctAnswer: 1,
      explanation: "With tight deadlines, n8n's visual workflows provide the fastest path to a working prototype, even for Python-skilled teams."
    },
    {
      question: "You need sub-100ms query responses and complex custom scoring. Which approach?",
      options: [
        "n8n visual workflows",
        "MongoDB Multimodal Search Library",
        "Direct PyMongo with custom optimizations",
        "LangChain integration"
      ],
      correctAnswer: 2,
      explanation: "For maximum performance and custom logic, direct PyMongo gives you complete control over queries and optimizations."
    },
    {
      question: "Your business users want to modify workflows themselves. Best choice?",
      options: [
        "Python with configuration files",
        "n8n visual workflows", 
        "Hybrid with n8n frontend",
        "LangChain with YAML configs"
      ],
      correctAnswer: 1,
      explanation: "n8n's visual interface allows non-technical users to understand and modify workflows without coding."
    }
  ]}
/>

The power of modern vector search is that your choice of development approach doesn't limit your capabilities - MongoDB Atlas, Voyage AI, and Gemini work seamlessly across all these patterns! üöÄ
```

# 95-community-resources.mdx

```mdx
---
sidebar_position: 95
---

# üåê Community Nodes & Extensions

Extend your PDF analysis system with community contributions!

## What Are Community Nodes?

Community nodes are extensions created by n8n users that add new integrations and capabilities.

## Installing Community Nodes

### 1. Access Community Nodes
1. Go to **Settings** ‚Üí **Community Nodes**
2. Click **Install a community node**
3. Enter the npm package name

### 2. Browse Available Nodes
Click **Browse** to see popular nodes:

- ü§ñ **AI/ML Nodes**: DeepSeek, Claude, Local LLMs
- üìä **Database Nodes**: Specialized connectors
- üîß **Utility Nodes**: Advanced processors
- üåê **API Nodes**: Service integrations

## Recommended Nodes for PDF Enhancement

### 1. n8n-nodes-deepseek
For alternative AI processing:

\`\`\`bash
npm install n8n-nodes-deepseek
\`\`\`

Use cases:
- Cost-effective alternative to OpenAI
- Specialized reasoning tasks
- Multi-language support

### 2. n8n-nodes-mcp

Model Context Protocol integration:

\`\`\`bash
npm install n8n-nodes-mcp
\`\`\`

Use cases:
- Advanced context management
- Multi-model orchestration
- Tool use capabilities

### 3. n8n-nodes-ocr-advanced

Enhanced OCR capabilities:

\`\`\`bash
npm install n8n-nodes-ocr-advanced
\`\`\`

Use cases:
- Extract text from images
- Handle scanned PDFs
- Multi-language OCR

## Creating Your Own Node

### Basic Structure

\`\`\`typescript
import {
  IExecuteFunctions,
  INodeExecutionData,
  INodeType,
  INodeTypeDescription,
} from 'n8n-workflow';

export class MyPdfNode implements INodeType {
  description: INodeTypeDescription = {
    displayName: 'My PDF Processor',
    name: 'myPdfProcessor',
    group: ['transform'],
    version: 1,
    description: 'Custom PDF processing',
    defaults: {
      name: 'My PDF Processor',
    },
    inputs: ['main'],
    outputs: ['main'],
    properties: [
      // Node properties
    ],
  };

  async execute(this: IExecuteFunctions): Promise<INodeExecutionData[][]> {
    // Your logic here
    return this.prepareOutputData(items);
  }
}
\`\`\`

### Publishing Your Node

1. Create npm package
2. Follow n8n node structure
3. Publish to npm
4. Share with community!

## Integration Ideas

### 1. Language Detection Node

\`\`\`javascript
// Detect PDF language and route accordingly
const language = await detectLanguage(pdfText);
switch(language) {
  case 'es': // Route to Spanish workflow
  case 'en': // Route to English workflow
  // etc.
}
\`\`\`

### 2. Image Enhancement Node

\`\`\`javascript
// Enhance images before embedding
const enhanced = await enhanceImage(imageBuffer);
// Better OCR results
// Clearer embeddings
\`\`\`

### 3. Compliance Check Node

\`\`\`javascript
// Check for sensitive information
const compliance = await checkCompliance(pdfContent);
if (compliance.hasPII) {
  // Redact or flag
}
\`\`\`

## Community Resources

### n8n Community Forum

- [https://community.n8n.io](https://community.n8n.io/)
- Share workflows
- Get help
- Find nodes

### Node Development

- [n8n Node Development Guide](https://docs.n8n.io/integrations/creating-nodes/)
- [Example Nodes](https://github.com/n8n-io/n8n/tree/master/packages/nodes-base/nodes)
- [Community Node Template](https://github.com/n8n-io/n8n-nodes-starter)

## Best Practices

### 1. Verify Node Quality

- Check npm downloads
- Review source code
- Test in development first

### 2. Security Considerations

- Audit node code
- Use specific versions
- Monitor for updates

### 3. Performance Impact

- Test with your workload
- Monitor execution time
- Consider caching

## Workshop Challenge

### Create a Custom Node

Try creating a simple node that:

1. Accepts PDF metadata
2. Adds custom fields
3. Formats for your use case

Example:

\`\`\`javascript
// Add reading level analysis
const readingLevel = analyzeReadingLevel(text);
item.json.readingLevel = readingLevel;
item.json.complexity = calculateComplexity(text);
\`\`\`

Share your nodes with the community!
```

# 95-docker-troubleshooting.mdx

```mdx
---
sidebar_position: 95
---

# üê≥ Docker Troubleshooting Guide

Common issues and solutions when running the workshop with Docker.

## üö® Service Won't Start

### Docker Desktop Not Running

**Symptoms:**
\`\`\`
Cannot connect to the Docker daemon at unix:///var/run/docker.sock
\`\`\`

**Solution:**
\`\`\`bash
# macOS
open -a Docker

# Windows
# Start Docker Desktop from Start Menu

# Linux
sudo systemctl start docker
\`\`\`

### Port Already in Use

**Symptoms:**
\`\`\`
Error: bind: address already in use
\`\`\`

**Solution:**
\`\`\`bash
# Find what's using the port
lsof -i :5678    # macOS/Linux
netstat -ano | findstr :5678  # Windows

# Kill the process
kill -9 <PID>    # macOS/Linux
taskkill /PID <PID> /F  # Windows

# Or change the port in docker-compose.yml
ports:
  - "5679:5678"  # Use 5679 instead
\`\`\`

## üîß Container Issues

### n8n Container Keeps Restarting

**Check logs:**
\`\`\`bash
docker-compose logs n8n
\`\`\`

**Common causes:**
1. **MongoDB not ready**
   \`\`\`bash
   # Restart in correct order
   docker-compose down
   docker-compose up -d n8n
   sleep 10
   docker-compose up -d n8n
   \`\`\`

2. **Invalid environment variables**
   \`\`\`bash
   # Check .env file
   cat .env
   # Ensure no spaces around = signs
   \`\`\`

### Cannot Connect to Services

**Verify containers are running:**
\`\`\`bash
docker-compose ps
\`\`\`

**Check container health:**
\`\`\`bash
docker-compose ps | grep healthy
\`\`\`

**Test connectivity:**
\`\`\`bash
# From host
curl http://localhost:5678
curl http://localhost:5678

# From inside container
docker-compose exec n8n wget -O- http://localhost:5678
\`\`\`

## üíæ Data & Volume Issues

### Lost Workflows After Restart

**Ensure volumes are properly mounted:**
\`\`\`bash
# Check volumes
docker volume ls | grep workshop

# Inspect volume
docker volume inspect workshop_n8n_data
\`\`\`

**Fix permission issues:**
\`\`\`bash
# Linux/macOS
sudo chown -R $(whoami):$(whoami) ./volumes

# Create directories if missing
mkdir -p volumes/n8n volumes/mongodb
\`\`\`

### MongoDB Connection Failed

**From n8n container:**
\`\`\`bash
# Test MongoDB connection
docker-compose exec n8n sh
ping mongodb  # Should resolve
exit
\`\`\`

**Check MongoDB logs:**
\`\`\`bash
docker-compose logs n8n | grep -i error
\`\`\`

## üåê Network Problems

### Services Can't Communicate

**Verify network exists:**
\`\`\`bash
docker network ls | grep workshop
\`\`\`

**Recreate network:**
\`\`\`bash
docker-compose down
docker network prune
docker-compose up -d
\`\`\`

### External API Calls Failing

**Check DNS from container:**
\`\`\`bash
docker-compose exec n8n nslookup google.com
\`\`\`

**Proxy issues:**
\`\`\`bash
# Add to docker-compose.yml
environment:
  - HTTP_PROXY=${HTTP_PROXY}
  - HTTPS_PROXY=${HTTPS_PROXY}
  - NO_PROXY=localhost
\`\`\`

## üßπ Clean Slate Reset

### Complete Reset

\`\`\`bash
# Stop everything
docker-compose down -v

# Remove all workshop containers
docker ps -a | grep workshop | awk '{print $1}' | xargs docker rm -f

# Remove all workshop images
docker images | grep workshop | awk '{print $3}' | xargs docker rmi -f

# Remove all workshop volumes
docker volume ls | grep workshop | awk '{print $2}' | xargs docker volume rm

# Start fresh
docker-compose up -d
\`\`\`

## üìä Performance Issues

### Containers Running Slowly

**Check resource usage:**
\`\`\`bash
docker stats
\`\`\`

**Increase memory limits:**
\`\`\`yaml
# docker-compose.yml
services:
  n8n:
    deploy:
      resources:
        limits:
          memory: 4G  # Increase from 2G
\`\`\`

**Clean up Docker:**
\`\`\`bash
# Remove unused data
docker system prune -a

# Check disk space
df -h
\`\`\`

## üîç Debugging Tips

### Enable Debug Logging

\`\`\`yaml
# docker-compose.yml
services:
  n8n:
    environment:
      - N8N_LOG_LEVEL=debug
      - DEBUG=n8n:*
\`\`\`

### Access Container Shell

\`\`\`bash
# n8n container
docker-compose exec n8n sh

# n8n container
docker-compose exec n8n sh
\`\`\`

### Export Logs

\`\`\`bash
# Save all logs
docker-compose logs > workshop_debug_$(date +%Y%m%d_%H%M%S).log

# Follow specific service logs
docker-compose logs -f n8n
\`\`\`

## üÜò Emergency Commands

### When Nothing Else Works

\`\`\`bash
# 1. Stop Docker Desktop completely
# 2. Restart your computer
# 3. Start Docker Desktop
# 4. Run these commands:

cd workshop
docker system prune -a --volumes
docker-compose build --no-cache
docker-compose up -d
\`\`\`

## üìù Checklist Before Asking for Help

1. ‚úÖ Docker Desktop is running
2. ‚úÖ You're in the correct directory
3. ‚úÖ .env file exists and is configured
4. ‚úÖ Checked `docker-compose logs`
5. ‚úÖ Tried the clean slate reset
6. ‚úÖ Collected error messages

### Information to Provide

\`\`\`bash
# System info
docker version
docker-compose version
uname -a  # or systeminfo on Windows

# Workshop status
docker-compose ps
docker-compose logs --tail 50

# Environment
cat .env | grep -v PASSWORD
\`\`\`

---

Still having issues? Check the workshop repository issues or ask in the workshop chat!
```

# index.mdx

```mdx
---
sidebar_position: 1
slug: /
title: Workshop Overview
---

# üéì Multimodal PDF Agent Workshop

Welcome to the **Build a Multimodal PDF Agent with n8n** workshop! This interactive, hands-on experience will guide you through creating a production-ready AI system that processes PDFs using cutting-edge technologies.

<WorkshopTransition 
  slideTopics={[
    "Workshop overview and learning objectives",
    "Multimodal AI and embedding concepts", 
    "Architecture: n8n, MongoDB Atlas, Voyage AI",
    "What we'll build together today"
  ]}
  instructor="Workshop Instructor"
/>

<InstructorNotes 
  timing="Transition from slides to hands-on content (2-3 minutes)"
  notes={[
    "Ensure all attendees can access this documentation URL",
    "Remind them to bookmark this page for future reference",
    "Check that Codespaces users have services running",
    "Ask if anyone needs help with local Docker setup"
  ]}
  tips={[
    "Use screen sharing to show the navigation",
    "Encourage questions before diving into technical content",
    "Mention they can return to this overview page anytime"
  ]}
/>

<ProgressTracker steps={[
  {
    title: "Environment Setup",
    description: "Set up Docker, n8n, and MongoDB Atlas",
    timeEstimate: "15 minutes",
    difficulty: "beginner"
  },
  {
    title: "PDF Processing Workflow", 
    description: "Build workflow to extract and process PDF content",
    timeEstimate: "25 minutes",
    difficulty: "intermediate"
  },
  {
    title: "Vector Search Implementation",
    description: "Configure MongoDB Atlas Vector Search",
    timeEstimate: "20 minutes", 
    difficulty: "intermediate"
  },
  {
    title: "AI Agent Creation",
    description: "Build intelligent agent with tool calling",
    timeEstimate: "30 minutes",
    difficulty: "advanced"
  },
  {
    title: "Memory & Context",
    description: "Add conversation history and context management",
    timeEstimate: "20 minutes",
    difficulty: "advanced"
  },
  {
    title: "Production Deployment",
    description: "Deploy and scale your multimodal agent",
    timeEstimate: "15 minutes",
    difficulty: "intermediate"
  }
]} />

## üöÄ Workshop Sections

### [üöÄ Getting Started](/docs/intro)
Begin your journey with environment setup and n8n basics. Choose between GitHub Codespaces for instant setup or local Docker development.

### [‚öôÔ∏è Setup & Configuration](/docs/mongodb-atlas-setup)
Configure MongoDB Atlas for vector search and set up Voyage AI for multimodal embeddings. These are the foundational services for your agent.

### [üîß Building Workflows](/docs/pdf-processing-workflow)
Create powerful automation workflows step by step. Start with PDF processing, implement vector search, and build an AI agent with tool calling.

### [üéØ Advanced Topics](/docs/complete-multimodal-agent)
Take your agent to production with advanced features like memory management, web interfaces, and production-grade vector search.

### [üìö Resources & Support](/docs/local-setup-tips)
Find Docker best practices, troubleshooting guides, and community resources to help you succeed.

## üéØ What You'll Build

By the end of this workshop, you'll have created:

- **PDF Ingestion Pipeline**: Automated processing of documents
- **Multimodal Embeddings**: Text and image understanding with Voyage AI
- **Vector Search API**: Fast similarity search with MongoDB Atlas
- **AI Agent**: Intelligent question answering with Gemini 2.0
- **Production Workflows**: Error handling, monitoring, and scaling

## üõ†Ô∏è Technologies Used

<div style={{display: 'flex', gap: '2rem', flexWrap: 'wrap', marginTop: '2rem'}}>
  <div style={{textAlign: 'center'}}>
    <img src="/img/n8n-logo.png" alt="n8n" width="80" />
    <p><strong>n8n</strong><br/>Visual Workflow Automation</p>
  </div>
  <div style={{textAlign: 'center'}}>
    <img src="/img/mongodb-logo.png" alt="MongoDB" width="80" />
    <p><strong>MongoDB Atlas</strong><br/>Vector Database</p>
  </div>
  <div style={{textAlign: 'center'}}>
    <img src="/img/voyage-logo.png" alt="Voyage AI" width="80" />
    <p><strong>Voyage AI</strong><br/>Multimodal Embeddings</p>
  </div>
  <div style={{textAlign: 'center'}}>
    <img src="/img/gemini-logo.png" alt="Gemini" width="80" />
    <p><strong>Gemini 2.0</strong><br/>AI Agent Capabilities</p>
  </div>
</div>

## üèÅ Ready to Start?

<WorkshopExercise 
  title="Pre-Workshop Checklist" 
  difficulty="beginner"
  timeEstimate="5 minutes"
  objectives={[
    "Verify you have all prerequisites",
    "Choose your development environment",
    "Get excited about building!"
  ]}
>

<ExerciseValidation 
  title="Before You Begin"
  checks={[
    {
      id: "github_account",
      description: "I have a GitHub account (for Codespaces) or Docker Desktop installed",
      hint: "GitHub account is free at github.com, Docker Desktop from docker.com"
    },
    {
      id: "mongodb_ready",
      description: "I'm ready to create a MongoDB Atlas account (free tier)",
      hint: "You'll need an email address for signup"
    },
    {
      id: "time_allocated",
      description: "I have ~2 hours for the complete workshop",
      hint: "You can also complete it in sections"
    },
    {
      id: "excitement_level",
      description: "I'm excited to build a multimodal AI agent! üöÄ",
      hint: "This is the most important prerequisite!"
    }
  ]}
/>

</WorkshopExercise>

[**Start the Workshop ‚Üí**](/docs/intro)

---

## üì± Workshop Access & Sharing

<QRCodeAccess 
  url={typeof window !== 'undefined' ? window.location.origin + '/docs/' : 'https://your-workshop-docs.vercel.app/docs/'}
  title="Share Workshop Materials"
/>

<InstructorNotes 
  timing="End of overview (1 minute)"
  notes={[
    "Show attendees how to bookmark this page",
    "Mention they can scan QR code for mobile access",
    "Remind them this documentation stays available after workshop"
  ]}
  tips={[
    "Consider sharing the URL in chat for easy copy/paste",
    "Encourage attendees to take notes directly in their own docs",
    "Mention the progress tracking will persist in their browser"
  ]}
/>
```

# summary.mdx

```mdx
---
sidebar_position: 100
---

# üéØ Workshop Summary

Congratulations! Following this tutorial, you have successfully:

‚úÖ **Built a complete multimodal PDF processing system**  
‚úÖ **Implemented vector search with MongoDB Atlas**  
‚úÖ **Created an AI agent with tool calling**  
‚úÖ **Configured n8n workflows for production**  
‚úÖ **Integrated Voyage AI for multimodal embeddings**  

## üèÜ Key Takeaways

### 1. Visual Workflows Beat Code
n8n's visual interface makes complex AI integrations intuitive and maintainable. You can see data flow in real-time and debug issues easily.

### 2. Multimodal Embeddings Unlock Powerful Search  
Voyage AI's unified vector space allows semantic search across both text and images, opening new possibilities for document understanding.

### 3. Vector Databases Are Essential for AI Applications
MongoDB Atlas Vector Search provides the foundation for RAG applications with enterprise-grade performance and scalability.

### 4. Agent Patterns Create Intelligent Systems
Function calling and tool integration enable AI agents to reason about when and how to retrieve information.

## üöÄ What You've Built

Your complete system includes:

- **PDF Ingestion Pipeline**: Automated processing of documents
- **Multimodal Embeddings**: Text and image understanding
- **Vector Search API**: Fast similarity search
- **AI Agent**: Intelligent question answering
- **Production Workflows**: Error handling and monitoring

## üîß Next Steps

### 1. Extend the System

**Add More Document Types**:
- PowerPoint presentations
- Word documents  
- Web pages
- Audio transcripts

**Implement Additional Tools**:
\`\`\`javascript
{
  name: "analyze_sentiment",
  description: "Analyze sentiment of document content"
},
{
  name: "extract_entities", 
  description: "Extract named entities from text"
},
{
  name: "generate_summary",
  description: "Create document summaries"
}
\`\`\`

**Create a Web Interface**:
- React frontend for file uploads
- Real-time chat interface  
- Document preview with highlights
- Search result visualization

### 2. Production Deployment

**Set Up Monitoring**:
- Track processing times
- Monitor API usage
- Alert on failures
- Performance dashboards

**Implement Caching**:
- Redis for query results
- CDN for static assets
- Embedding cache for documents

**Add Authentication**:
- User management
- API key rotation
- Role-based access control

### 3. Advanced Features

**Multi-Language Support**:
\`\`\`javascript
// Detect document language
const language = await detectLanguage(content);

// Route to language-specific processing
if (language === 'es') {
  // Use Spanish embedding model
}
\`\`\`

**Document Clustering**:
\`\`\`javascript
// Group similar documents
const clusters = await clusterDocuments(embeddings);

// Enable browse by topic
\`\`\`

**Automatic Summarization**:
\`\`\`javascript
// Generate document summaries
const summary = await generateSummary(content);

// Store for quick access
\`\`\`

## üìö Resources for Continued Learning

<WorkshopResources />

## üåü Share Your Success

We'd love to see what you've built! Share your projects:

- **GitHub**: Fork and extend the workshop code
- **LinkedIn**: Tag @MongoDB and show your workflows  
- **Twitter**: Use #n8n #MongoDB #VoyageAI
- **Discord**: Join the MongoDB Developer Community

## üí¨ Get Help

If you need assistance:

1. **MongoDB Developer Community** - Active forums and Discord
2. **n8n Community** - Workflow help and best practices  
3. **Stack Overflow** - Technical questions with tags
4. **GitHub Issues** - Report bugs or request features

## üéâ Thank You!

Thank you for participating in this hands-on workshop! You've learned cutting-edge techniques for building multimodal AI systems with visual workflows.

The combination of n8n's visual approach, MongoDB's vector search capabilities, and Voyage AI's multimodal embeddings creates powerful possibilities for document intelligence applications.

Keep building, keep learning, and keep innovating! üöÄ

---

**Workshop Repository**: [GitHub - Multimodal PDF Agent with n8n](https://github.com/mrlynn/ai4-multimodal-agents-n8n)

**Feedback**: We value your input! Please share your workshop experience and suggestions for improvement.

<WorkshopFeedback 
  workshopTitle="Multimodal PDF Agent with n8n Workshop"
  instructorEmail="michael.lynn@mongodb.com"
  githubRepo="mongodb-developer/multimodal-pdf-agent-n8n"
/>
```

