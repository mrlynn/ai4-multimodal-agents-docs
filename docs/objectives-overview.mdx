---
id: objectives-overview
title: ðŸŽ¯ Workshop Objectives
sidebar_label: ðŸŽ¯ Objectives
---

# Workshop Objectives: Multimodal PDF Agents

## ðŸŽ¯ What You'll Build

Create intelligent agents that can process, understand, and interact with PDF documents containing both text and images.

### Core Capabilities
- **Process PDFs** - Extract and chunk text content from PDF documents
- **Generate Embeddings** - Create vector representations using pre-generated multimodal embeddings
- **Store & Search** - Use MongoDB Vector Search for semantic retrieval
- **Reason & Act** - Build agents using the ReAct pattern (Reasoning + Acting)
- **Chat Interface** - Natural conversations about document content

### Python + Jupyter + LangChain

**What you'll do:**
- Work with pre-generated embeddings from the enhanced notebook
- Use MongoDB Atlas Local (Docker-based)
- Build ReAct agents with LangChain
- Complete 4 hands-on exercises

## ðŸ“š Technologies Used

- **MongoDB Atlas Vector Search** - Document storage and semantic search
- **Voyage AI Multimodal Embeddings** - 1024-dimensional vectors for text + images
- **Google Gemini 2.0** - LLM with function calling capabilities
- **ReAct Pattern** - Reasoning + Acting agent architecture

## ðŸŽ¯ Learning Outcomes

After completing this workshop, you will:

1. **Understand multimodal AI** and how to work with text + image embeddings
2. **Build production-ready vector search** using MongoDB Atlas
3. **Create intelligent agents** that can reason and take actions
4. **Process real PDF documents** and make them searchable