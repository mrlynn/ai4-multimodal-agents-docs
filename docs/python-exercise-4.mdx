---
id: python-exercise-4
title: üß† Exercise 4 - Memory & Advanced Features
sidebar_label: üß† Exercise 4 - Memory & ReAct
---

# Exercise 4: Memory & Advanced Features

<div style={{textAlign: 'center', margin: '20px 0'}}>
  <a 
    href="https://codespaces.new/mrlynn/multimodal-agents-lab" 
    target="_blank" 
    rel="noopener noreferrer"
    style={{
      backgroundColor: '#589636',
      color: 'white',
      padding: '12px 24px',
      borderRadius: '6px',
      textDecoration: 'none',
      fontWeight: '900',
      display: 'inline-block',
      fontSize: '16px'
    }}
  >
    üöÄ Open in GitHub Codespaces
  </a>
</div>

## üéØ Objective
Add conversation memory to enable multi-turn interactions and implement the ReAct (Reasoning + Acting) pattern for more sophisticated agent behavior.

## üìã Prerequisites
- Completed Exercises 1-3
- Working AI agent with vector search
- Understanding of conversation context

## üîß Lab Steps

### Step 12: Add Memory to the Agent

Implement a conversation memory system using MongoDB to maintain context across interactions.

#### Part A: Set Up Memory Storage

```python
from datetime import datetime

# Instantiate the history collection
history_collection = mongodb_client[DB_NAME]["history"]

# üß™ TODO: Create an index on `session_id` on the `history_collection` collection
<CODE_BLOCK_13>

print("‚úÖ Memory storage initialized with session index!")
```

<details>
<summary>üí° Solution for CODE_BLOCK_13</summary>

```python
history_collection.create_index("session_id")
```

</details>

üìö **Reference**: [PyMongo - Collection.create_index](https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_index)

#### Part B: Store Chat Messages

```python
def store_chat_message(session_id: str, role: str, type: str, content: str) -> None:
    """
    Create chat history document and store it in MongoDB

    Args:
        session_id (str): Session ID
        role (str): Message role, one of `user` or `agent`
        type (str): Type of message, one of `text` or `image`
        content (str): Content of the message. For images, this is the image key.
    """
    # Create a message object with all required fields
    message = {
        "session_id": session_id,
        "role": role,
        "type": type,
        "content": content,
        "timestamp": datetime.now(),
    }
    
    # üß™ TODO: Insert the `message` into the `history_collection` collection
    <CODE_BLOCK_14>
```

<details>
<summary>üí° Solution for CODE_BLOCK_14</summary>

```python
history_collection.insert_one(message)
```

</details>

üìö **Reference**: [PyMongo - Collection.insert_one](https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_one)

#### Part C: Retrieve Session History

```python
def retrieve_session_history(session_id: str) -> List:
    """
    Retrieve chat history for a particular session.

    Args:
        session_id (str): Session ID

    Returns:
        List: List of messages. Can be a combination of text and images.
    """
    # üß™ TODO: Query the `history_collection` for documents with matching session_id
    # Sort the results in increasing order by `timestamp`
    cursor = <CODE_BLOCK_15>
    
    messages = []
    if cursor:
        for msg in cursor:
            # If message type is `text`, append content as is
            if msg["type"] == "text":
                messages.append(msg["content"])
            # If message type is `image`, open the image
            elif msg["type"] == "image":
                messages.append(Image.open(msg["content"]))
    return messages
```

<details>
<summary>üí° Solution for CODE_BLOCK_15</summary>

```python
cursor = history_collection.find({"session_id": session_id}).sort("timestamp", 1)
```

</details>

üìö **Reference**: [PyMongo - Cursor.sort](https://pymongo.readthedocs.io/en/stable/api/pymongo/cursor.html#pymongo.cursor.Cursor.sort)

#### Part D: Update Agent with Memory

```python
def generate_answer(session_id: str, user_query: str, images: List = []) -> str:
    """
    Execute any tools and generate a response with memory

    Args:
        session_id (str): Session ID
        user_query (str): User's query string
        images (List): List of filepaths. Defaults to [].

    Returns:
        str: LLM-generated response
    """
    # üß™ TODO: Retrieve past conversation history for the session
    history = <CODE_BLOCK_16>
    
    # Determine if any additional tools need to be called
    tool_call = select_tool(history + [user_query])
    if (
        tool_call is not None
        and tool_call.name == "get_information_for_question_answering"
    ):
        print(f"Agent: Calling tool: {tool_call.name}")
        tool_images = get_information_for_question_answering(**tool_call.args)
        images.extend(tool_images)

    # Pass system prompt, conversation history, user query and retrieved context
    system_prompt = f"Answer the questions based on the provided context only. If the context is not sufficient, say I DON'T KNOW. DO NOT use any other information to answer the question."
    contents = (
        [system_prompt]
        + history
        + [user_query]
        + [Image.open(image) for image in images]
    )
    
    # Get a response from the LLM
    response = gemini_client.models.generate_content(
        model=LLM,
        contents=contents,
        config=types.GenerateContentConfig(temperature=0.0),
    )
    answer = response.text
    
    # üß™ TODO: Store the user query in memory
    # Role: "user", Type: "text"
    <CODE_BLOCK_17>
    
    # üß™ TODO: Store each image filepath in memory
    # Role: "user", Type: "image"
    for image in images:
        <CODE_BLOCK_18>
    
    # üß™ TODO: Store the agent response in memory
    # Role: "agent", Type: "text"
    <CODE_BLOCK_19>
    
    return answer
```

<details>
<summary>üí° Solutions for CODE_BLOCK_16-19</summary>

```python
# CODE_BLOCK_16:
history = retrieve_session_history(session_id)

# CODE_BLOCK_17:
store_chat_message(session_id, "user", "text", user_query)

# CODE_BLOCK_18:
store_chat_message(session_id, "user", "image", image)

# CODE_BLOCK_19:
store_chat_message(session_id, "agent", "text", answer)
```

</details>

#### Part E: Update Execute Function

```python
def execute_agent(session_id: str, user_query: str, images: List = []) -> None:
    """
    Execute the agent with memory support.

    Args:
        session_id (str): Session ID
        user_query (str): User query
        images (List, optional): List of filepaths. Defaults to [].
    """
    response = generate_answer(session_id, user_query, images)
    print("Agent:", response)
```

### Testing Memory-Enabled Agent

```python
# First query in a session
execute_agent(
    "session_1",
    "What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?",
)

# Follow-up question to test memory
execute_agent(
    "session_1",
    "What did I just ask you?",
)
```

## ü¶∏‚Äç‚ôÄÔ∏è Bonus: ReAct Agent Implementation

Implement a ReAct (Reasoning + Acting) agent that can reason about whether it has enough information and iteratively gather more data.

### ReAct Agent Implementation

```python
def generate_answer(user_query: str, images: List = []) -> str:
    """
    Implement a ReAct agent

    Args:
        user_query (str): User's query string
        images (List): List of filepaths. Defaults to [].

    Returns:
        str: LLM-generated response
    """
    # Define reasoning prompt
    system_prompt = [
        (
            "You are an AI assistant. Based on the current information, decide if you have enough to answer the user query, or if you need more information."
            "If you have enough information, respond with 'ANSWER: <your answer>'."
            "If you need more information, respond with 'TOOL: <question for the tool>'. Keep the question concise."
            f"User query: {user_query}\n"
            "Current information:\n"
        )
    ]
    
    # Set max iterations
    max_iterations = 3
    current_iteration = 0
    # Initialize list to accumulate information
    current_information = []

    # If the user input has images, add them
    if len(images) != 0:
        current_information.extend([Image.open(image) for image in images])

    # Run the reasoning ‚Üí action loop
    while current_iteration < max_iterations:
        current_iteration += 1
        print(f"Iteration {current_iteration}:")
        
        # Generate action (answer or tool call)
        response = gemini_client.models.generate_content(
            model=LLM,
            contents=system_prompt + current_information,
            config=types.GenerateContentConfig(temperature=0.0),
        )
        answer = response.text
        print(f"Agent: {answer}")
        
        # If the agent has the final answer, return it
        if "ANSWER" in answer:
            return answer
        # If the agent decides to call a tool
        else:
            # determine which tool to call
            tool_call = select_tool([answer])
            if (
                tool_call is not None
                and tool_call.name == "get_information_for_question_answering"
            ):
                print(f"Agent: Calling tool: {tool_call.name}")
                # Call the tool with the arguments
                tool_images = get_information_for_question_answering(**tool_call.args)
                # Add images to current information
                current_information.extend([Image.open(image) for image in tool_images])
                continue
    
    return "I couldn't find a satisfactory answer within the iteration limit."

def execute_agent(user_query: str, images: List = []) -> None:
    """
    Execute the ReAct agent.

    Args:
        user_query (str): User query
        images (List, optional): List of filepaths. Defaults to [].
    """
    response = generate_answer(user_query, images)
    print("Agent:", response)
```

### Testing ReAct Agent

```python
# Test with a question requiring reasoning
execute_agent("What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?")

# Test with image analysis
execute_agent("Explain the graph in this image:", ["data/test.png"])
```

## ‚úÖ Success Criteria

By the end of this exercise, you should have:
- [ ] Memory storage set up with session indexing
- [ ] Chat messages being stored correctly
- [ ] Session history retrieval working
- [ ] Agent maintaining conversation context
- [ ] Follow-up questions answered using memory
- [ ] (Bonus) ReAct agent with iterative reasoning

## üéØ What You've Accomplished

In this exercise, you've:
1. Built a conversation memory system using MongoDB
2. Enabled multi-turn conversations with context retention
3. Implemented session-based chat history
4. (Bonus) Created a ReAct agent with sophisticated reasoning
5. Completed a production-ready multimodal AI system

## üí° Key Concepts

- **Conversation Memory**: Maintaining context across interactions
- **Session Management**: Isolating conversations by session ID
- **ReAct Pattern**: Reasoning about information sufficiency before acting
- **Iterative Refinement**: Gathering information until confident
- **Multimodal Context**: Storing both text and image references

## üéâ Workshop Complete!

Congratulations! You've successfully built a comprehensive multimodal AI agent system with:
- PDF processing and image extraction
- Vector search for retrieval
- Function calling with Gemini 2.0 Flash
- Conversation memory
- Advanced reasoning capabilities

## üöÄ Next Steps

Consider extending your agent with:
1. **Web Interface**: Build a chat UI with Streamlit or Gradio
2. **Multiple Tools**: Add web search, calculations, or other capabilities
3. **Fine-tuning**: Optimize prompts for your specific use case
4. **Production Deployment**: Deploy as an API service
5. **Analytics**: Track usage patterns and improve responses

## üìö Additional Resources

- [MongoDB Atlas Vector Search](https://www.mongodb.com/docs/atlas/atlas-vector-search/)
- [Google Gemini API Docs](https://ai.google.dev/gemini-api/docs)
- [ReAct Paper](https://arxiv.org/abs/2210.03629)
- [Function Calling Best Practices](https://ai.google.dev/gemini-api/docs/function-calling)

Thank you for completing the Multimodal Agents Workshop! üôè