---
id: python-exercise-2
title: üîç Exercise 2 - Vector Search Setup
sidebar_label: üîç Exercise 2 - Vector Search
---

import SlideRecap from '@site/src/components/SlideRecap';
import { QuickCheck } from '@site/src/components/Quiz';

# Exercise 2: Vector Search Setup

<div style={{textAlign: 'center', margin: '20px 0'}}>
  <a 
    href="https://codespaces.new/mongodb-developer/ai4-multimodal-agents-lab" 
    target="_blank" 
    rel="noopener noreferrer"
    style={{
      backgroundColor: '#589636',
      color: 'white',
      padding: '12px 24px',
      borderRadius: '6px',
      textDecoration: 'none',
      fontWeight: '900',
      display: 'inline-block',
      fontSize: '16px'
    }}
  >
    üöÄ Open in GitHub Codespaces
  </a>
</div>

## üéØ Objective
Create a vector search index in MongoDB Atlas and build the search tool function that your AI agent will use to retrieve relevant information.

## üìã Prerequisites
- Completed [Exercise 1](./python-exercise-1) with documents ingested into MongoDB
- MongoDB collection with embeddings in place
- Understanding of vector search concepts

:::tip Solo Learner Tip
Vector index creation can take 5-15 minutes depending on your data size. Be patient and use the Atlas UI to monitor progress. Don't proceed until the status shows "READY".
:::

## üîß Lab Steps

### Step 6: Create a Vector Search Index

Define and create a vector search index to enable similarity searches on your multimodal embeddings.

```python
VS_INDEX_NAME = "vector_index"

# Create vector index definition specifying:
# path: Path to the embeddings field
# numDimensions: Number of embedding dimensions - depends on the embedding model used
# similarity: Similarity metric. One of cosine, euclidean, dotProduct.
model = {
    "name": VS_INDEX_NAME,
    "type": "vectorSearch",
    "definition": {
        "fields": [
            {
                "type": "vector",
                "path": "embedding",
                "numDimensions": 1024,
                "similarity": "cosine",
            }
        ]
    },
}

# üß™ TODO: Create a vector search index with the above `model` for the `collection` collection
<CODE_BLOCK_4>

# Verify that the index is in READY status before proceeding
list(collection.list_search_indexes())
```

<details>
<summary>üí° Solution for CODE_BLOCK_4</summary>

```python
collection.create_search_index(model=model)

# ‚úÖ Checkpoint: Verify index creation started
print("‚úÖ Vector search index creation initiated!")
print("Note: Index building is asynchronous and may take 5-15 minutes.")
```

</details>

üìö **Reference**: [PyMongo - Collection.create_search_index](https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_search_index)

üí° **Important Notes**:
- The index creation is asynchronous and may take a few minutes
- Check the Atlas UI or wait for status to be "READY" before proceeding
- Voyage multimodal embeddings have 1024 dimensions

### Step 7: Create Agent Tools

Build the vector search function that will serve as the core tool for your AI agent.

#### Part A: Implement the Search Function

```python
from typing import List

def get_information_for_question_answering(user_query: str) -> List[str]:
    """
    Retrieve information using vector search to answer a user query.

    Args:
        user_query (str): The user's query string.

    Returns:
        List[str]: The retrieved image file paths.
    """
    # Embed the user query using our serverless endpoint
    response = requests.post(
        url=SERVERLESS_URL,
        json={
            "task": "get_embedding",
            "data": {"input": user_query, "input_type": "query"},
        },
    )
    # Extract the embedding from the response
    query_embedding = response.json()["embedding"]

    # üß™ TODO: Define an aggregation pipeline consisting of:
    # 1. A $vectorSearch stage with:
    #    - index: VS_INDEX_NAME
    #    - path: "embedding"
    #    - queryVector: query_embedding
    #    - numCandidates: 150
    #    - limit: 2
    # 2. A $project stage that:
    #    - Excludes the `_id` field
    #    - Includes: `key`, `width`, `height`
    #    - Includes the vector search score as `score`
    pipeline = <CODE_BLOCK_5>

    # üß™ TODO: Execute the aggregation `pipeline` against the `collection` collection
    results = <CODE_BLOCK_6>
    
    # Get images from local storage
    keys = [result["key"] for result in results]
    print(f"Keys: {keys}")
    return keys
```

<details>
<summary>üí° Solution for CODE_BLOCK_5</summary>

```python
pipeline = [
    {
        "$vectorSearch": {
            "index": VS_INDEX_NAME,
            "path": "embedding",
            "queryVector": query_embedding,
            "numCandidates": 150,
            "limit": 2,
        }
    },
    {
        "$project": {
            "_id": 0,
            "key": 1,
            "width": 1,
            "height": 1,
            "score": {"$meta": "vectorSearchScore"},
        }
    },
]
```

</details>

<details>
<summary>üí° Solution for CODE_BLOCK_6</summary>

```python
results = list(collection.aggregate(pipeline))

# ‚úÖ Checkpoint: Verify search results
assert len(results) > 0, "‚ùå No search results found. Check if your vector index is READY."
print(f"‚úÖ Vector search returned {len(results)} results!")
```

</details>

üìö **Reference**: [MongoDB Atlas Vector Search - Basic Example](https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples)

#### Part B: Define Function Declaration for Gemini

Create the function declaration that tells Gemini about your search tool.

```python
# Define the function declaration for the `get_information_for_question_answering` function
get_information_for_question_answering_declaration = {
    "name": <CODE_BLOCK_7>,
    "description": "Retrieve information using vector search to answer a user query.",
    "parameters": {
        "type": "object",
        "properties": {
            "user_query": {
                "type": <CODE_BLOCK_8>,
                "description": "Query string to use for vector search",
            }
        },
        "required": <CODE_BLOCK_9>,
    },
}
```

<details>
<summary>üí° Solutions for CODE_BLOCK_7, 8, 9</summary>

```python
# CODE_BLOCK_7:
"get_information_for_question_answering"

# CODE_BLOCK_8:
"string"

# CODE_BLOCK_9:
["user_query"]
```

</details>

üìö **Reference**: [Gemini Function Calling - Define Function Declaration](https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#step_1_define_function_declaration)

### Testing Your Vector Search

Once your index is ready, test the search function:

```python
# Test the vector search function
test_query = "What is deep learning?"
results = get_information_for_question_answering(test_query)
print(f"Found {len(results)} relevant images for query: '{test_query}'")
for i, key in enumerate(results):
    print(f"  {i+1}. {key}")
```

## ‚úÖ Success Criteria

By the end of this exercise, you should have:
- [ ] Vector search index created and in READY status
- [ ] Search function implemented with proper aggregation pipeline
- [ ] Function declaration defined for Gemini integration
- [ ] Successful test of vector search returning relevant images
- [ ] Understanding of how vector search works with embeddings

## üéØ What You've Accomplished

In this exercise, you've:
1. Created a vector search index optimized for multimodal embeddings
2. Built a search function that converts queries to embeddings
3. Implemented MongoDB aggregation pipeline for similarity search
4. Prepared function declaration for AI agent integration
5. Created the retrieval component of your RAG system

## üí° Key Concepts

- **Vector Search**: Finds similar documents based on embedding similarity
- **numCandidates**: Number of candidates to consider (affects recall)
- **limit**: Final number of results to return
- **Cosine Similarity**: Measures angle between embedding vectors
- **Function Declaration**: Schema that tells the LLM how to use your tool

<QuickCheck
  question="What does numCandidates: 150 control in vector search?"
  options={[
    "Number of documents initially considered for similarity matching",
    "Maximum number of results returned",
    "Number of embedding dimensions"
  ]}
  correctAnswer={0}
  explanation="numCandidates controls the recall vs speed trade-off. More candidates = better accuracy but slower queries."
/>

<QuickCheck
  question="True or False: Higher numCandidates values always improve search quality."
  options={[
    "True, but at the cost of slower query performance",
    "False, there's no relationship"
  ]}
  correctAnswer={0}
  explanation="Higher values improve accuracy but increase query time. You need to balance based on your specific needs."
/>

:::info Performance & Cost Tip
**Optimizing Vector Search**:
- `numCandidates`: 10-20x your `limit` for good accuracy
- For production: Start with 100 candidates, increase if needed
- Monitor query latency vs accuracy trade-offs
- Consider caching frequent queries
:::

<SlideRecap 
  title="Checkpoint Recap: Vector Search Foundation"
  items={[
    { icon: "üì¶", title: "Index Created", description: "Vector search index configured for 1024-dimensional embeddings." },
    { icon: "üîç", title: "Search Function Built", description: "Query embedding and similarity search pipeline working." },
    { icon: "‚öôÔ∏è", title: "Function Declaration Ready", description: "Gemini can now understand how to use your search tool." },
    { icon: "üéØ", title: "RAG Foundation Complete", description: "Ready to build an intelligent agent that retrieves relevant context." }
  ]}
  nextSection="Up next: Building your AI agent with Gemini!"
/>

---

**Navigation:** **[‚Üê Exercise 1: Setup & Data](./python-exercise-1)** | **[Exercise 3: Build AI Agent ‚Üí](./python-exercise-3)** | **[üè† Workshop Overview](./index)**

## üöÄ Next Steps

Once you've successfully completed all tasks and your vector search is working, proceed to [Exercise 3: Build the AI Agent](./python-exercise-3) where you'll integrate Gemini and create your intelligent agent.