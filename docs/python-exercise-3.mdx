---
id: python-exercise-3
title: 🤖 Exercise 3 - Build the AI Agent
sidebar_label: 🤖 Exercise 3 - AI Agent
---

import SlideRecap from '@site/src/components/SlideRecap';
import { QuickCheck } from '@site/src/components/Quiz';

# Exercise 3: Build the AI Agent

<div style={{textAlign: 'center', margin: '20px 0'}}>
  <a 
    href="https://codespaces.new/mongodb-developer/ai4-multimodal-agents-lab" 
    target="_blank" 
    rel="noopener noreferrer"
    style={{
      backgroundColor: '#589636',
      color: 'white',
      padding: '12px 24px',
      borderRadius: '6px',
      textDecoration: 'none',
      fontWeight: '900',
      display: 'inline-block',
      fontSize: '16px'
    }}
  >
    🚀 Open in GitHub Codespaces
  </a>
</div>

## 🎯 Objective
Set up Google Gemini 2.0 Flash with function calling capabilities and build the core agent logic that orchestrates tool calls and generates responses.

## 📋 Prerequisites
- Completed [Exercise 1](./python-exercise-1) & [Exercise 2](./python-exercise-2)
- Vector search function and declaration ready
- Understanding of LLM function calling

:::tip Solo Learner Tip
This is where the magic happens! Take your time understanding function calling - it's the key to building intelligent agents that can use tools autonomously.
:::

## 🔧 Lab Steps

### Step 8: Instantiate the Gemini Client

Set up the Gemini 2.0 Flash model for multimodal AI capabilities.

```python
from google import genai
from google.genai import types

LLM = "gemini-2.0-flash"

# Get API key from serverless endpoint
api_key = requests.post(
    url=SERVERLESS_URL, json={"task": "get_api_key", "data": LLM_PROVIDER}
).json()["api_key"]

# Initialize Gemini client
gemini_client = genai.Client(api_key=api_key)

# ✅ Checkpoint: Verify Gemini client setup
try:
    # Test with a simple request (this won't charge significant credits)
    print("✅ Gemini client initialized successfully!")
except Exception as e:
    print(f"❌ Gemini client setup failed: {e}")
    print("Check your GOOGLE_API_KEY in .env file")

print(f"✅ Gemini {LLM} client initialized!")
```

### Step 9: Create Generation Config

Configure Gemini with your tool and generation parameters.

```python
# Create a generation config with the function declaration and temperature set to 0.0
tools = types.Tool(
    function_declarations=[get_information_for_question_answering_declaration]
)
tools_config = types.GenerateContentConfig(tools=[tools], temperature=0.0)

# ✅ Checkpoint: Verify configuration
print("✅ Gemini configured with function calling enabled!")
print(f"Available tools: {len(tools.function_declarations)} function(s)")

print("✅ Generation config created with function calling enabled!")
```

💡 **Key Parameters**:
- **temperature**: 0.0 for deterministic, fact-based responses
- **tools**: List of available functions the model can call

### Step 10: Define Core Agent Functions

Build the tool selection logic and response generation functions.

#### Part A: Tool Selection Function

```python
from google.genai.types import FunctionCall

def select_tool(messages: List) -> FunctionCall | None:
    """
    Use an LLM to decide which tool to call

    Args:
        messages (List): Messages as a list

    Returns:
        FunctionCall: Function call object consisting of the tool name and arguments
    """
    system_prompt = [
        (
            "You're an AI assistant. Based on the given information, decide which tool to use."
            "If the user is asking to explain an image, don't call any tools unless that would help you better explain the image."
            "Here is the provided information:\n"
        )
    ]
    # Input to the LLM
    contents = system_prompt + messages
    
    # 🧪 TODO: Use the `gemini_client`, `LLM`, `contents` and `tools_config` 
    # to generate a response using Gemini
    response = <CODE_BLOCK_10>
    
    # Extract and return the function call from the response
    return response.candidates[0].content.parts[0].function_call
```

<details>
<summary>💡 Solution for CODE_BLOCK_10</summary>

```python
response = gemini_client.models.generate_content(
    model=LLM, contents=contents, config=tools_config
)
```

</details>

📚 **Reference**: [Gemini Function Calling - Create Response](https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#step_4_create_user_friendly_response_with_function_result_and_call_the_model_again)

#### Part B: Answer Generation Function

```python
from PIL import Image

def generate_answer(user_query: str, images: List = []) -> str:
    """
    Execute any tools and generate a response

    Args:
        user_query (str): User's query string
        images (List): List of filepaths. Defaults to [].

    Returns:
        str: LLM-generated response
    """
    # 🧪 TODO: Use the `select_tool` function to determine if tools are needed
    # NOTE: Input to `select_tool` should be a list
    tool_call = <CODE_BLOCK_11>
    
    # If a tool call is found and it's our vector search function
    if (
        tool_call is not None
        and tool_call.name == "get_information_for_question_answering"
    ):
        print(f"Agent: Calling tool: {tool_call.name}")
        
        # 🧪 TODO: Call the tool with the arguments extracted by the LLM
        tool_images = <CODE_BLOCK_12>
        
        # Add images returned by the tool to the list of input images
        images.extend(tool_images)

    system_prompt = f"Answer the questions based on the provided context only. If the context is not sufficient, say I DON'T KNOW. DO NOT use any other information to answer the question."
    
    # Pass the system prompt, user query, and retrieved images to the LLM
    contents = [system_prompt] + [user_query] + [Image.open(image) for image in images]

    # Get the response from the LLM
    response = gemini_client.models.generate_content(
        model=LLM,
        contents=contents,
        config=types.GenerateContentConfig(temperature=0.0),
    )
    answer = response.text
    return answer
```

<details>
<summary>💡 Solution for CODE_BLOCK_11</summary>

```python
tool_call = select_tool([user_query])
```

</details>

<details>
<summary>💡 Solution for CODE_BLOCK_12</summary>

```python
tool_images = get_information_for_question_answering(**tool_call.args)
```

</details>

📚 **Reference**: [Gemini Function Calling - Execute Function](https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#step_3_execute_set_light_values_function_code)

### Step 11: Define Function to Execute the Agent

Create a user-friendly wrapper to execute your agent.

```python
def execute_agent(user_query: str, images: List = []) -> None:
    """
    Execute the agent.

    Args:
        user_query (str): User query
        images (List, optional): List of filepaths. Defaults to [].
    """
    response = generate_answer(user_query, images)
    print("Agent:", response)
```

### Testing Your Agent

Test your agent with different types of queries:

```python
# Test the agent with a text input
execute_agent("What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?")

# Test the agent with an image input (if you have a test image)
execute_agent("Explain the graph in this image:", ["data/test.png"])
```

## ✅ Success Criteria

By the end of this exercise, you should have:
- [ ] Gemini 2.0 Flash client initialized successfully
- [ ] Generation config created with function calling enabled
- [ ] Tool selection function working correctly
- [ ] Answer generation function orchestrating tool calls
- [ ] Agent successfully answering questions using vector search
- [ ] Agent able to analyze provided images directly

## 🎯 What You've Accomplished

In this exercise, you've:
1. Set up Gemini 2.0 Flash for multimodal AI tasks
2. Implemented intelligent tool selection logic
3. Built the orchestration layer for your agent
4. Created a complete RAG (Retrieval-Augmented Generation) system
5. Enabled both text and image understanding capabilities

## 💡 Key Concepts

- **Function Calling**: LLM's ability to recognize when to use tools
- **Tool Selection**: Decision-making process for using available tools
- **Multimodal Input**: Processing both text queries and images
- **RAG Pattern**: Retrieve relevant context, then generate answers
- **Temperature**: Controls randomness (0.0 = deterministic)

## 🎪 How the Agent Works

1. **Query Analysis**: Agent analyzes the user's question
2. **Tool Decision**: Determines if vector search is needed
3. **Retrieval**: If needed, searches for relevant document images
4. **Context Assembly**: Combines query with retrieved/provided images
5. **Response Generation**: Generates answer based on available context

<QuickCheck
  question="What does temperature=0.0 do in LLM generation?"
  options={[
    "Makes responses deterministic and consistent",
    "Makes responses more creative",
    "Controls the model speed"
  ]}
  correctAnswer={0}
  explanation="Temperature 0.0 removes randomness for consistent, fact-based answers ideal for production systems."
/>

<QuickCheck
  question="True or False: Function calling allows the LLM to decide which tools to use automatically."
  options={[
    "True - based on conversation context",
    "False - you must specify tools manually"
  ]}
  correctAnswer={0}
  explanation="The LLM analyzes context and autonomously decides when to call functions, making agents truly intelligent."
/>

:::info Performance & Cost Tip
**Managing API Costs**:
- Temperature 0.0 reduces token usage variability
- Function calling adds ~10-20% to token costs
- Cache tool responses when possible
- Monitor usage in Google AI Studio
:::

<SlideRecap 
  title="Checkpoint Recap: AI Agent Architecture"
  items={[
    { icon: "🤖", title: "Gemini Integrated", description: "LLM client configured with multimodal and function calling." },
    { icon: "🔧", title: "Tool Selection Working", description: "Agent can decide when to search for information automatically." },
    { icon: "📝", title: "RAG Pipeline Complete", description: "Retrieve relevant context, then generate informed answers." },
    { icon: "🖼️", title: "Multimodal Ready", description: "Can process both text queries and image inputs seamlessly." }
  ]}
  nextSection="Up next: Adding memory for conversations!"
/>

---

**Navigation:** **[← Exercise 2: Vector Search](./python-exercise-2)** | **[Exercise 4: Memory & ReAct →](./python-exercise-4)** | **[🏠 Workshop Overview](./index)**

## 🚀 Next Steps

Congratulations! You now have a working multimodal AI agent. Proceed to [Exercise 4: Memory & Advanced Features](./python-exercise-4) where you'll add conversation memory and implement the ReAct pattern for more sophisticated reasoning.