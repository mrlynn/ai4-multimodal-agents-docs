---
id: python-exercise-1
title: üìä Exercise 1 - Environment Setup & Data Processing
sidebar_label: üìä Exercise 1 - Setup & Data
---

import SlideRecap from '@site/src/components/SlideRecap';
import { QuickCheck } from '@site/src/components/Quiz';

# Exercise 1: Environment Setup & Data Processing

<div style={{textAlign: 'center', margin: '20px 0'}}>
  <a 
    href="https://codespaces.new/mongodb-developer/ai4-multimodal-agents-lab" 
    target="_blank" 
    rel="noopener noreferrer"
    style={{
      backgroundColor: '#589636',
      color: 'white',
      padding: '12px 24px',
      borderRadius: '6px',
      textDecoration: 'none',
      fontWeight: '900',
      display: 'inline-block',
      fontSize: '16px'
    }}
  >
    üöÄ Open in GitHub Codespaces
  </a>
</div>

## üéØ Objective
Set up your environment, process a PDF document into images, and ingest pre-generated embeddings into MongoDB Atlas.

## üìã Prerequisites
- MongoDB Atlas connection string ([Setup Guide](./atlas-setup))
- Access to the serverless endpoint for embeddings
- Python environment with required packages ([Codespaces Setup](./codespaces-setup))

:::tip Solo Learner Tip
If you're working alone, complete the [Codespaces Setup Guide](./codespaces-setup) first. Give yourself 15 minutes to configure your `.env` file with all credentials.
:::

## üîß Lab Steps

### Step 1: Setup Prerequisites

First, let's import necessary libraries and establish our MongoDB connection.

```python
import os
from pymongo import MongoClient

# If you are using your own MongoDB Atlas cluster, use the connection string for your cluster here
MONGODB_URI = os.getenv("MONGODB_URI")
# Initialize a MongoDB Python client
mongodb_client = MongoClient(MONGODB_URI)
# Check the connection to the server
result = mongodb_client.admin.command("ping")

# ‚úÖ Checkpoint: Verify MongoDB connection
assert result.get("ok") == 1, "‚ùå MongoDB connection failed. Check your connection string in .env"
print("‚úÖ MongoDB connection successful!")

SERVERLESS_URL = os.getenv("SERVERLESS_URL")
LLM_PROVIDER = "google"
```

‚úÖ **Success Criteria**: MongoDB connection established with successful ping response

### Step 2: Read PDF from URL

Download and open the DeepSeek research paper from arXiv.

```python
import pymupdf
import requests

# Download the DeepSeek paper
response = requests.get("https://arxiv.org/pdf/2501.12948")
if response.status_code != 200:
    raise ValueError(f"Failed to download PDF. Status code: {response.status_code}")

# Get the content of the response
pdf_stream = response.content

# üß™ TODO: Open the data in `pdf_stream` as a PDF document and store it in `pdf`.
# HINT: Set the `filetype` argument to "pdf".
pdf = <CODE_BLOCK_1>

# ‚úÖ Checkpoint: Verify PDF loaded
assert pdf.page_count > 0, "‚ùå PDF has no pages. Check the download URL."
print(f"‚úÖ PDF loaded successfully with {pdf.page_count} pages!")
```

<details>
<summary>üí° Solution for CODE_BLOCK_1</summary>

```python
pdf = pymupdf.Document(stream=pdf_stream, filetype="pdf")
```

</details>

üìö **Reference**: [PyMuPDF - Opening Remote Files](https://pymupdf.readthedocs.io/en/latest/how-to-open-a-file.html#opening-remote-files)

### Step 3: Store PDF Images Locally and Extract Metadata

Convert each PDF page to a high-resolution image for multimodal processing.

```python
from tqdm import tqdm

docs = []
zoom = 3.0
# Set image matrix dimensions
mat = pymupdf.Matrix(zoom, zoom)

# Iterate through the pages of the PDF
for n in tqdm(range(pdf.page_count)):
    temp = {}
    
    # üß™ TODO: Use the `get_pixmap` method to render the PDF page as a matrix of pixels
    # HINT: Access the PDF page as pdf[n]
    pix = <CODE_BLOCK_2>
    
    # Store image locally
    key = f"data/images/{n+1}.png"
    pix.save(key)
    
    # Extract image metadata to be stored in MongoDB
    temp["key"] = key
    temp["width"] = pix.width
    temp["height"] = pix.height
    docs.append(temp)

# ‚úÖ Checkpoint: Verify image extraction
assert len(docs) > 0, "‚ùå No images were extracted. Check the PDF processing."
print(f"‚úÖ Successfully extracted {len(docs)} page images!")
```

<details>
<summary>üí° Solution for CODE_BLOCK_2</summary>

```python
pix = pdf[n].get_pixmap(matrix=mat)
```

</details>

üìö **Reference**: [PyMuPDF - Page.get_pixmap](https://pymupdf.readthedocs.io/en/latest/page.html#Page.get_pixmap)

### Step 4: Generate Image Embeddings (Optional)

This step shows how to generate embeddings using Voyage AI. For the workshop, we'll use pre-generated embeddings.

```python
# Uncomment this section only if you are generating embeddings using your own Voyage AI API key

# from voyageai import Client
# from PIL import Image

# # Set Voyage AI API Key
# os.environ["VOYAGE_API_KEY"] = "your-api-key"
# voyageai_client = Client()

# def get_embedding(data, input_type):
#     """Get Voyage AI embeddings for images and text."""
#     embedding = voyageai_client.multimodal_embed(
#         inputs=[[data]], model="voyage-multimodal-3", input_type=input_type
#     ).embeddings[0]
#     return embedding

# embedded_docs = []
# for doc in tqdm(docs):
#     img = Image.open(f"{doc['key']}")
#     doc["embedding"] = get_embedding(img, "document")
#     embedded_docs.append(doc)
```

üí° **Note**: To get a Voyage AI API key, follow the steps [here](https://docs.voyageai.com/docs/api-key-and-installation#authentication-with-api-keys)

### Step 5: Write Embeddings and Metadata to MongoDB

Load pre-generated embeddings and ingest them into MongoDB Atlas.

```python
import json

# Database name
DB_NAME = "mongodb_aiewf"
# Name of the collection to insert documents into
COLLECTION_NAME = "multimodal_workshop"

# Connect to the collection
collection = mongodb_client[DB_NAME][COLLECTION_NAME]

# Read data from local file
with open("data/embeddings.json", "r") as data_file:
    json_data = data_file.read()
data = json.loads(json_data)

# Delete existing documents from the collection
collection.delete_many({})
print(f"Deleted existing documents from the {COLLECTION_NAME} collection.")

# üß™ TODO: Bulk insert documents in `data`, into the `collection` collection.
<CODE_BLOCK_3>

doc_count = collection.count_documents({})
print(f"‚úÖ {doc_count} documents ingested into the {COLLECTION_NAME} collection.")

# ‚úÖ Checkpoint: Verify data ingestion
assert doc_count > 0, "‚ùå No documents were ingested. Check your embeddings.json file."
print("‚úÖ All embeddings successfully stored in MongoDB!")
```

<details>
<summary>üí° Solution for CODE_BLOCK_3</summary>

```python
collection.insert_many(data)
```

</details>

üìö **Reference**: [PyMongo - Collection.insert_many](https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_many)

## ‚úÖ Success Criteria

By the end of this exercise, you should have:
- [ ] MongoDB connection established successfully
- [ ] PDF downloaded and opened with PyMuPDF
- [ ] All PDF pages extracted as high-resolution images
- [ ] Images saved to `data/images/` directory
- [ ] Embeddings data loaded from JSON file
- [ ] Documents with embeddings ingested into MongoDB collection
- [ ] Verification that all documents are stored correctly

## üéØ What You've Accomplished

In this exercise, you've:
1. Set up the foundational environment for the workshop
2. Processed a research paper PDF into individual page images
3. Prepared for multimodal search by storing documents with embeddings
4. Created the data foundation for building your AI agent

<QuickCheck
  question="What format does PyMuPDF use to scale PDF pages during image extraction?"
  options={[
    "Percentage (e.g., 300%)",
    "Matrix transformation with zoom factor",
    "DPI settings"
  ]}
  correctAnswer={1}
  explanation="PyMuPDF uses pymupdf.Matrix(zoom, zoom) for scaling pages during image extraction."
/>

<QuickCheck
  question="True or False: MongoDB Atlas requires you to manually create databases before inserting documents."
  options={[
    "True - databases must be created first",
    "False - databases are created automatically"
  ]}
  correctAnswer={1}
  explanation="MongoDB Atlas creates databases and collections automatically on first insert (lazy creation)."
/>

:::info Performance & Cost Tip
**Image Quality vs Storage**: The zoom factor of 3.0 creates high-quality images but uses more disk space. For production:
- Use 2.0-2.5 zoom for most documents
- Consider batch processing for large PDFs
- Monitor storage costs with many documents
:::

<SlideRecap 
  title="Checkpoint Recap: Environment & Data Processing"
  items={[
    { icon: "üîå", title: "MongoDB Connected", description: "Successfully connected to Atlas and verified with ping." },
    { icon: "üìÑ", title: "PDF Processed", description: "Downloaded DeepSeek paper and extracted all pages as images." },
    { icon: "üíæ", title: "Data Ingested", description: "Pre-generated embeddings loaded into MongoDB collection." },
    { icon: "‚úÖ", title: "Ready for Search", description: "Foundation complete for building vector search capabilities." }
  ]}
  nextSection="Up next: Creating your vector search index!"
/>

---

**Navigation:** [üè† Workshop Overview](./index) | **[Exercise 2: Vector Search Setup ‚Üí](./python-exercise-2)**

## üöÄ Next Steps

Once you've successfully completed all tasks, proceed to [Exercise 2: Vector Search Setup](./python-exercise-2) where you'll create a vector search index and build the search tool for your agent.