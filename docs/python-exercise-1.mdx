---
id: python-exercise-1
title: ðŸ“Š Exercise 1 - Environment Setup & Data Processing
sidebar_label: ðŸ“Š Exercise 1 - Setup & Data
---

# Exercise 1: Environment Setup & Data Processing

<div style={{textAlign: 'center', margin: '20px 0'}}>
  <a 
    href="https://codespaces.new/mrlynn/multimodal-agents-lab" 
    target="_blank" 
    rel="noopener noreferrer"
    style={{
      backgroundColor: '#589636',
      color: 'white',
      padding: '12px 24px',
      borderRadius: '6px',
      textDecoration: 'none',
      fontWeight: '900',
      display: 'inline-block',
      fontSize: '16px'
    }}
  >
    ðŸš€ Open in GitHub Codespaces
  </a>
</div>

## ðŸŽ¯ Objective
Set up your environment, process a PDF document into images, and ingest pre-generated embeddings into MongoDB Atlas.

## ðŸ“‹ Prerequisites
- MongoDB Atlas connection string
- Access to the serverless endpoint for embeddings
- Python environment with required packages

## ðŸ”§ Lab Steps

### Step 1: Setup Prerequisites

First, let's import necessary libraries and establish our MongoDB connection.

```python
import os
from pymongo import MongoClient

# If you are using your own MongoDB Atlas cluster, use the connection string for your cluster here
MONGODB_URI = os.getenv("MONGODB_URI")
# Initialize a MongoDB Python client
mongodb_client = MongoClient(MONGODB_URI)
# Check the connection to the server
mongodb_client.admin.command("ping")

SERVERLESS_URL = os.getenv("SERVERLESS_URL")
LLM_PROVIDER = "google"
```

âœ… **Success Criteria**: MongoDB connection established with successful ping response

### Step 2: Read PDF from URL

Download and open the DeepSeek research paper from arXiv.

```python
import pymupdf
import requests

# Download the DeepSeek paper
response = requests.get("https://arxiv.org/pdf/2501.12948")
if response.status_code != 200:
    raise ValueError(f"Failed to download PDF. Status code: {response.status_code}")

# Get the content of the response
pdf_stream = response.content

# ðŸ§ª TODO: Open the data in `pdf_stream` as a PDF document and store it in `pdf`.
# HINT: Set the `filetype` argument to "pdf".
pdf = <CODE_BLOCK_1>
```

<details>
<summary>ðŸ’¡ Solution for CODE_BLOCK_1</summary>

```python
pdf = pymupdf.Document(stream=pdf_stream, filetype="pdf")
```

</details>

ðŸ“š **Reference**: [PyMuPDF - Opening Remote Files](https://pymupdf.readthedocs.io/en/latest/how-to-open-a-file.html#opening-remote-files)

### Step 3: Store PDF Images Locally and Extract Metadata

Convert each PDF page to a high-resolution image for multimodal processing.

```python
from tqdm import tqdm

docs = []
zoom = 3.0
# Set image matrix dimensions
mat = pymupdf.Matrix(zoom, zoom)

# Iterate through the pages of the PDF
for n in tqdm(range(pdf.page_count)):
    temp = {}
    
    # ðŸ§ª TODO: Use the `get_pixmap` method to render the PDF page as a matrix of pixels
    # HINT: Access the PDF page as pdf[n]
    pix = <CODE_BLOCK_2>
    
    # Store image locally
    key = f"data/images/{n+1}.png"
    pix.save(key)
    
    # Extract image metadata to be stored in MongoDB
    temp["key"] = key
    temp["width"] = pix.width
    temp["height"] = pix.height
    docs.append(temp)
```

<details>
<summary>ðŸ’¡ Solution for CODE_BLOCK_2</summary>

```python
pix = pdf[n].get_pixmap(matrix=mat)
```

</details>

ðŸ“š **Reference**: [PyMuPDF - Page.get_pixmap](https://pymupdf.readthedocs.io/en/latest/page.html#Page.get_pixmap)

### Step 4: Generate Image Embeddings (Optional)

This step shows how to generate embeddings using Voyage AI. For the workshop, we'll use pre-generated embeddings.

```python
# Uncomment this section only if you are generating embeddings using your own Voyage AI API key

# from voyageai import Client
# from PIL import Image

# # Set Voyage AI API Key
# os.environ["VOYAGE_API_KEY"] = "your-api-key"
# voyageai_client = Client()

# def get_embedding(data, input_type):
#     """Get Voyage AI embeddings for images and text."""
#     embedding = voyageai_client.multimodal_embed(
#         inputs=[[data]], model="voyage-multimodal-3", input_type=input_type
#     ).embeddings[0]
#     return embedding

# embedded_docs = []
# for doc in tqdm(docs):
#     img = Image.open(f"{doc['key']}")
#     doc["embedding"] = get_embedding(img, "document")
#     embedded_docs.append(doc)
```

ðŸ’¡ **Note**: To get a Voyage AI API key, follow the steps [here](https://docs.voyageai.com/docs/api-key-and-installation#authentication-with-api-keys)

### Step 5: Write Embeddings and Metadata to MongoDB

Load pre-generated embeddings and ingest them into MongoDB Atlas.

```python
import json

# Database name
DB_NAME = "mongodb_aiewf"
# Name of the collection to insert documents into
COLLECTION_NAME = "multimodal_workshop"

# Connect to the collection
collection = mongodb_client[DB_NAME][COLLECTION_NAME]

# Read data from local file
with open("data/embeddings.json", "r") as data_file:
    json_data = data_file.read()
data = json.loads(json_data)

# Delete existing documents from the collection
collection.delete_many({})
print(f"Deleted existing documents from the {COLLECTION_NAME} collection.")

# ðŸ§ª TODO: Bulk insert documents in `data`, into the `collection` collection.
<CODE_BLOCK_3>

print(
    f"{collection.count_documents({})} documents ingested into the {COLLECTION_NAME} collection."
)
```

<details>
<summary>ðŸ’¡ Solution for CODE_BLOCK_3</summary>

```python
collection.insert_many(data)
```

</details>

ðŸ“š **Reference**: [PyMongo - Collection.insert_many](https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_many)

## âœ… Success Criteria

By the end of this exercise, you should have:
- [ ] MongoDB connection established successfully
- [ ] PDF downloaded and opened with PyMuPDF
- [ ] All PDF pages extracted as high-resolution images
- [ ] Images saved to `data/images/` directory
- [ ] Embeddings data loaded from JSON file
- [ ] Documents with embeddings ingested into MongoDB collection
- [ ] Verification that all documents are stored correctly

## ðŸŽ¯ What You've Accomplished

In this exercise, you've:
1. Set up the foundational environment for the workshop
2. Processed a research paper PDF into individual page images
3. Prepared for multimodal search by storing documents with embeddings
4. Created the data foundation for building your AI agent

## ðŸš€ Next Steps

Once you've successfully completed all tasks, proceed to [Exercise 2: Vector Search Setup](./python-exercise-2) where you'll create a vector search index and build the search tool for your agent.