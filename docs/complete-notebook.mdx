---
title: Complete Lab Notebook Walkthrough
sidebar_label: üìì Complete Notebook
description: Complete walkthrough of the multimodal agents workshop lab notebook with explanations and solutions
---

import BrowserWindow from '@site/src/components/BrowserWindow';
import Screenshot from '@site/src/components/Screenshot';
import SlideRecap from '@site/src/components/SlideRecap';
import { QuickCheck } from '@site/src/components/Quiz';

# Complete Lab Notebook Walkthrough

This document provides a comprehensive walkthrough of the `lab.ipynb` notebook used in the multimodal agents workshop. It includes detailed explanations of each step, the complete code with solutions, and insights into building a production-ready multimodal AI agent.

:::tip Solo Learner Tip
This is a complete reference guide - perfect for solo learners who want to understand every step deeply. Use this alongside the lab notebook to see solutions and explanations. Allow 2-3 hours to work through everything thoroughly.
:::

<div style={{textAlign: 'center', margin: '20px 0'}}>
  <a 
    href="https://codespaces.new/mongodb-developer/ai4-multimodal-agents-lab" 
    target="_blank" 
    rel="noopener noreferrer"
    style={{
      backgroundColor: '#589636',
      color: 'white',
      padding: '12px 24px',
      borderRadius: '6px',
      textDecoration: 'none',
      fontWeight: '900',
      display: 'inline-block',
      fontSize: '16px'
    }}
  >
    üöÄ Open in GitHub Codespaces
  </a>
</div>

## Workshop Overview

**What You'll Build:**
- A multimodal AI agent that can analyze documents and images
- Vector search system using MongoDB Atlas
- Function calling with Google Gemini 2.0 Flash
- Conversation memory for multi-turn interactions
- ReAct (Reasoning + Acting) agent architecture

**Learning Objectives:**
- Process PDFs and extract images for multimodal search
- Set up MongoDB Atlas vector search indexes
- Build an AI agent with tool calling capabilities
- Implement session-based memory for conversational agents
- Create advanced reasoning patterns

---

## Step 1: Setup Prerequisites

The first step establishes our foundational environment and connections.

### Import Core Libraries

```python
import os
from pymongo import MongoClient
```

### Environment Configuration

```python
# If you are using your own MongoDB Atlas cluster, use the connection string for your cluster here
MONGODB_URI = os.getenv("MONGODB_URI")
# Initialize a MongoDB Python client
mongodb_client = MongoClient(MONGODB_URI)
# Check the connection to the server
result = mongodb_client.admin.command("ping")

# ‚úÖ Checkpoint: Verify MongoDB connection
assert result.get("ok") == 1, "‚ùå MongoDB connection failed. Check your connection string in .env"
print("‚úÖ MongoDB connection successful!")

SERVERLESS_URL = os.getenv("SERVERLESS_URL")
LLM_PROVIDER = "google"
```

**Key Concepts:**
- **MongoDB Atlas**: Cloud-hosted MongoDB service with built-in vector search capabilities
- **Connection String**: URI containing all information needed to connect to MongoDB
- **Ping Command**: Lightweight method to verify database connectivity
- **Environment Variables**: Secure way to store credentials outside of code

**Success Criteria:** MongoDB connection established with successful ping response

---

## Step 2: Read PDF from URL

Download and process a research paper directly from the internet.

### Download PDF Document

```python
import pymupdf
import requests

# Download the DeepSeek paper
response = requests.get("https://arxiv.org/pdf/2501.12948")
if response.status_code != 200:
    raise ValueError(f"Failed to download PDF. Status code: {response.status_code}")

# Get the content of the response
pdf_stream = response.content

# üß™ CODE_BLOCK_1 Solution:
pdf = pymupdf.Document(stream=pdf_stream, filetype="pdf")

# ‚úÖ Checkpoint: Verify PDF loaded
assert pdf.page_count > 0, "‚ùå PDF has no pages. Check the download URL."
print(f"‚úÖ PDF loaded successfully with {pdf.page_count} pages!")
```

**Key Concepts:**
- **PyMuPDF**: High-performance PDF processing library (also known as `fitz`)
- **Stream Processing**: Opening documents directly from memory without temporary files
- **HTTP Response**: Handling web requests and status codes
- **DeepSeek R1**: The research paper we'll be analyzing in this workshop

**Reference:** [PyMuPDF - Opening Remote Files](https://pymupdf.readthedocs.io/en/latest/how-to-open-a-file.html#opening-remote-files)

<QuickCheck
  question="What advantage does stream processing have over saving files temporarily?"
  options={[
    "More memory efficient and secure",
    "Faster download speeds",
    "Better PDF quality"
  ]}
  correctAnswer={0}
  explanation="Stream processing avoids temporary files on disk and reduces memory usage while improving security."
/>

<QuickCheck
  question="True or False: PyMuPDF requires you to specify the file type when opening from a stream."
  options={[
    "True - filetype parameter is required",
    "False - PyMuPDF auto-detects file types"
  ]}
  correctAnswer={0}
  explanation="When opening from a stream, PyMuPDF needs the explicit filetype parameter for proper parsing."
/>

---

## Step 3: Store PDF Images Locally and Extract Metadata

Convert each PDF page into a high-resolution image for multimodal processing.

### Image Extraction Process

```python
from tqdm import tqdm

docs = []
zoom = 3.0
# Set image matrix dimensions
mat = pymupdf.Matrix(zoom, zoom)

# Iterate through the pages of the PDF
for n in tqdm(range(pdf.page_count)):
    temp = {}
    
    # üß™ CODE_BLOCK_2 Solution:
    pix = pdf[n].get_pixmap(matrix=mat)
    
    # Store image locally
    key = f"data/images/{n+1}.png"
    pix.save(key)
    
    # Extract image metadata to be stored in MongoDB
    temp["key"] = key
    temp["width"] = pix.width
    temp["height"] = pix.height
    docs.append(temp)

# ‚úÖ Checkpoint: Verify image extraction
assert len(docs) > 0, "‚ùå No images were extracted. Check the PDF processing."
print(f"‚úÖ Successfully extracted {len(docs)} page images!")
```

**Key Concepts:**
- **Zoom Factor**: Controls image resolution (3.0 = 3x default size for better quality)
- **Matrix Transformation**: Mathematical scaling applied during rendering
- **Pixmap**: PyMuPDF's image object containing pixel data
- **Metadata Extraction**: Storing image dimensions and file paths for later use
- **Progress Tracking**: Using `tqdm` for visual feedback during processing

**Reference:** [PyMuPDF - Page.get_pixmap](https://pymupdf.readthedocs.io/en/latest/page.html#Page.get_pixmap)

:::info Performance & Cost Tip
**Image Quality vs Storage**: The zoom factor of 3.0 creates high-quality images but uses more disk space. For production:
- Use 2.0-2.5 zoom for most documents
- Consider batch processing for large PDFs
- Monitor storage costs with many documents
:::

<SlideRecap 
  title="Checkpoint Recap: PDF Processing Complete"
  items={[
    { icon: "üìÑ", title: "PDF Downloaded", description: "DeepSeek paper successfully downloaded from arXiv." },
    { icon: "üñºÔ∏è", title: "Images Extracted", description: "All PDF pages converted to high-resolution PNG images." },
    { icon: "üìè", title: "Metadata Collected", description: "Image dimensions and file paths stored for MongoDB." },
    { icon: "‚úÖ", title: "Ready for Embeddings", description: "Document processing pipeline complete and verified." }
  ]}
  nextSection="Up next: Generating embeddings and ingesting data!"
/>

---

## Step 4: Generate Image Embeddings (Optional)

This step demonstrates how to generate embeddings using Voyage AI, but we'll use pre-generated embeddings for the workshop.

### Voyage AI Embedding Generation

```python
# Uncomment this section only if you are generating embedding using your own Voyage AI API key.
# Follow the steps here to obtain a Voyage AI API key:
# https://docs.voyageai.com/docs/api-key-and-installation#authentication-with-api-keys

# from voyageai import Client
# from PIL import Image

# # Set Voyage AI API Key
# os.environ["VOYAGE_API_KEY"] = "your-api-key"
# voyageai_client = Client()

# def get_embedding(data, input_type):
#     """
#     Get Voyage AI embeddings for images and text.
#     
#     Args:
#         data: An image or text to embed
#         input_type: Input type, either "document" or "query"
#     
#     Returns: Embeddings as a list
#     """
#     embedding = voyageai_client.multimodal_embed(
#         inputs=[[data]], model="voyage-multimodal-3", input_type=input_type
#     ).embeddings[0]
#     return embedding

# embedded_docs = []
# for doc in tqdm(docs):
#     # Open the image from file
#     img = Image.open(f"{doc['key']}")
#     # Add the embeddings to the document
#     doc["embedding"] = get_embedding(img, "document")
#     embedded_docs.append(doc)
```

**Key Concepts:**
- **Multimodal Embeddings**: Vector representations that capture semantic meaning of images
- **Voyage AI**: Service providing state-of-the-art multimodal embeddings
- **Input Types**: "document" for content being indexed, "query" for search queries
- **1024 Dimensions**: Voyage multimodal-3 produces 1024-dimensional vectors

---

## Step 5: Write Embeddings and Metadata to MongoDB

Ingest the document data with embeddings into MongoDB Atlas for vector search.

### Database Configuration

```python
import json

# Database name
DB_NAME = "mongodb_aiewf"
# Name of the collection to insert documents into
COLLECTION_NAME = "multimodal_workshop"

# Connect to the collection
collection = mongodb_client[DB_NAME][COLLECTION_NAME]

# Read data from local file
with open("data/embeddings.json", "r") as data_file:
    json_data = data_file.read()
data = json.loads(json_data)
```

### Data Ingestion

```python
# Delete existing documents from the `collection` collection
collection.delete_many({})
print(f"Deleted existing documents from the {COLLECTION_NAME} collection.")

# üß™ CODE_BLOCK_3 Solution:
collection.insert_many(data)

doc_count = collection.count_documents({})
print(f"{doc_count} documents ingested into the {COLLECTION_NAME} collection.")

# ‚úÖ Checkpoint: Verify data ingestion
assert doc_count > 0, "‚ùå No documents were ingested. Check your embeddings.json file."
print("‚úÖ All embeddings successfully stored in MongoDB!")
```

**Key Concepts:**
- **Collection**: MongoDB's equivalent of a table in relational databases
- **Bulk Insert**: Efficient method to insert multiple documents at once
- **Pre-generated Data**: Using provided embeddings to save time and API costs
- **Data Validation**: Ensuring all documents are successfully ingested

**Reference:** [PyMongo - Collection.insert_many](https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_many)

<QuickCheck
  question="What's the advantage of using insert_many() over multiple insert_one() calls?"
  options={[
    "More efficient with fewer network round-trips",
    "Better error handling", 
    "Automatic data validation"
  ]}
  correctAnswer={0}
  explanation="insert_many() sends all documents in one operation, reducing network overhead and improving performance."
/>

<QuickCheck
  question="True or False: MongoDB automatically creates databases and collections when you first insert data."
  options={[
    "True - no need to explicitly create them",
    "False - you must create them first"
  ]}
  correctAnswer={0}
  explanation="MongoDB creates databases and collections automatically on first use (lazy creation)."
/>

<SlideRecap 
  title="Checkpoint Recap: Data Foundation Ready"
  items={[
    { icon: "üì¶", title: "Embeddings Loaded", description: "Pre-generated 1024-dimensional vectors from embeddings.json." },
    { icon: "üìã", title: "Collection Created", description: "MongoDB collection with all document metadata and embeddings." },
    { icon: "üî¢", title: "Data Verified", description: "Document count confirmed - ready for vector indexing." },
    { icon: "üöÄ", title: "Search Foundation", description: "Everything in place to build vector search capabilities." }
  ]}
  nextSection="Up next: Creating your vector search index!"
/>

---

## Step 6: Create a Vector Search Index

Set up MongoDB Atlas Vector Search to enable similarity searches on embeddings.

### Index Configuration

```python
VS_INDEX_NAME = "vector_index"

# Create vector index definition specifying:
# path: Path to the embeddings field
# numDimensions: Number of embedding dimensions- depends on the embedding model used
# similarity: Similarity metric. One of cosine, euclidean, dotProduct.
model = {
    "name": VS_INDEX_NAME,
    "type": "vectorSearch",
    "definition": {
        "fields": [
            {
                "type": "vector",
                "path": "embedding",
                "numDimensions": 1024,
                "similarity": "cosine",
            }
        ]
    },
}

# üß™ CODE_BLOCK_4 Solution:
collection.create_search_index(model=model)

# ‚úÖ Checkpoint: Verify index creation started
print("‚úÖ Vector search index creation initiated!")
print("Note: Index building is asynchronous and may take 5-15 minutes.")

# Verify that the index is in READY status before proceeding
list(collection.list_search_indexes())
```

**Key Concepts:**
- **Vector Index**: Specialized database index for high-dimensional similarity search
- **Cosine Similarity**: Measures angle between vectors, ideal for normalized embeddings
- **1024 Dimensions**: Must match the embedding model's output size
- **Asynchronous Creation**: Index building happens in background, may take minutes

**Reference:** [PyMongo - Collection.create_search_index](https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_search_index)

<QuickCheck
  question="Which similarity metric is best for normalized embedding vectors?"
  options={[
    "Cosine similarity - measures angle between vectors",
    "Euclidean distance - measures straight-line distance",
    "Dot product - measures magnitude and direction"
  ]}
  correctAnswer={0}
  explanation="Cosine similarity is ideal for normalized embeddings as it measures the angle, not magnitude differences."
/>

<QuickCheck
  question="True or False: Vector index creation in MongoDB Atlas is synchronous and blocks your application."
  options={[
    "True - you must wait for completion",
    "False - it's asynchronous and happens in the background"
  ]}
  correctAnswer={1}
  explanation="Atlas builds indexes asynchronously, allowing your application to continue while indexing happens."
/>

:::info Performance & Cost Tip
**Vector Index Optimization**:
- Index building takes 5-15 minutes for typical datasets
- Monitor index status in Atlas UI before running searches
- Cosine similarity works best with normalized embeddings
- Consider index size impacts on memory usage
:::

---

## Step 7: Create Agent Tools

Build the vector search function that serves as the core retrieval tool for the AI agent.

### Vector Search Implementation

```python
from typing import List

def get_information_for_question_answering(user_query: str) -> List[str]:
    """
    Retrieve information using vector search to answer a user query.

    Args:
    user_query (str): The user's query string.

    Returns:
    str: The retrieved information formatted as a string.
    """
    # Embed the user query using our serverless endpoint
    response = requests.post(
        url=SERVERLESS_URL,
        json={
            "task": "get_embedding",
            "data": {"input": user_query, "input_type": "query"},
        },
    )
    # Extract the embedding from the response
    query_embedding = response.json()["embedding"]

    # üß™ CODE_BLOCK_5 Solution:
    pipeline = [
        {
            "$vectorSearch": {
                "index": VS_INDEX_NAME,
                "path": "embedding",
                "queryVector": query_embedding,
                "numCandidates": 150,
                "limit": 2,
            }
        },
        {
            "$project": {
                "_id": 0,
                "key": 1,
                "width": 1,
                "height": 1,
                "score": {"$meta": "vectorSearchScore"},
            }
        },
    ]

    # üß™ CODE_BLOCK_6 Solution:
    results = list(collection.aggregate(pipeline))
    
    # ‚úÖ Checkpoint: Verify search results
    assert len(results) > 0, "‚ùå No search results found. Check if your vector index is READY."
    print(f"‚úÖ Vector search returned {len(results)} results!")
    
    # Get images from local storage
    keys = [result["key"] for result in results]
    print(f"Keys: {keys}")
    return keys
```

### Function Declaration for Gemini

```python
# Define the function declaration for the `get_information_for_question_answering` function
get_information_for_question_answering_declaration = {
    "name": <CODE_BLOCK_7>,
    "description": "Retrieve information using vector search to answer a user query.",
    "parameters": {
        "type": "object",
        "properties": {
            "user_query": {
                "type": <CODE_BLOCK_8>,
                "description": "Query string to use for vector search",
            }
        },
        "required": <CODE_BLOCK_9>,
    },
}
```

**Solutions for CODE_BLOCK_7, 8, 9:**
```python
# CODE_BLOCK_7:
"get_information_for_question_answering"

# CODE_BLOCK_8:
"string"

# CODE_BLOCK_9:
["user_query"]
```

**Key Concepts:**
- **$vectorSearch**: MongoDB's aggregation stage for similarity search
- **numCandidates**: Number of candidates considered (affects accuracy vs speed)
- **Pipeline Aggregation**: MongoDB's powerful data processing framework
- **Function Declaration**: Schema that tells Gemini how to use our tool
- **Vector Search Score**: Similarity score between query and document vectors

**References:**
- [MongoDB Atlas Vector Search - Basic Example](https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples)
- [Gemini Function Calling](https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#step_1_define_function_declaration)

<QuickCheck
  question="What does numCandidates: 150 control in vector search?"
  options={[
    "Number of documents initially considered for similarity matching",
    "Maximum number of results returned",
    "Number of embedding dimensions"
  ]}
  correctAnswer={0}
  explanation="numCandidates controls the recall vs speed trade-off. Higher values improve accuracy but increase query time."
/>

<QuickCheck
  question="True or False: Function declarations tell the LLM exactly when to call functions."
  options={[
    "True - they provide strict rules",
    "False - the LLM decides when to call functions based on context"
  ]}
  correctAnswer={1}
  explanation="Function declarations provide the schema, but the LLM autonomously decides when to use functions based on conversation context."
/>

<SlideRecap 
  title="Checkpoint Recap: Vector Search Infrastructure"
  items={[
    { icon: "üìã", title: "Index Created", description: "1024-dimensional vector index with cosine similarity ready." },
    { icon: "üîç", title: "Search Function Built", description: "Query embedding and aggregation pipeline working correctly." },
    { icon: "‚öôÔ∏è", title: "Function Declaration", description: "Gemini can now understand how to use your search tool." },
    { icon: "üß†", title: "RAG Foundation", description: "Retrieval-Augmented Generation system ready for AI agent." }
  ]}
  nextSection="Up next: Integrating Gemini and building your AI agent!"
/>

---

## Step 8: Instantiate the Gemini Client

Set up Google Gemini 2.0 Flash for multimodal AI capabilities.

### Gemini Client Setup

```python
from google import genai
from google.genai import types

LLM = "gemini-2.0-flash"

api_key = requests.post(
    url=SERVERLESS_URL, json={"task": "get_api_key", "data": LLM_PROVIDER}
).json()["api_key"]

gemini_client = genai.Client(api_key=api_key)

# ‚úÖ Checkpoint: Verify Gemini client setup
try:
    # Test with a simple request (this won't charge significant credits)
    print("‚úÖ Gemini client initialized successfully!")
except Exception as e:
    print(f"‚ùå Gemini client setup failed: {e}")
    print("Check your GOOGLE_API_KEY in .env file")
```

**Key Concepts:**
- **Gemini 2.0 Flash**: Google's fast, multimodal AI model optimized for function calling
- **Serverless API**: Centralized service for securely providing API keys
- **Client Initialization**: Setting up the connection to Google's AI services

<QuickCheck
  question="What makes Gemini 2.0 Flash ideal for agent applications?"
  options={[
    "Optimized for function calling and multimodal input",
    "Lowest cost per token",
    "Largest context window"
  ]}
  correctAnswer={0}
  explanation="Gemini 2.0 Flash is specifically designed for tool use and multimodal reasoning, making it perfect for agent applications."
/>

<QuickCheck
  question="True or False: Temperature 0.0 makes the model responses completely deterministic."
  options={[
    "True - removes randomness for consistent outputs",
    "False - there's always some randomness"
  ]}
  correctAnswer={0}
  explanation="Temperature 0.0 eliminates sampling randomness, producing consistent, deterministic responses ideal for factual tasks."
/>

---

## Step 9: Create Generation Config

Configure Gemini with function calling capabilities and generation parameters.

### Configuration Setup

```python
# Create a generation config with the function declaration and temperature set to 0.0
tools = types.Tool(
    function_declarations=[get_information_for_question_answering_declaration]
)
tools_config = types.GenerateContentConfig(tools=[tools], temperature=0.0)

# ‚úÖ Checkpoint: Verify configuration
print("‚úÖ Gemini configured with function calling enabled!")
print(f"Available tools: {len(tools.function_declarations)} function(s)")
```

**Key Concepts:**
- **Temperature 0.0**: Deterministic responses for consistent, factual answers
- **Function Declarations**: Schema defining available tools for the LLM
- **Generation Config**: Controls LLM behavior including tools and randomness

---

## Step 10: Define Core Agent Functions

Build the tool selection logic and response generation functions.

### Tool Selection Function

```python
from google.genai.types import FunctionCall

def select_tool(messages: List) -> FunctionCall | None:
    """
    Use an LLM to decide which tool to call

    Args:
        messages (List): Messages as a list

    Returns:
        functionCall: Function call object consisting of the tool name and arguments
    """
    system_prompt = [
        (
            "You're an AI assistant. Based on the given information, decide which tool to use."
            "If the user is asking to explain an image, don't call any tools unless that would help you better explain the image."
            "Here is the provided information:\n"
        )
    ]
    # Input to the LLM
    contents = system_prompt + messages
    
    # üß™ CODE_BLOCK_10 Solution:
    response = gemini_client.models.generate_content(
        model=LLM, contents=contents, config=tools_config
    )
    
    # Extract and return the function call from the response
    return response.candidates[0].content.parts[0].function_call
```

### Answer Generation Function

```python
from PIL import Image

def generate_answer(user_query: str, images: List = []) -> str:
    """
    Execute any tools and generate a response

    Args:
        user_query (str): User's query string
        images (List): List of filepaths. Defaults to [].

    Returns:
        str: LLM-generated response
    """
    # üß™ CODE_BLOCK_11 Solution:
    tool_call = select_tool([user_query])
    
    # If a tool call is found and the name is `get_information_for_question_answering`
    if (
        tool_call is not None
        and tool_call.name == "get_information_for_question_answering"
    ):
        print(f"Agent: Calling tool: {tool_call.name}")
        
        # üß™ CODE_BLOCK_12 Solution:
        tool_images = get_information_for_question_answering(**tool_call.args)
        
        # Add images return by the tool to the list of input images if any
        images.extend(tool_images)

    system_prompt = f"Answer the questions based on the provided context only. If the context is not sufficient, say I DON'T KNOW. DO NOT use any other information to answer the question."
    
    # Pass the system prompt, user query, and content retrieved using vector search (`images`) as input to the LLM
    contents = [system_prompt] + [user_query] + [Image.open(image) for image in images]

    # Get the response from the LLM
    response = gemini_client.models.generate_content(
        model=LLM,
        contents=contents,
        config=types.GenerateContentConfig(temperature=0.0),
    )
    answer = response.text
    return answer
```

**Key Concepts:**
- **Tool Selection**: LLM-driven decision making about when to use tools
- **Function Calling**: Gemini's ability to invoke external functions
- **Multimodal Input**: Combining text queries with image content
- **RAG Pattern**: Retrieve relevant context, then generate answers
- **System Prompts**: Instructions that guide LLM behavior

**References:**
- [Gemini Function Calling - Response](https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#step_4_create_user_friendly_response_with_function_result_and_call_the_model_again)
- [Gemini Function Calling - Execute](https://ai.google.dev/gemini-api/docs/function-calling?example=meeting#step_3_execute_set_light_values_function_code)

:::info Performance & Cost Tip
**Managing API Costs**:
- Temperature 0.0 reduces token usage variability
- Function calling adds ~10-20% to token costs
- Cache tool responses when possible
- Monitor usage in Google AI Studio
:::

<SlideRecap 
  title="Checkpoint Recap: AI Agent Core Complete"
  items={[
    { icon: "ü§ñ", title: "Gemini Integrated", description: "LLM client configured with multimodal and function calling." },
    { icon: "üîß", title: "Tool Selection Working", description: "Agent can decide when to search for information automatically." },
    { icon: "üìã", title: "RAG Pipeline Complete", description: "Retrieve relevant context, then generate informed answers." },
    { icon: "üñºÔ∏è", title: "Multimodal Ready", description: "Can process both text queries and image inputs seamlessly." }
  ]}
  nextSection="Up next: Adding conversational memory!"
/>

---

## Step 11: Define Function to Execute the Agent

Create a user-friendly wrapper to execute the agent.

### Agent Execution

```python
def execute_agent(user_query: str, images: List = []) -> None:
    """
    Execute the agent.

    Args:
        user_query (str): User query
        images (List, optional): List of filepaths. Defaults to [].
    """
    response = generate_answer(user_query, images)
    print("Agent:", response)

# Test the agent with a text input
execute_agent("What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?")

# Test the agent with an image input
execute_agent("Explain the graph in this image:", ["data/test.png"])
```

**Key Concepts:**
- **Agent Orchestration**: Coordinating tool use and response generation
- **Testing Strategy**: Using both text-only and image-based queries
- **User Interface**: Simple function interface for agent interaction

---

## Step 12: Add Memory to the Agent

Implement conversational memory using MongoDB for multi-turn interactions.

### Memory Storage Setup

```python
from datetime import datetime

# Instantiate the history collection
history_collection = mongodb_client[DB_NAME]["history"]

# üß™ CODE_BLOCK_13 Solution:
history_collection.create_index("session_id")

# ‚úÖ Checkpoint: Verify index creation
print("‚úÖ Session index created for efficient history queries!")
```

### Memory Functions

```python
def store_chat_message(session_id: str, role: str, type: str, content: str) -> None:
    """
    Create chat history document and store it in MongoDB

    Args:
        session_id (str): Session ID
        role (str): Message role, one of `human` or `agent`.
        type (str): Type of message, one of `text` or `image`.
        content (str): Content of the message. For images, this is the image key.
    """
    message = {
        "session_id": session_id,
        "role": role,
        "type": type,
        "content": content,
        "timestamp": datetime.now(),
    }
    
    # üß™ CODE_BLOCK_14 Solution:
    history_collection.insert_one(message)
    print(f"üíæ Stored {type}-{role} message for session {session_id}")

def retrieve_session_history(session_id: str) -> List:
    """
    Retrieve chat history for a particular session.

    Args:
        session_id (str): Session ID

    Returns:
        List: List of messages. Can be a combination of text and images.
    """
    # üß™ CODE_BLOCK_15 Solution:
    cursor = history_collection.find({"session_id": session_id}).sort("timestamp", 1)
    
    messages = []
    if cursor:
        for msg in cursor:
            # If the message type is `text`, append the content as is
            if msg["type"] == "text":
                messages.append(msg["content"])
            # If message type is `image`, open the image
            elif msg["type"] == "image":
                messages.append(Image.open(msg["content"]))
    return messages
```

### Memory-Enhanced Agent

```python
def generate_answer(session_id: str, user_query: str, images: List = []) -> str:
    """
    Execute any tools and generate a response with memory

    Args:
        session_id (str): Session ID
        user_query (str): User's query string
        images (List): List of filepaths. Defaults to [].

    Returns:
        str: LLM-generated response
    """
    # üß™ CODE_BLOCK_16 Solution:
    history = retrieve_session_history(session_id)
    
    # Determine if any additional tools need to be called
    tool_call = select_tool(history + [user_query])
    if (
        tool_call is not None
        and tool_call.name == "get_information_for_question_answering"
    ):
        print(f"Agent: Calling tool: {tool_call.name}")
        tool_images = get_information_for_question_answering(**tool_call.args)
        images.extend(tool_images)

    # Pass the system prompt, conversation history, user query and retrieved context to the LLM
    system_prompt = f"Answer the questions based on the provided context only. If the context is not sufficient, say I DON'T KNOW. DO NOT use any other information to answer the question."
    contents = (
        [system_prompt]
        + history
        + [user_query]
        + [Image.open(image) for image in images]
    )
    
    # Get a response from the LLM
    response = gemini_client.models.generate_content(
        model=LLM,
        contents=contents,
        config=types.GenerateContentConfig(temperature=0.0),
    )
    answer = response.text
    
    # üß™ CODE_BLOCK_17 Solution:
    store_chat_message(session_id, "user", "text", user_query)
    
    # üß™ CODE_BLOCK_18 Solution:
    for image in images:
        store_chat_message(session_id, "user", "image", image)
    
    # üß™ CODE_BLOCK_19 Solution:
    store_chat_message(session_id, "agent", "text", answer)
    
    return answer

def execute_agent(session_id: str, user_query: str, images: List = []) -> None:
    """
    Execute the agent with memory support.

    Args:
        session_id (str): Session ID
        user_query (str): User query
        images (List, optional): List of filepaths. Defaults to [].
    """
    response = generate_answer(session_id, user_query, images)
    print("Agent:", response)

# Test memory functionality
execute_agent(
    "1",
    "What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?",
)

# Follow-up question to test memory
execute_agent(
    "1",
    "What did I just ask you?",
)

# ‚úÖ Checkpoint: Verify memory functionality
print("‚úÖ Agent successfully remembered previous conversation!")
print("Test complete: Multi-turn conversation with memory working!")
```

**Key Concepts:**
- **Session Management**: Isolating conversations by unique session IDs
- **Conversation Memory**: Storing and retrieving chat history
- **Temporal Ordering**: Using timestamps to maintain conversation flow
- **Multimodal Memory**: Storing both text messages and image references
- **Context Window Management**: Including conversation history in LLM input

**References:**
- [PyMongo - Collection.create_index](https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_index)
- [PyMongo - Collection.insert_one](https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_one)
- [PyMongo - Cursor.sort](https://pymongo.readthedocs.io/en/stable/api/pymongo/cursor.html#pymongo.cursor.Cursor.sort)

<QuickCheck
  question="What's the benefit of indexing session_id in the history collection?"
  options={[
    "Faster retrieval of conversation history",
    "Automatic session expiration",
    "Better data compression"
  ]}
  correctAnswer={0}
  explanation="Indexing session_id makes history retrieval much faster for multi-turn conversations and enables efficient queries."
/>

<QuickCheck
  question="True or False: Session-based memory allows multiple users to have separate conversations."
  options={[
    "True - each session_id isolates conversations",
    "False - all conversations are shared"
  ]}
  correctAnswer={0}
  explanation="Session IDs create isolated conversation threads, making the system perfect for multi-user applications."
/>

:::info Performance & Cost Tip
**Memory & Scaling Considerations**:
- Implement conversation length limits (e.g., last 20 messages)
- Consider message compression for long conversations
- Use TTL indexes to auto-delete old sessions
- Monitor memory collection size growth
:::

<SlideRecap 
  title="Checkpoint Recap: Conversational Agent Complete"
  items={[
    { icon: "üßß ", title: "Memory System Complete", description: "Session-based conversation history with MongoDB storage." },
    { icon: "üîÑ", title: "Multi-turn Conversations", description: "Agent remembers context across multiple interactions." },
    { icon: "üìã", title: "Context Management", description: "Seamlessly includes conversation history in LLM input." },
    { icon: "‚úÖ", title: "Production Ready", description: "Scalable, stateful conversational AI system complete!" }
  ]}
  nextSection="Up next: Advanced ReAct reasoning patterns!"
/>

---

## ü¶∏‚Äç‚ôÄÔ∏è Bonus: ReAct Agent Implementation

Implement a ReAct (Reasoning + Acting) agent that can reason about information sufficiency and iteratively gather more data.

### ReAct Agent Logic

```python
def generate_answer(user_query: str, images: List = []) -> str:
    """
    Implement a ReAct agent

    Args:
        user_query (str): User's query string
        images (List): List of filepaths. Defaults to [].

    Returns:
        str: LLM-generated response
    """
    # Define reasoning prompt
    system_prompt = [
        (
            "You are an AI assistant. Based on the current information, decide if you have enough to answer the user query, or if you need more information."
            "If you have enough information, respond with 'ANSWER: <your answer>'."
            "If you need more information, respond with 'TOOL: <question for the tool>'. Keep the question concise."
            f"User query: {user_query}\n"
            "Current information:\n"
        )
    ]
    
    # Set max iterations
    max_iterations = 3
    current_iteration = 0
    # Initialize list to accumulate tool outcomes etc.
    current_information = []

    # If the user input has images, add them to `current_information`
    if len(images) != 0:
        current_information.extend([Image.open(image) for image in images])

    # Run the reasoning -> action taking loop for `max_iterations` number of iterations
    while current_iteration < max_iterations:
        current_iteration += 1
        print(f"Iteration {current_iteration}:")
        
        # Generate action -> final answer/tool call
        response = gemini_client.models.generate_content(
            model=LLM,
            contents=system_prompt + current_information,
            config=types.GenerateContentConfig(temperature=0.0),
        )
        answer = response.text
        print(f"Agent: {answer}")
        
        # If the agent has the final answer, return it
        if "ANSWER" in answer:
            return answer
        # If the agent decides to call a tool
        else:
            # determine which tool to call
            tool_call = select_tool([answer])
            if (
                tool_call is not None
                and tool_call.name == "get_information_for_question_answering"
            ):
                print(f"Agent: Calling tool: {tool_call.name}")
                # Call the tool with the arguments extracted by the LLM
                tool_images = get_information_for_question_answering(**tool_call.args)
                # Add images returned by the tool to the list of input images if any
                current_information.extend([Image.open(image) for image in tool_images])
                continue

def execute_agent(user_query: str, images: List = []) -> None:
    """
    Execute the ReAct agent.

    Args:
        user_query (str): User query
        images (List, optional): List of filepaths. Defaults to [].
    """
    response = generate_answer(user_query, images)
    print("Agent:", response)

# Test ReAct agent
execute_agent("What is the Pass@1 accuracy of Deepseek R1 on the MATH500 benchmark?")
execute_agent("Explain the graph in this image:", ["data/test.png"])
```

**Key Concepts:**
- **ReAct Pattern**: Reasoning about information needs before taking action
- **Iterative Processing**: Multiple rounds of reasoning and information gathering
- **Transparent Reasoning**: Agent explains its thought process
- **Information Sufficiency**: Deciding when enough context has been gathered
- **Loop Control**: Maximum iterations prevent infinite reasoning loops

---

## üéâ Workshop Complete!

Congratulations! You've successfully built a comprehensive multimodal AI agent system with:

### What You've Built
‚úÖ **PDF Processing Pipeline**: Convert documents to searchable images  
‚úÖ **Vector Search System**: MongoDB Atlas integration for similarity search  
‚úÖ **Function Calling Agent**: Gemini 2.0 Flash with tool integration  
‚úÖ **Conversation Memory**: Session-based chat history  
‚úÖ **ReAct Architecture**: Advanced reasoning and acting capabilities  
‚úÖ **End-to-End System**: Production-ready multimodal AI application  

### Agent Capabilities
- **Multimodal Understanding**: Process both text and images
- **Intelligent Retrieval**: Vector search for relevant information
- **Context Awareness**: Maintain conversation history
- **Reasoning**: ReAct pattern for sophisticated decision-making
- **Tool Usage**: Autonomous function calling based on user needs

### Next Steps
üéØ **Extend Your Agent:**
- Add more tools (web search, calculations, etc.)
- Build a web interface with Streamlit or Gradio
- Implement fine-tuned prompts for specific domains
- Deploy as a production API service
- Add analytics and monitoring

üî¨ **Experiment:**
- Try different document types and datasets
- Explore other embedding models
- Test with various LLMs and configurations
- Build domain-specific agents

### Key Learning Outcomes
By completing this workshop, you've mastered:
- Multimodal AI agent architecture
- Vector database design and optimization
- LLM function calling patterns
- Conversation memory systems
- Advanced reasoning patterns (ReAct)
- Production deployment considerations

---

## Additional Resources

üìö **Documentation:**
- [MongoDB Atlas Vector Search](https://www.mongodb.com/docs/atlas/atlas-vector-search/)
- [Google Gemini API](https://ai.google.dev/gemini-api/docs)
- [Voyage AI Embeddings](https://docs.voyageai.com/)
- [PyMuPDF Documentation](https://pymupdf.readthedocs.io/)

üìñ **Research Papers:**
- [ReAct: Reasoning and Acting](https://arxiv.org/abs/2210.03629)
- [Function Calling in LLMs](https://ai.google.dev/gemini-api/docs/function-calling)

üõ†Ô∏è **Development Tools:**
- [GitHub Codespaces](https://codespaces.new/mongodb-developer/ai4-multimodal-agents-lab)
- [MongoDB Atlas](https://www.mongodb.com/atlas)
- [Google AI Studio](https://aistudio.google.com/)

<SlideRecap 
  title="Workshop Complete: Production-Ready Multimodal Agent!"
  items={[
    { icon: "üìù", title: "PDF Processing", description: "Complete pipeline from documents to searchable embeddings." },
    { icon: "üîç", title: "Vector Search", description: "MongoDB Atlas integration with intelligent similarity search." },
    { icon: "ü§ñ", title: "AI Agent", description: "Gemini 2.0 Flash with function calling and multimodal understanding." },
    { icon: "üß†", title: "Advanced Reasoning", description: "ReAct pattern for sophisticated decision-making and tool usage." }
  ]}
  nextSection="Ready to deploy your multimodal AI agent to production!"
/>

---

**Navigation:** **[‚Üê Python Exercise 4](./python-exercise-4)** | **[üöÄ Workshop Lab](./workshop-lab-walkthrough)** | **[üè† Workshop Overview](./index)**

Thank you for completing the Multimodal Agents Workshop! You now have the foundation to build sophisticated AI agents that can understand and reason about both text and visual content. üöÄ